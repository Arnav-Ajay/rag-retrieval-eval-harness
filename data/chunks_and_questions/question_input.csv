gold_doc_id,document_name,gold_chunk_id,chunk_text,question_id,question_intent,question_text
1,Attention Is All You Need.pdf,1,"basedoncomplexrecurrentor convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions entirely. Experiments on two machine translation tasks show these models to besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly less time to train. Our model achi",1,definition,What architectural components are removed in the Transformer model?
1,Attention Is All You Need.pdf,6,"utandoutput sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently t tâˆ’1 sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved significantimprovementsincomputationalefficiencythroughfactorizationtricks[18]andconditional computation[26],whilealsoimprovingmodel",2,mechanism,What aspect of recurrent neural networks prevents parallelization during training?
1,Attention Is All You Need.pdf,7,"onditional computation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental constraintofsequentialcomputation,however,remains. Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc- tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein theinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms areusedinconjunctionwitharecurrentnetwork. InthisworkweproposetheTransformer,amodelarchitecturees",3,capability,What capability do attention mechanisms provide for modeling dependencies in sequences?
1,Attention Is All You Need.pdf,8,"isworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput. TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs. 2 Background ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbui",4,definition,What mechanism does the Transformer rely on instead of recurrence?
1,Attention Is All You Need.pdf,9,"allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolut",5,comparative,How does the number of operations required to relate distant positions differ between convolutional models and the Transformer?
1,Attention Is All You Need.pdf,10,"erations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepr",6,definition,How is self-attention defined?
1,Attention Is All You Need.pdf,11,"lentailmentandlearningtask-independentsentencerepresentations[4,22,23,19]. End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[28]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution. Inthefollowingsections,",7,definition,What distinguishes the Transformer from earlier transduction models?
1,Attention Is All You Need.pdf,13,"ut 1 n sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive 1 m [9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively. 3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. Thefirst",8,definition,What overall encoder–decoder architecture does the Transformer use?
1,Attention Is All You Need.pdf,14,"al layers. Each layer has two sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- 2 Figure1: TheTransformer-modelarchitecture. wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswell",9,definition,What two sub-layers make up each encoder layer?
1,Attention Is All You Need.pdf,15,"residualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512. model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention sub-layer in the decoder stack to prevent pos",10,definition,What additional sub-layer is added to each decoder layer?
1,Attention Is All You Need.pdf,16,"tion sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum ofthevalues,wheretheweightassignedtoeachvalueiscomputedby",11,mechanism,Why is masking applied in the decoder self-attention layer?
1,Attention Is All You Need.pdf,18,"everal attentionlayersrunninginparallel. âˆš querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( âˆš )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenti",12,mechanism,What operation is applied to scaled dot products before weighting the values?
1,Attention Is All You Need.pdf,21,"alueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v output values. These are concatenated and once again projected, resulting in the final values, as depictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.",13,capability,What advantage does multi-head attention provide?
1,Attention Is All You Need.pdf,25,"oder. â€¢ Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis insideofscaleddot-productattentionbymaskingout(settingtoâˆ’âˆž)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercont",14,mechanism,How are illegal attention connections prevented in the decoder?
1,Attention Is All You Need.pdf,26,"b-layers,eachofthelayersinourencoderanddecodercontainsa fully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This consistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner",15,mechanism,What is the functional form of the position-wise feed-forward network?
1,Attention Is All You Need.pdf,30,"ncodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings, learnedandfixed[8]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2Ï€to10000Â·2Ï€. We chosethisfunctionbecausewehypothesizeditwouldallowthem",16,mechanism,What mathematical functions are used to construct positional encodings?
2,Large Language Models A Survey.pdf,66," LLMs. in November 2022. LLMsâ€™ ability of general-purpose language understanding and generation is acquired by training billions of Statisticallanguagemodels(SLMs)viewtextasasequence modelâ€™sparametersonmassiveamountsoftextdata,aspredicted of words, and estimate the probability of text as the product by scaling laws [1], [2]. The research area of LLMs, while very of their word probabilities. The dominating form of SLMs recent, is evolving rapidly in many different ways. In this paper, are Markov ",17,definition,How do statistical language models compute the probability of text?
2,Large Language Models A Survey.pdf,67,"in many different ways. In this paper, are Markov chain models known as the n-gram models, we review some of the most prominent LLMs, including three which compute the probability of a word conditioned on its popular LLM families (GPT, LLaMA, PaLM), and discuss their immediate proceeding nâˆ’1 words. Since word probabilities characteristics, contributions and limitations. We also give an are estimated using word and n-gram counts collected from overview of techniques developed to build, and augmen",18,definition,What context length does an n-gram model condition on?
2,Large Language Models A Survey.pdf,68,"rview of techniques developed to build, and augment LLMs. text corpora, the model needs to deal with data sparsity (i.e., We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation assigning zero probabilities to unseen words or n-grams) by metrics, and compare the performance of several popular LLMs using smoothing, where some probability mass of the model on a set of representative benchmarks. Finally, we conclude is reserved for u",19,mechanism,What technique is used to mitigate data sparsity in n-gram models?
2,Large Language Models A Survey.pdf,70,"opic, dat- deal with data sparsity by mapping words to low-dimensional ing back to the 1950s with Shannonâ€™s application of informa- continuous vectors (embedding vectors) and predict the next tion theory to human language, where he measured how well word based on the aggregation of the embedding vectors of simple n-gram language models predict or compress natural its proceeding words using neural networks. The embedding language text [3]. Since then, statistical language modeling vectors learned",20,comparative,How do neural language models address data sparsity differently from n-gram models?
2,Large Language Models A Survey.pdf,71,"hen, statistical language modeling vectors learned by NLMs define a hidden space where the became fundamental to many natural language understanding semantic similarity between vectors can be readily computed and generation tasks, ranging from speech recognition, ma- as their distance. This opens the door to computing semantic chine translation, to information retrieval [4], [5], [6]. similarityofanytwoinputsregardlesstheirforms(e.g.,queries vs. documents in Web search [17], [18], sentences in d",21,principle,What property of embedding vectors enables semantic similarity computation?
2,Large Language Models A Survey.pdf,73,"4 can be used not data and their learned hidden space is task-specific. only for natural language processing, but also as general task solvers to power Microsoftâ€™s Co-Pilot systems, for instance, Pre-trained language models (PLMs), unlike early NLMs, can follow human instructions of complex new tasks per- are task-agnostic. This generality also extends to the learned forming multi-step reasoning when needed. LLMs are thus hidden embedding space. The training and inference of PLMs becoming the ba",22,comparative,What distinguishes pre-trained language models from early neural language models?
2,Large Language Models A Survey.pdf,74,"The training and inference of PLMs becoming the basic building block for the development of follows the pre-training and fine-tuning paradigm, where lan- general-purpose AI agents or artificial general intelligence guage models with recurrent neural networks [23] or trans- (AGI). formers[24],[25],[26]arepre-trainedonWeb-scaleunlabeled textcorporaforgeneraltaskssuchaswordprediction,andthen As the field of LLMs is moving fast, with new findings, finetuned to specific tasks using small amounts of (",23,process,What are the two stages of the pre-training and fine-tuning paradigm?
2,Large Language Models A Survey.pdf,76,"ecent advances on LLMs. We transformer-based neural language models 1 that contain hope this survey will prove a valuable and accessible resource tens to hundreds of billions of parameters, which are pre- for students, researchers and developers. trained on massive text data, such as PaLM [31], LLaMA [32], and GPT-4 [33], as summarized in Table III. Compared LLMsarelarge-scale,pre-trained,statisticallanguagemod-els based on neural networks. The recent success of LLMs is 1Recently, several very ",24,definition,How are large language models defined in terms of scale and architecture?
2,Large Language Models A Survey.pdf,80,"ergent abilities that are evaluatingLLMs,andsummarizethereportedLLMevaluation not present in smaller-scale language models. As illustrated results. Finally, Section VII concludes the paper by summa- in Fig. 1, these emergent abilities include (1) in-context rizing the challenges and future research directions. learning, where LLMs learn a new task from a small set of examples presented in the prompt at inference time, (2) instruction following, where LLMs, after instruction tuning, II. LARGELANG",25,capability,What emergent abilities are observed in large language models?
2,Large Language Models A Survey.pdf,81,"here LLMs, after instruction tuning, II. LARGELANGUAGEMODELS can follow the instructions for new types of tasks without In this section we start with a review of early pre-trained using explicit examples, and (3) multi-step reasoning, where neural language models as they are the base of LLMs, and LLMs can solve a complex task by breaking down that task then focus our discussion on three families of LLMs: GPT, into intermediate reasoning steps as demonstrated in the LlaMA, and PaLM. Table I provi",26,capability,What capability does instruction tuning enable?
2,Large Language Models A Survey.pdf,83,"ng with Language modeling using neural networks was pioneered human feedback (RLHF)). by[38],[39],[40].Bengioetal.[13]developedoneofthefirst Through advanced usage and augmentation techniques, neurallanguagemodels(NLMs)thatarecomparableton-gram LLMscanbedeployedasso-calledAIagents:artificialentities models. Then, [14] successfully applied NLMs to machine thatsensetheirenvironment,makedecisions,andtakeactions. translation. The release of RNNLM (an open source NLM Previousresearchhasfocusedondevel",27,historical,Which neural architectures were commonly used before Transformers?
2,Large Language Models A Survey.pdf,85,",werewidely instaticsettings,AIagentsneedtotakeactionstointeractwith usedformanynaturallanguageapplicationsincludingmachine dynamic environment. Therefore, LLM-based agents often translation, text generation and text classification [43]. need to augment LLMs to e.g., obtain updated information Then, the invention of the Transformer architecture [44] fromexternalknowledgebases,verifywhetherasystemaction marks another milestone in the development of NLMs. By produces the expected result, and cope ",28,comparative,What advantage do Transformers have over recurrent neural networks?
2,Large Language Models A Survey.pdf,91,"er models. Comprehen- language models. BERT consists of three modules: (1) an sive surveys of early PLMs are provided in [43], [28]. embedding module that converts input text into a sequence of embedding vectors, (2) a stack of Transformer encoders thatconvertsembeddingvectorsintocontextualrepresentation 1)Encoder-onlyPLMs: Asthenamesuggests,theencoder- vectors, and (3) a fully connected layer that converts the onlymodelsonlyconsistofanencodernetwork.Thesemodels representation vectors (at the fi",29,definition,What components make up the BERT architecture?
2,Large Language Models A Survey.pdf,92,"work.Thesemodels representation vectors (at the final layer) to one-hot vectors. are originally developed for language understanding tasks, BERT is pre-trained uses two objectives: masked language such as text classification, where the models need to predict a modeling(MLM)andnextsentenceprediction.Thepre-trained classlabelforaninputtext.Representativeencoder-onlymod- BERT model can be fine-tuned by adding a classifier layer els include BERT and its variants, e.g., RoBERTa, ALBERT, for many lang",30,process,What pre-training objectives are used in BERT?
2,Large Language Models A Survey.pdf,104,"mprovemodelsâ€™generalization.ELECTRA[46] usesanewpre-trainingtask,knownasreplacedtokendetection (RTD),whichisempiricallyproventobemoresample-efficient than MLM. Instead of masking the input, RTD corrupts it by all permutations of the factorization order. UNILM (UNIfied replacingsometokenswithplausiblealternativessampledfrom pre-trained Language Model) [49] is pre-trained using three a small generator network. Then, instead of training a model typesoflanguagemodelingtasks:unidirectional,bidirectio",31,mechanism,How does replaced token detection differ from masked language modeling?
2,Large Language Models A Survey.pdf,107,"ingual language parameters are shared across the LM objectives (i.e., bidirec- models using two methods: (1) a unsupervised method that tionalLM,unidirectionalLM,andsequence-to-sequenceLM). only relies on monolingual data, and (2) a supervised method Courtesy of [49]. that leverages parallel data with a new cross-lingual language model objective, as illustrated in Fig 5. XLMs had obtained state-of-the-art results on cross-lingual classification, unsuper- 2)Decoder-only PLMs: Two of the most wide",32,process,What training approaches are used in XLM?
2,Large Language Models A Survey.pdf,128,"GPT-3,withafewminorarchitecturalmodifications,including (1) using a SwiGLU activation function instead of ReLU, (2) using rotary positional embeddings instead of absolute positional embedding, and (3) using root-mean-squared layer- Fig. 10: The high-level overview of RLHF. Courtesy of [59]. normalization instead of standard layer-normalization. The open-source LLaMA-13B model outperforms the proprietary GPT-3 (175B) model on most benchmarks, making it a good The most important milestone of LLM d",33,comparative,What architectural modifications distinguish LLaMA from GPT-3?
2,Large Language Models A Survey.pdf,155," have up to 137B parameters and are of [86]. pre-trainedon1.56Twordsofpublicdialogdataandwebtext. Chinchilla:In[2],Hoffmannetal.investigatedtheoptimal model size and number of tokens for training a transformer language model under a given compute budget. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they found that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for e",34,principle,What scaling rule does the Chinchilla study identify for compute-optimal training?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,516,"nvestigated forextractivedownstreamtasks. Weexploreageneral-purposefine-tuningrecipe forretrieval-augmentedgeneration(RAG)â€”modelswhichcombinepre-trained parametricandnon-parametricmemoryforlanguagegeneration. Weintroduce RAGmodelswheretheparametricmemoryisapre-trainedseq2seqmodeland thenon-parametricmemoryisadensevectorindexofWikipedia,accessedwith a pre-trained neural retriever. We compare two RAG formulations, one which conditionsonthesameretrievedpassagesacrossthewholegeneratedsequence, andan",35,definition,What types of memory are combined in retrieval-augmented generation models?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,516,"nvestigated forextractivedownstreamtasks. Weexploreageneral-purposefine-tuningrecipe forretrieval-augmentedgeneration(RAG)â€”modelswhichcombinepre-trained parametricandnon-parametricmemoryforlanguagegeneration. Weintroduce RAGmodelswheretheparametricmemoryisapre-trainedseq2seqmodeland thenon-parametricmemoryisadensevectorindexofWikipedia,accessedwith a pre-trained neural retriever. We compare two RAG formulations, one which conditionsonthesameretrievedpassagesacrossthewholegeneratedsequence, andan",36,definition,What serves as the non-parametric memory in RAG models?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,517,"evedpassagesacrossthewholegeneratedsequence, andanotherwhichcanusedifferentpassagespertoken. Wefine-tuneandevaluate ourmodelsonawiderangeofknowledge-intensiveNLPtasksandsetthestateof theartonthreeopendomainQAtasks,outperformingparametricseq2seqmodels andtask-specificretrieve-and-extractarchitectures. Forlanguagegenerationtasks, wefindthatRAGmodelsgeneratemorespecific,diverseandfactuallanguagethan astate-of-the-artparametric-onlyseq2seqbaseline. 1 Introduction Pre-trainedneurallanguagemodelshaveb",37,comparative,How do the two RAG formulations differ in how retrieved passages are used?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,518," Introduction Pre-trainedneurallanguagemodelshavebeenshowntolearnasubstantialamountofin-depthknowl- edgefromdata[47]. Theycandosowithoutanyaccesstoanexternalmemory,asaparameterized implicitknowledgebase[51,52]. Whilethisdevelopmentisexciting,suchmodelsdohavedown- sides: Theycannoteasilyexpandorrevisetheirmemory,canâ€™tstraightforwardlyprovideinsightinto theirpredictions,andmayproduceâ€œhallucinationsâ€[38]. Hybridmodelsthatcombineparametric memorywithnon-parametric(i.e.,retrieval-based)memories[20,26",38,principle,What limitations of purely parametric language models are identified?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,519,"non-parametric(i.e.,retrieval-based)memories[20,26,48]canaddresssomeofthese issuesbecauseknowledgecanbedirectlyrevisedandexpanded,andaccessedknowledgecanbe inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combinemaskedlanguagemodels[8]withadifferentiableretriever,haveshownpromisingresults, 34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020),Vancouver,Canada. Define(cid:3)""middle(cid:3)ea(cid:85)""(x) The(cid:3)middle(cid:3)ea(cid:85)(cid:3)incl(",39,definition,Which models combine masked language models with a differentiable retriever?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,525,"combineapre-trainedretriever(QueryEncoder+Document Index)withapre-trainedseq2seqmodel(Generator)andfine-tuneend-to-end. Forqueryx,weuse MaximumInnerProductSearch(MIPS)tofindthetop-Kdocumentsz . Forfinalpredictiony,we i treatzasalatentvariableandmarginalizeoverseq2seqpredictionsgivendifferentdocuments. buthaveonlyexploredopen-domainextractivequestionanswering. Here,webringhybridparametric andnon-parametricmemorytotheâ€œworkhorseofNLP,â€i.e. sequence-to-sequence(seq2seq)models. Weendowpre-trained,par",40,mechanism,What search method is used to retrieve top-K documents in RAG?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,526,"to-sequence(seq2seq)models. Weendowpre-trained,parametric-memorygenerationmodelswithanon-parametricmemorythrough ageneral-purposefine-tuningapproachwhichwerefertoasretrieval-augmentedgeneration(RAG). WebuildRAGmodelswheretheparametricmemoryisapre-trainedseq2seqtransformer,andthe non-parametricmemoryisadensevectorindexofWikipedia,accessedwithapre-trainedneural retriever. Wecombinethesecomponentsinaprobabilisticmodeltrainedend-to-end(Fig. 1). The retriever(DensePassageRetriever[26],henceforthDPR)p",41,definition,Which sequence-to-sequence model is used as the generator in RAG?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,527,"etriever(DensePassageRetriever[26],henceforthDPR)provideslatentdocumentsconditionedon theinput,andtheseq2seqmodel(BART[32])thenconditionsontheselatentdocumentstogetherwith theinputtogeneratetheoutput. Wemarginalizethelatentdocumentswithatop-Kapproximation, eitheronaper-outputbasis(assumingthesamedocumentisresponsibleforalltokens)oraper-token basis(wheredifferentdocumentsareresponsiblefordifferenttokens). LikeT5[51]orBART,RAG canbefine-tunedonanyseq2seqtask,wherebyboththegeneratorandretrieverarej",42,mechanism,What approximation is used to marginalize over retrieved documents during generation?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,528,"eq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned. Therehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric memorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive knowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgei",43,comparative,How does this approach differ from training non-parametric memory from scratch?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,529,"ainedaccessmechanisms,theabilitytoaccessknowledgeis presentwithoutadditionaltraining. Ourresultshighlightthebenefitsofcombiningparametricandnon-parametricmemorywithgenera- tionforknowledge-intensivetasksâ€”tasksthathumanscouldnotreasonablybeexpectedtoperform withoutaccesstoanexternalknowledgesource. OurRAGmodelsachievestate-of-the-artresults onopenNaturalQuestions[29],WebQuestions[3]andCuratedTrec[2]andstronglyoutperform recentapproachesthatusespecialisedpre-trainingobjectivesonTriviaQA[24]. Despi",44,historical,Which open-domain QA datasets achieve state-of-the-art results with RAG?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,531,"estrongretrievalsupervision. Finally,wedemonstratethat thenon-parametricmemorycanbereplacedtoupdatethemodelsâ€™knowledgeastheworldchanges.1 2 Methods WeexploreRAGmodels,whichusetheinputsequencextoretrievetextdocumentszandusethem as additional context when generating the target sequence y. As shown in Figure 1, our models leveragetwocomponents: (i)aretrieverp (z x)withparametersâŒ˜ thatreturns(top-Ktruncated) âŒ˜ | distributionsovertextpassagesgivenaqueryxand(ii)ageneratorp (y x,z,y )parametrized âœ“ i 1",45,definition,What components make up a retrieval-augmented generation model?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,533,"theretrieverandgeneratorend-to-end,wetreattheretrieveddocumentasalatentvariable. Weproposetwomodelsthatmarginalizeoverthelatentdocumentsindifferentwaystoproducea distributionovergeneratedtext. Inoneapproach,RAG-Sequence,themodelusesthesamedocument topredicteachtargettoken. Thesecondapproach,RAG-Token,canpredicteachtargettokenbased onadifferentdocument. Inthefollowing,weformallyintroducebothmodelsandthendescribethe p andp components,aswellasthetraininganddecodingprocedure. âŒ˜ âœ“ 2.1 Models RAG-Sequ",46,mechanism,What assumption does the RAG-Sequence model make about retrieved documents?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,535," (y x,z) = p (z x) p (y x,z,y ) RAG-Sequence | â‡¡ âŒ˜ | âœ“ | âŒ˜ | âœ“ i | 1:i  1 z 2 topX-k(p( Â·| x)) z 2 topX-k(p( Â·| x)) Y i RAG-TokenModel IntheRAG-Tokenmodelwecandrawadifferentlatentdocumentforeach targettokenandmarginalizeaccordingly. Thisallowsthegeneratortochoosecontentfromseveral documents when producing an answer. Concretely, the top K documents are retrieved using the retriever,andthenthegeneratorproducesadistributionforthenextoutputtokenforeachdocument, beforemarginalizing,andrepeatingthepr",47,comparative,What capability does RAG-Token introduce compared to RAG-Sequence?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,536,"achdocument, beforemarginalizing,andrepeatingtheprocesswiththefollowingoutputtoken,Formally,wedefine: N p (y x) p (z x)p (y x,z ,y ) RAG-Token | â‡¡ âŒ˜ | âœ“ i | i 1:i  1 Y i z 2 topX-k(p( Â·| x)) Finally,wenotethatRAGcanbeusedforsequenceclassificationtasksbyconsideringthetargetclass asatargetsequenceoflengthone,inwhichcaseRAG-SequenceandRAG-Tokenareequivalent. 2.2 Retriever: DPR Theretrievalcomponentp (z x)isbasedonDPR[26]. DPRfollowsabi-encoderarchitecture: âŒ˜ | p âŒ˜ (z x) exp d(z) > q(x) d(z)=BERT d",48,process,How can RAG be adapted for sequence classification?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,537,"tecture: âŒ˜ | p âŒ˜ (z x) exp d(z) > q(x) d(z)=BERT d (z), q(x)=BERT q (x) | / whered(z)isadenserepresentationofadocumentproducedbyaBERT BASE documentencoder[8], andq(x)aqueryrepresentationproducedbyaqueryencoder,alsobasedonBERT . Calculating BASE top-k(p ( x)),thelistofkdocumentszwithhighestpriorprobabilityp (z x),isaMaximumInner âŒ˜ âŒ˜ Â·| | ProductSearch(MIPS)problem,whichcanbeapproximatelysolvedinsub-lineartime[23]. Weuse apre-trainedbi-encoderfromDPRtoinitializeourretrieverandtobuildthedocumenti",49,mechanism,What similarity operation is used to score query–document pairs in DPR?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,538,"mDPRtoinitializeourretrieverandtobuildthedocumentindex. This retrieverwastrainedtoretrievedocumentswhichcontainanswerstoTriviaQA[24]questionsand NaturalQuestions[29]. Werefertothedocumentindexasthenon-parametricmemory. 2.3 Generator: BART Thegeneratorcomponentp (y x,z,y )couldbemodelledusinganyencoder-decoder. Weuse âœ“ i 1:i 1 BART-large[32],apre-trainedse | q2seqtransformer[58]with400Mparameters.Tocombinetheinput xwiththeretrievedcontentzwhengeneratingfromBART,wesimplyconcatenatethem. BARTwas p",50,historical,On which datasets was the DPR retriever originally trained?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,539,"eratingfromBART,wesimplyconcatenatethem. BARTwas pre-trainedusingadenoisingobjectiveandavarietyofdifferentnoisingfunctions. Ithasobtained state-of-the-artresultsonadiversesetofgenerationtasksandoutperformscomparably-sizedT5 models[32]. WerefertotheBARTgeneratorparametersâœ“astheparametricmemoryhenceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what documentshouldberetrieved. Givenafine-tuningtrainingcorpusofinput/outputpairs(x ,y ),we",51,definition,What model is used as the generator in the RAG framework?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,540,"-tuningtrainingcorpusofinput/outputpairs(x ,y ),we j j 3 minimizethenegativemarginallog-likelihoodofeachtarget, logp(y x )usingstochastic j j | j gradientdescentwithAdam[28]. UpdatingthedocumentencoderBERT duringtrainingiscostlyas d P itrequiresthedocumentindextobeperiodicallyupdatedasREALMdoesduringpre-training[20]. We do not find this step necessary for strong performance, and keep the document encoder (and index)fixed,onlyfine-tuningthequeryencoderBERT andtheBARTgenerator. q 2.5 Decoding Att",52,process,"Which components are fine-tuned during training, and which are kept fixed?"
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,543,"itywithp (z x)andthensumtheprobabilitiesacross âŒ˜ | beamsforthemarginals. Werefertothisdecodingprocedureasâ€œThoroughDecoding.â€ Forlonger outputsequences, Y canbecomelarge,requiringmanyforwardpasses. Formoreefficientdecoding, | | wecanmakeafurtherapproximationthatp (y x,z ) 0whereywasnotgeneratedduringbeam âœ“ i | â‡¡ searchfromx,z . ThisavoidstheneedtorunadditionalforwardpassesoncethecandidatesetY has i beengenerated. Werefertothisdecodingprocedureasâ€œFastDecoding.â€ 3 Experiments WeexperimentwithRAGina",53,comparative,What distinguishes Fast Decoding from Thorough Decoding in RAG?
3,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,573,"thuswell-suitedforwordoverlap-basedretrieval. Differentiableretrievalimproves resultsonallothertasks,especiallyforOpen-DomainQA,whereitiscrucial. Indexhot-swapping Anadvantageofnon-parametricmemorymodelslikeRAGisthatknowledge canbeeasilyupdatedattesttime. Parametric-onlymodelslikeT5orBARTneedfurthertrainingto updatetheirbehaviorastheworldchanges. Todemonstrate,webuildanindexusingtheDrQA[5] WikipediadumpfromDecember2016andcompareoutputsfromRAGusingthisindextothenewer indexfromourmainresults(Decem",54,mechanism,How can RAG models update world knowledge without retraining?
