chunk_id,doc_id,text
0,Attention Is All You Need.pdf,Attention Is All You Need AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗ GoogleBrain GoogleBrain GoogleResearch GoogleResearch avaswani@google.com noam@google.com nikip@google.com usz@google.com LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗ GoogleResearch UniversityofToronto GoogleBrain llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com IlliaPolosukhin∗ ‡ illia.polosukhin@gmail.com Abstract Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetwo
1,Attention Is All You Need.pdf,"basedoncomplexrecurrentor convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions entirely. Experiments on two machine translation tasks show these models to besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly less time to train. Our model achi"
2,Attention Is All You Need.pdf,"ngsignificantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask, ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.0after trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe bestmodelsfromtheliterature. 1 Introduction Recurrentneuralnetworks,longshort-termmemory[12]andgatedrecurrent[7]neuralnetw"
3,Attention Is All You Need.pdf,"short-termmemory[12]andgatedrecurrent[7]neuralnetworks inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand transductionproblemssuchaslanguagemodelingandmachinetranslation[29,2,5]. Numerous effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder architectures[31,21,13]. ∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthe"
4,Attention Is All You Need.pdf,"hisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and efficientinferenceandvisualizations."
5,Attention Is All You Need.pdf,"lcodebase,and efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating ourresearch. †WorkperformedwhileatGoogleBrain. ‡WorkperformedwhileatGoogleResearch. 31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.
Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput sequences. Aligningthepositionstosteps"
6,Attention Is All You Need.pdf,"utandoutput sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently t t−1 sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved significantimprovementsincomputationalefficiencythroughfactorizationtricks[18]andconditional computation[26],whilealsoimprovingmodel"
7,Attention Is All You Need.pdf,"onditional computation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental constraintofsequentialcomputation,however,remains. Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc- tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein theinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms areusedinconjunctionwitharecurrentnetwork. InthisworkweproposetheTransformer,amodelarchitecturees"
8,Attention Is All You Need.pdf,"isworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput. TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs. 2 Background ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbui"
9,Attention Is All You Need.pdf,"allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolut"
10,Attention Is All You Need.pdf,"erations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2. Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepr"
11,Attention Is All You Need.pdf,"lentailmentandlearningtask-independentsentencerepresentations[4,22,23,19]. End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[28]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution. Inthefollowingsections,"
12,Attention Is All You Need.pdf," alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate self-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8]. 3 ModelArchitecture Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29]. Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence 1 n of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output 1 n sequence(y ,...,y )ofsymbolsoneelementatati"
13,Attention Is All You Need.pdf,"ut 1 n sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive 1 m [9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively. 3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. Thefirst"
14,Attention Is All You Need.pdf,"al layers. Each layer has two sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- 2
Figure1: TheTransformer-modelarchitecture. wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswell"
15,Attention Is All You Need.pdf,"residualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512. model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention sub-layer in the decoder stack to prevent pos"
16,Attention Is All You Need.pdf,"tion sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani. 3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum ofthevalues,wheretheweightassignedtoeachvalueiscomputedby"
17,Attention Is All You Need.pdf,"lues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention Wecallourparticularattention""ScaledDot-ProductAttention""(Figure2). Theinputconsistsof queriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe k v 3
ScaledDot-ProductAttention Multi-HeadAttention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attentionlayersrunninginparallel. √ querywi"
18,Attention Is All You Need.pdf,"everal attentionlayersrunninginparallel. √ querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values. Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas: QKT Attention(Q,K,V)=softmax( √ )V (1) d k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention. Dot-productattentionisidenti"
19,Attention Is All You Need.pdf," plicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode. Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutsc"
20,Attention Is All You Need.pdf,"ttentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 . dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries, model wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned linearprojections"
21,Attention Is All You Need.pdf,"alueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v output values. These are concatenated and once again projected, resulting in the final values, as depictedinFigure2. Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis."
22,Attention Is All You Need.pdf,"s. Withasingleattentionhead,averaginginhibitsthis. 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom variableswithmean0andvariance1.Thentheirdotproduct,q·k= (cid:80)dk q k ,hasmean0andvarianced . i=1 i i k 4
MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h wherehead =Attention(QWQ,KWK,VWV) i i i i WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv i i i andWO ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layer"
23,Attention Is All You Need.pdf,"this work we employ h = 8 parallel attention layers, or heads. For each of these we use d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost k v model issimilartothatofsingle-headattentionwithfulldimensionality. 3.2.3 ApplicationsofAttentioninourModel TheTransformerusesmulti-headattentioninthreedifferentways: • In""encoder-decoderattention""layers,thequeriescomefromthepreviousdecoderlayer, andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery positioninthed"
24,Attention Is All You Need.pdf,"outputoftheencoder. Thisallowsevery positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31,2,8]. • Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe encoder. • Similarly,self-attentionlayersinthedecoder"
25,Attention Is All You Need.pdf,"oder. • Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2. 3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercont"
26,Attention Is All You Need.pdf,"b-layers,eachofthelayersinourencoderanddecodercontainsafully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This consistsoftwolineartransformationswithaReLUactivationinbetween. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner"
27,Attention Is All You Need.pdf,"lity of input and output is d = 512, and the inner-layer has dimensionality model d =2048. ff 3.4 EmbeddingsandSoftmax Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor- model mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax lineartransformation,similarto[24]. I"
28,Attention Is All You Need.pdf,"pre-√softmax lineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d . model 3.5 PositionalEncoding Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe tokensinthesequence. Tothisend,weadd""positionalencodings""totheinputembeddingsatthe 5
Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations fordifferentlayertypes. nistheseq"
29,Attention Is All You Need.pdf,"entialoperations fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention. LayerType ComplexityperLayer Sequential MaximumPathLength Operations Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n) Convolutional O(k·n·d2) O(1) O(log (n)) k Self-Attention(restricted) O(r·n·d) O(1) O(n/r) bottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond model astheembedding"
30,Attention Is All You Need.pdf,"ncodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings, learnedandfixed[8]. Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We chosethisfunctionbecausewehypothesizeditwouldallowthem"
31,Attention Is All You Need.pdf,"ethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof pos+k PE . pos Wealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered duringtraining. 4 WhySelf-Attention In this section we compare various asp"
32,Attention Is All You Need.pdf,"f-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden 1 n 1 n i i layerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe considerthreedesiderata. Oneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan be"
33,Attention Is All You Need.pdf,"erlayer. Anotheristheamountofcomputationthatcan beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired. Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range dependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto traverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput andoutputsequences,theeasieritistolearn"
34,Attention Is All You Need.pdf,"intheinput andoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe differentlayertypes. AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence length n is smaller than the"
35,Attention Is All You Need.pdf,"layerswhenthesequence length n is smaller than the representation dimensionality d, which is most often the case with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin 6
theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum pathlengthtoO(n/r). Weplan"
36,Attention Is All You Need.pdf,"wouldincreasethemaximum pathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework. Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput positions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, orO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths k betweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan recurrent layers, by a factor of k. Separable convolutions ["
37,Attention Is All You Need.pdf,"layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, theapproachwetakeinourmodel. Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions fromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention headsclearlylearntoperformdifferent"
38,Attention Is All You Need.pdf,"idualattention headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic andsemanticstructureofthesentences. 5 Training Thissectiondescribesthetrainingregimeforourmodels. 5.1 TrainingDataandBatching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- targetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT 2014Engl"
39,Attention Is All You Need.pdf,"sh-French,weusedthesignificantlylargerWMT 2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece vocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 targettokens. 5.2 HardwareandSchedule Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We trained"
40,Attention Is All You Need.pdf,"er,eachtrainingsteptookabout0.4seconds. We trainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe bottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps (3.5days). 5.3 Optimizer WeusedtheAdamoptimizer[17]withβ =0.9,β =0.98and(cid:15)=10−9. Wevariedthelearning 1 2 rateoverthecourseoftraining,accordingtotheformula: lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3) model Thiscorrespondstoincreasingthelearningratelinearlyforthefirs"
41,Attention Is All You Need.pdf,"pondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused warmup_steps=4000. 5.4 Regularization Weemploythreetypesofregularizationduringtraining: ResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe sub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe positionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof "
42,Attention Is All You Need.pdf,"deranddecoderstacks. Forthebasemodel,weusearateof P =0.1. drop 7
Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost. BLEU TrainingCost(FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[15] 23.75 Deep-Att+PosUnk[32] 39.2 1.0·1020 GNMT+RL[31] 24.6 39.92 2.3·1019 1.4·1020 ConvS2S[8] 25.16 40.46 9.6·1018 1.5·1020 MoE[26] 26.03 40.56 2.0·1019 1.2·1020 Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1"
43,Attention Is All You Need.pdf,"19 1.2·1020 Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1020 GNMT+RLEnsemble[31] 26.30 41.16 1.8·1020 1.1·1021 ConvS2SEnsemble[8] 26.36 41.29 7.7·1019 1.2·1021 Transformer(basemodel) 27.3 38.1 3.3·1018 Transformer(big) 28.4 41.0 2.3·1019 LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This ls hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore. 6 Results 6.1 MachineTranslation OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel"
44,Attention Is All You Need.pdf,"sh-to-Germantranslationtask,thebigtransformermodel(Transformer(big) inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0 BLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof thecompetitivemodels. OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreo"
45,Attention Is All You Need.pdf,"enchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0, outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe previousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused dropoutrateP =0.1,insteadof0.3. drop Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which werewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[31]. Thesehype"
46,Attention Is All You Need.pdf,"abeamsizeof4andlengthpenaltyα = 0.6[31]. Thesehyperparameters werechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring inferencetoinputlength+50,butterminateearlywhenpossible[31]. Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel architecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained single-precisionfloating-pointcapa"
47,Attention Is All You Need.pdf,"eofthesustained single-precisionfloating-pointcapacityofeachGPU5. 6.2 ModelVariations ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno checkpointaveraging. WepresenttheseresultsinTable3. InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions, keeping the amount of computa"
48,Attention Is All You Need.pdf,"yandvaluedimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads. 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively. 8
Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase model. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed perplexitiesareper-wordpiece,accordingtoourbyte-pai"
49,Attention Is All You Need.pdf,"erplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto per-wordperplexities. train PPL BLEU params N d d h d d P (cid:15) model ff k v drop ls steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 4 128 128 5.00 25.5 (A) 16 32 32 4.91 25.8 32 16 16 5.01 25.4 16 5.16 25.1 58 (B) 32 5.01 25.4 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0"
50,Attention Is All You Need.pdf,".66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 (D) 0.0 4.67 25.3 0.2 5.47 25.7 (E) positionalembeddinginsteadofsinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This k suggests that determining compatibility is not easy and that a more sophisticated compatibility functionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected, biggermodelsarebetter,anddropo"
51,Attention Is All You Need.pdf,"(D)that,asexpected, biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour sinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical resultstothebasemodel. 7 Conclusion Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith multi-headedself-attention. For translation tasks, the Transformer can be trained significan"
52,Attention Is All You Need.pdf,"n tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest modeloutperformsevenallpreviouslyreportedensembles. Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand toinvestigatelocal,re"
53,Attention Is All You Need.pdf,"utmodalitiesotherthantextand toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs suchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. Acknowledgements Wearegratefulto NalKalchbrennerand StephanGouwsfor theirfruitful comments,correctionsandinspiration. 9
References [1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. L"
54,Attention Is All You Need.pdf,"1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint arXiv:1607.06450,2016. [2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly learningtoalignandtranslate. CoRR,abs/1409.0473,2014. [3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural machinetranslationarchitectures. CoRR,abs/1703.03906,2017. [4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine reading. arXivpreprintarXiv:1601"
55,Attention Is All You Need.pdf,"etworksformachine reading. arXivpreprintarXiv:1601.06733,2016. [5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk, andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation. CoRR,abs/1406.1078,2014. [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprintarXiv:1610.02357,2016. [7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation ofgatedrecurrentneuraln"
56,Attention Is All You Need.pdf,"engio. Empiricalevaluation ofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014. [8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu- tionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017. [9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,2013. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Co"
57,Attention Is All You Need.pdf,"im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages770–778,2016. [11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin recurrentnets: thedifficultyoflearninglong-termdependencies,2001. [12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,1997. [13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring thelimitsoflanguagemodeling. arXivpr"
58,Attention Is All You Need.pdf,"Wu. Exploring thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016. [14] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference onLearningRepresentations(ICLR),2016. [15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 2017. [16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks. InInternationalConferenceonLearningRe"
59,Attention Is All You Need.pdf,"ionnetworks. InInternationalConferenceonLearningRepresentations,2017. [17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015. [18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint arXiv:1703.10722,2017. [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130,2017. [20] SamyBengioŁukaszKaiser. Canactiveme"
60,Attention Is All You Need.pdf,"130,2017. [20] SamyBengioŁukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural InformationProcessingSystems,(NIPS),2016. 10
[21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention- basedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015. [22] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention model. InEmpiricalMethodsinNaturalLanguageProcessing,2016. [23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmo"
61,Attention Is All You Need.pdf,"s,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive summarization. arXivpreprintarXiv:1705.04304,2017. [24] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv preprintarXiv:1608.05859,2016. [25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords withsubwordunits. arXivpreprintarXiv:1508.07909,2015. [26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton, andJeffDean. Outrageouslylargeneuralnetworks: "
62,Attention Is All You Need.pdf,"on, andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts layer. arXivpreprintarXiv:1701.06538,2017. [27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi- nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine LearningResearch,15(1):1929–1958,2014. [28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, edi"
63,Attention Is All You Need.pdf,"D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates, Inc.,2015. [29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural networks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014. [30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015. [31] Yonghui Wu, Mike Schuster"
64,Attention Is All You Need.pdf,"bs/1512.00567,2015. [31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine translationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint arXiv:1609.08144,2016. [32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016. 11
"
65,Large Language Models A Survey.pdf,"Large Language Models: A Survey Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu Richard Socher, Xavier Amatriain, Jianfeng Gao Abstract—Large Language Models (LLMs) have drawn a that have different starting points and velocity: statistical lan- lot of attention due to their strong performance on a wide guage models, neural language models, pre-trained language range of natural language tasks, since the release of ChatGPT models and LLMs. in November 2022. LLMs’ ability of general-"
66,Large Language Models A Survey.pdf," LLMs. in November 2022. LLMs’ ability of general-purpose language understanding and generation is acquired by training billions of Statisticallanguagemodels(SLMs)viewtextasasequence model’sparametersonmassiveamountsoftextdata,aspredicted of words, and estimate the probability of text as the product by scaling laws [1], [2]. The research area of LLMs, while very of their word probabilities. The dominating form of SLMs recent, is evolving rapidly in many different ways. In this paper, are Markov "
67,Large Language Models A Survey.pdf,"in many different ways. In this paper, are Markov chain models known as the n-gram models, we review some of the most prominent LLMs, including three which compute the probability of a word conditioned on its popular LLM families (GPT, LLaMA, PaLM), and discuss their immediate proceeding n−1 words. Since word probabilities characteristics, contributions and limitations. We also give an are estimated using word and n-gram counts collected from overview of techniques developed to build, and augmen"
68,Large Language Models A Survey.pdf,"rview of techniques developed to build, and augment LLMs. text corpora, the model needs to deal with data sparsity (i.e., We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation assigning zero probabilities to unseen words or n-grams) by metrics, and compare the performance of several popular LLMs using smoothing, where some probability mass of the model on a set of representative benchmarks. Finally, we conclude is reserved for u"
69,Large Language Models A Survey.pdf,"benchmarks. Finally, we conclude is reserved for unseen n-grams [12]. N-gram models are the paper by discussing open challenges and future research widely used in many NLP systems. However, these models directions. are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity. I. INTRODUCTION Earlyneurallanguagemodels(NLMs)[13],[14],[15],[16] Language modeling is a long-standing research topic, dat- deal with data sparsity by mapping word"
70,Large Language Models A Survey.pdf,"opic, dat- deal with data sparsity by mapping words to low-dimensional ing back to the 1950s with Shannon’s application of informa- continuous vectors (embedding vectors) and predict the next tion theory to human language, where he measured how well word based on the aggregation of the embedding vectors of simple n-gram language models predict or compress natural its proceeding words using neural networks. The embedding language text [3]. Since then, statistical language modeling vectors learned"
71,Large Language Models A Survey.pdf,"hen, statistical language modeling vectors learned by NLMs define a hidden space where the became fundamental to many natural language understanding semantic similarity between vectors can be readily computed and generation tasks, ranging from speech recognition, ma- as their distance. This opens the door to computing semantic chine translation, to information retrieval [4], [5], [6]. similarityofanytwoinputsregardlesstheirforms(e.g.,queries vs. documents in Web search [17], [18], sentences in d"
72,Large Language Models A Survey.pdf,"documents in Web search [17], [18], sentences in different The recent advances on transformer-based large language languagesinmachinetranslation[19],[20])ormodalities(e.g., models (LLMs), pretrained on Web-scale text corpora, signif- imageandtextinimagecaptioning[21],[22]).EarlyNLMsare icantly extended the capabilities of language models (LLMs). task-specific models, in that they are trained on task-specific For example, OpenAI’s ChatGPT and GPT-4 can be used not data and their learned hidden sp"
73,Large Language Models A Survey.pdf,"4 can be used not data and their learned hidden space is task-specific. only for natural language processing, but also as general task solvers to power Microsoft’s Co-Pilot systems, for instance, Pre-trained language models (PLMs), unlike early NLMs, can follow human instructions of complex new tasks per- are task-agnostic. This generality also extends to the learned forming multi-step reasoning when needed. LLMs are thus hidden embedding space. The training and inference of PLMs becoming the ba"
74,Large Language Models A Survey.pdf,"The training and inference of PLMs becoming the basic building block for the development of follows the pre-training and fine-tuning paradigm, where lan- general-purpose AI agents or artificial general intelligence guage models with recurrent neural networks [23] or trans- (AGI). formers[24],[25],[26]arepre-trainedonWeb-scaleunlabeled textcorporaforgeneraltaskssuchaswordprediction,andthen As the field of LLMs is moving fast, with new findings, finetuned to specific tasks using small amounts of ("
75,Large Language Models A Survey.pdf,"netuned to specific tasks using small amounts of (labeled) models and techniques being published in a matter of months task-specific data. Recent surveys on PLMs include [8], [27], or weeks [7], [8], [9], [10], [11], AI researchers and practi- [28]. tioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks. This paper Large language models (LLMs) mainly refer to gives a timely survey of the recent advances on LLMs. We transformer-based neura"
76,Large Language Models A Survey.pdf,"ecent advances on LLMs. We transformer-based neural language models 1 that contain hope this survey will prove a valuable and accessible resource tens to hundreds of billions of parameters, which are pre- for students, researchers and developers. trained on massive text data, such as PaLM [31], LLaMA [32], and GPT-4 [33], as summarized in Table III. Compared LLMsarelarge-scale,pre-trained,statisticallanguagemod- els based on neural networks. The recent success of LLMs is 1Recently, several very "
77,Large Language Models A Survey.pdf,"recent success of LLMs is 1Recently, several very promising non-transformer LLMs have been pro- an accumulation of decades of research and development of posed, such as the LLMs based on structured state space models [29], [30]. language models, which can be categorized into four waves SeeSectionVIIformoredetails. 4202 beF 02 ]LC.sc[ 2v69160.2042:viXra
Summarization Multi choice QA Simplification Boolean QA Reading Comprehension Arithmetic XNLI Symbolic Crosslingual QA Comprehension Translation "
78,Large Language Models A Survey.pdf,ymbolic Crosslingual QA Comprehension Translation Common Sense Logical Self-refinement Crosslingual Tasks Self-cirtisim Reasoning Multilingual Completion Self-improvement Wikipedia QA Multi choice QA Few-shot Ste s p o l b v y in s g tep Function Calling Turn based Task definition Symbolic Physical acting Task Tool planning World Instruction Pos/Neg example reference Virtual acting decomposition Kno u w ti l l e iz d a g t e io b n ase knowledge API calling following In-context As p s la ig n n 
79,Large Language Models A Survey.pdf,"API calling following In-context As p s la ig n n n m in e g nt Tool learning utilization Coding Interacting with users Basic Emerging Augmented LLM Capabilities Fig. 1: LLM Capabilities. to PLMs, LLMs are not only much larger in model size, but LLMs are used, and augmented for real-world applications also exhibit stronger language understanding and generation SectionsVandVIreviewpopulardatasetsandbenchmarksfor abilities, and more importantly, emergent abilities that are evaluatingLLMs,andsummar"
80,Large Language Models A Survey.pdf,"ergent abilities that are evaluatingLLMs,andsummarizethereportedLLMevaluation not present in smaller-scale language models. As illustrated results. Finally, Section VII concludes the paper by summa- in Fig. 1, these emergent abilities include (1) in-context rizing the challenges and future research directions. learning, where LLMs learn a new task from a small set of examples presented in the prompt at inference time, (2) instruction following, where LLMs, after instruction tuning, II. LARGELANG"
81,Large Language Models A Survey.pdf,"here LLMs, after instruction tuning, II. LARGELANGUAGEMODELS can follow the instructions for new types of tasks without In this section we start with a review of early pre-trained using explicit examples, and (3) multi-step reasoning, where neural language models as they are the base of LLMs, and LLMs can solve a complex task by breaking down that task then focus our discussion on three families of LLMs: GPT, into intermediate reasoning steps as demonstrated in the LlaMA, and PaLM. Table I provi"
82,Large Language Models A Survey.pdf,"demonstrated in the LlaMA, and PaLM. Table I provides an overview of some of chain-of-thought prompt [34]. LLMs can also be augmented these models and their characteristics. by using external knowledge and tools [35], [36] so that they can effectively interact with users and environment [37], and continually improve itself using feedback data collected A. Early Pre-trained Neural Language Models through interactions (e.g. via reinforcement learning with Language modeling using neural networks wa"
83,Large Language Models A Survey.pdf,"ng with Language modeling using neural networks was pioneered human feedback (RLHF)). by[38],[39],[40].Bengioetal.[13]developedoneofthefirst Through advanced usage and augmentation techniques, neurallanguagemodels(NLMs)thatarecomparableton-gram LLMscanbedeployedasso-calledAIagents:artificialentities models. Then, [14] successfully applied NLMs to machine thatsensetheirenvironment,makedecisions,andtakeactions. translation. The release of RNNLM (an open source NLM Previousresearchhasfocusedondevel"
84,Large Language Models A Survey.pdf," open source NLM Previousresearchhasfocusedondevelopingagentsforspecific toolkit) by Mikolov [41], [42] helped significantly popularize tasks and domains. The emergent abilities demonstrated by NLMs.Afterwards,NLMsbasedonrecurrentneuralnetworks LLMs make it possible to build general-purpose AI agents (RNNs) and their variants, such as long short-term memory basedonLLMs.WhileLLMsaretrainedtoproduceresponses (LSTM)[19]andgatedrecurrentunit(GRU)[20],werewidely instaticsettings,AIagentsneedtotakeact"
85,Large Language Models A Survey.pdf,",werewidely instaticsettings,AIagentsneedtotakeactionstointeractwith usedformanynaturallanguageapplicationsincludingmachine dynamic environment. Therefore, LLM-based agents often translation, text generation and text classification [43]. need to augment LLMs to e.g., obtain updated information Then, the invention of the Transformer architecture [44] fromexternalknowledgebases,verifywhetherasystemaction marks another milestone in the development of NLMs. By produces the expected result, and cope "
86,Large Language Models A Survey.pdf,"f NLMs. By produces the expected result, and cope with when things do applying self-attention to compute in parallel for every word not go as expected, etc. We will discuss in detail LLM-based in a sentence or document an “attention score” to model the agents in Section IV. influence each word has on another, Transformers allow for Intherestofthispaper,SectionIIpresentsanoverviewof muchmoreparallelizationthanRNNs,whichmakesitpossible stateoftheartofLLMs,focusingonthreeLLMfamilies(GPT, to efficie"
87,Large Language Models A Survey.pdf,"tofLLMs,focusingonthreeLLMfamilies(GPT, to efficiently pre-train very big language models on large LLaMA and PaLM) and other representative models. Section amounts of data on GPUs. These pre-trained language models III discusses how LLMs are built. Section IV discusses how (PLMs) can be fine-tuned for many downstream tasks.
Paper Strcuture II Large Language Models Early Pre-trained III HOW LLMS ARE BUILT A Language Models Dominant LLM B Large Language A Architectures E Model Pre-training Model F"
88,Large Language Models A Survey.pdf,"guage A Architectures E Model Pre-training Model Families B Data Cleaning F Fine-tuning and C Other Representative Instruction Tuning LLMs C Tokenizations G Alignment D Positional Encoding H Decoding Strategies I HOW LLMS ARE USED AND AUGMENTED I Cost-Effective Training/Inference, A LLM limitations Adaptation & Compression B Using LLMs: Prompt Design and Engineering C Augmenting LLMs through V POPULAR DATASETS FOR LLMS external knowledge - RAG A Datasets for Basic Tasks: language D Using Externa"
89,Large Language Models A Survey.pdf,"Datasets for Basic Tasks: language D Using External Tools modeling/understanding/generation E LLM Agents B Datasets i f n o s r t r E u m ct e io rg n e f n o t l : l o IC w L in , g reasoning, C Datasets for Augmented: using external knowledge/tools VI PROMINENT LLMS’ PERFORMANCE ON BENCHMARKS A Popular Metrics for Evaluating LLMs VII CHALLENGES AND FUTURE DIRECTIONS B LLMs’ Performance on Different Tasks A Smaller and more efficient Language Models B New Post-attention Architectural Paradigms "
90,Large Language Models A Survey.pdf,"dels B New Post-attention Architectural Paradigms C Multi-modal Models D Improved LLM Usage and Augmentation techniques D Security and Ethical/Responsible AI Fig. 2: The paper structure. WegroupearlypopularTransformer-basedPLMs,basedon BERT (Birectional Encoder Representations from Trans- their neural architectures, into three main categories: encoder- formers) [24] is one of the most widely used encoder-only only, decoder-only, and encoder-decoder models. Comprehen- language models. BERT consis"
91,Large Language Models A Survey.pdf,"er models. Comprehen- language models. BERT consists of three modules: (1) an sive surveys of early PLMs are provided in [43], [28]. embedding module that converts input text into a sequence of embedding vectors, (2) a stack of Transformer encoders thatconvertsembeddingvectorsintocontextualrepresentation 1)Encoder-onlyPLMs: Asthenamesuggests,theencoder- vectors, and (3) a fully connected layer that converts the onlymodelsonlyconsistofanencodernetwork.Thesemodels representation vectors (at the fi"
92,Large Language Models A Survey.pdf,"work.Thesemodels representation vectors (at the final layer) to one-hot vectors. are originally developed for language understanding tasks, BERT is pre-trained uses two objectives: masked language such as text classification, where the models need to predict a modeling(MLM)andnextsentenceprediction.Thepre-trained classlabelforaninputtext.Representativeencoder-onlymod- BERT model can be fine-tuned by adding a classifier layer els include BERT and its variants, e.g., RoBERTa, ALBERT, for many lang"
93,Large Language Models A Survey.pdf,"its variants, e.g., RoBERTa, ALBERT, for many language understanding tasks, ranging from text DeBERTa, XLM, XLNet, UNILM, as to be described below.
TABLE I: High-level Overview of Popular Language Models Type ModelName #Parameters Release BaseModels Open #Tokens Trainingdataset Source BERT 110M,340M 2018 - ✓ 137B BooksCorpus,EnglishWikipedia RoBERTa 355M 2019 - ✓ 2.2T BooksCorpus, English Wikipedia, CC-NEWS, STORIES(asubsetofCommonCrawl),Reddit ALBERT 12M, 18M, 60M, 2019 - ✓ 137B BooksCorpus,Eng"
94,Large Language Models A Survey.pdf,"LBERT 12M, 18M, 60M, 2019 - ✓ 137B BooksCorpus,EnglishWikipedia Encoder-Only 235M DeBERTa - 2020 - ✓ - BooksCorpus,EnglishWikipedia,STORIES,Red- ditcontent XLNet 110M,340M 2019 - ✓ 32.89B BooksCorpus, English Wikipedia, Giga5, Com- monCrawl,ClueWeb2012-B GPT-1 120M 2018 - ✓ 1.3B BooksCorpus Decoder-only GPT-2 1.5B 2019 - ✓ 10B Redditoutbound T5(Base) 223M 2019 - ✓ 156B CommonCrawl MT5(Base) 300M 2020 - ✓ - New Common Crawl-based dataset in 101 lan- Encoder-Decoder guages(mCommonCrawl) BART(Base)"
95,Large Language Models A Survey.pdf,"n- Encoder-Decoder guages(mCommonCrawl) BART(Base) 139M 2019 - ✓ - Corruptingtext GPT-3 125M, 350M, 2020 × 300B Common Crawl (filtered), WebText2, Books1, 760M, 1.3B, 2.7B, Books2,Wikipedia 6.7B,13B,175B CODEX 12B 2021 GPT ✓ - PublicGitHubsoftwarerepositories GPTFamily WebGPT 760M,13B,175B 2021 GPT-3 × - ELI5 GPT-4 1.76T 2023 - × 13T - LLaMA1 7B,13B,33B,65B 2023 - ✓ 1T,1.4T Onlinesources LLaMA2 7B,13B,34B,70B 2023 - ✓ 2T Onlinesources Alpaca 7B 2023 LLaMA1 ✓ - GPT-3.5 Vicuna-13B 13B 2023 LLaMA1 "
96,Large Language Models A Survey.pdf,"023 LLaMA1 ✓ - GPT-3.5 Vicuna-13B 13B 2023 LLaMA1 ✓ - GPT-3.5 Koala 13B 2023 LLaMA ✓ - Dialoguedata LLaMAFamily Mistral-7B 7.3B 2023 ✓ - - CodeLlama 34 2023 LLaMA2 ✓ 500B Publiclyavailablecode LongLLaMA 3B,7B 2023 OpenLLaMA ✓ 1T - LLaMA-Pro-8B 8.3B 2024 LLaMA2-7B ✓ 80B Codeandmathcorpora TinyLlama-1.1B 1.1B 2024 LLaMA1.1B ✓ 3T SlimPajama,Starcoderdata PaLM 8B,62B,540B 2022 - × 780B Webdocuments,books,Wikipedia,conversations, GitHubcode U-PaLM 8B,62B,540B 2022 - × 1.3B Webdocuments,books,Wikipedi"
97,Large Language Models A Survey.pdf,"62B,540B 2022 - × 1.3B Webdocuments,books,Wikipedia,conversations, GitHubcode PaLM-2 340B 2023 - ✓ 3.6T Webdocuments,books,code,mathematics,con- PaLMFamily versationaldata Med-PaLM 540B 2022 PaLM × 780B HealthSearchQA,MedicationQA,LiveQA Med-PaLM2 - 2023 PaLM2 × - MedQA,MedMCQA,HealthSearchQA,LiveQA, MedicationQA FLAN 137B 2021 LaMDA-PT ✓ - Webdocuments,code,dialogdata,Wikipedia Gopher 280B 2021 - × 300B MassiveText ERNIE4.0 10B 2023 - × 4TB Chinesetext Retro 7.5B 2021 - × 600B MassiveText LaMDA"
98,Large Language Models A Survey.pdf,"esetext Retro 7.5B 2021 - × 600B MassiveText LaMDA 137B 2022 - × 168B publicdialogdataandwebdocuments ChinChilla 70B 2022 - × 1.4T MassiveText Galactia-120B 120B 2022 - 450B CodeGen 16.1B 2022 - ✓ - THEPILE,BIGQUERY,BIGPYTHON OtherPopularLLMs BLOOM 176B 2022 - ✓ 366B ROOTS Zephyr 7.24B 2023 Mistral-7B ✓ 800B Syntheticdata Grok-0 33B 2023 - × - Onlinesource ORCA-2 13B 2023 LLaMA2 - 2001B - StartCoder 15.5B 2023 - ✓ 35B GitHub MPT 7B 2023 - ✓ 1T RedPajama,mCommonCrawl,S2ORC,Common Crawl Mixtral-8x"
99,Large Language Models A Survey.pdf,"dPajama,mCommonCrawl,S2ORC,Common Crawl Mixtral-8x7B 46.7B 2023 - ✓ - Instructiondataset Falcon180B 180B 2023 - ✓ 3.5T RefinedWeb Gemini 1.8B,3.25B 2023 ✓ - Web documents, books, and code, image data, audiodata,videodata DeepSeek-Coder 1.3B,6.7B,33B 2024 - ✓ 2T GitHub’sMarkdownandStackExchange DocLLM 1B,7B 2024 - × 2T IIT-CDIPTestCollection1.0,DocBank classification, question answering to language inference. A largermini-batchesandlearningrates.ALBERT[45]usestwo high-leveloverviewofBERTframework"
100,Large Language Models A Survey.pdf,"LBERT[45]usestwo high-leveloverviewofBERTframeworkisshowninFig3.As parameter-reductiontechniquestolowermemoryconsumption BERT significantly improved state of the art on a wide range and increase the training speed of BERT: (1) splitting the oflanguageunderstandingtaskswhenitwaspublished,theAI embedding matrix into two smaller matrices, and (2) using communitywasinspiredtodevelopmanysimilarencoder-only repeating layers split among groups. DeBERTa (Decoding- language models based on BERT. enhanced"
101,Large Language Models A Survey.pdf,"(Decoding- language models based on BERT. enhancedBERTwithdisentangledattention)[26]improvesthe BERT and RoBERTa models using two novel techniques. The RoBERTa [25] significantly improves the robustness of firstisthedisentangledattentionmechanism,whereeachword BERT using a set of model design choices and training strate- is represented using two vectors that encode its content and gies, such as modifying a few key hyperparameters, removing position, respectively, and the attention weights among "
102,Large Language Models A Survey.pdf,"on, respectively, and the attention weights among words thenext-sentencepre-trainingobjectiveandtrainingwithmuch
Fig. 3: Overall pre-training and fine-tuning procedures for BERT. Courtesy of [24] Fig. 5: Cross-lingual language model pretraining. The MLM objective is similar to BERT, but with continuous streams arecomputedusingdisentangledmatricesontheircontentsand of text as opposed to sentence pairs. The TLM objective relative positions, respectively. Second, an enhanced mask de- extends MLM to"
103,Large Language Models A Survey.pdf,"ively. Second, an enhanced mask de- extends MLM to pairs of parallel sentences. To predict a coderisusedtoincorporateabsolutepositionsinthedecoding maskedEnglishword,themodelcanattendtoboththeEnglish layer to predict the masked tokens in model pre-training. In sentence and its French translation, and is encouraged to align addition,anovelvirtualadversarialtrainingmethodisusedfor English and French representations. Courtesy of [47]. fine-tuningtoimprovemodels’generalization.ELECTRA[46] usesanewpr"
104,Large Language Models A Survey.pdf,"mprovemodels’generalization.ELECTRA[46] usesanewpre-trainingtask,knownasreplacedtokendetection (RTD),whichisempiricallyproventobemoresample-efficient than MLM. Instead of masking the input, RTD corrupts it by all permutations of the factorization order. UNILM (UNIfied replacingsometokenswithplausiblealternativessampledfrom pre-trained Language Model) [49] is pre-trained using three a small generator network. Then, instead of training a model typesoflanguagemodelingtasks:unidirectional,bidirectio"
105,Large Language Models A Survey.pdf,"soflanguagemodelingtasks:unidirectional,bidirectional, that predicts the original identities of the corrupted tokens, a and sequence-to-sequence prediction. This is achieved by discriminative model is trained to predict whether a token in employingasharedTransformernetworkandutilizingspecific thecorruptedinputwasreplacedbyageneratedsampleornot. self-attention masks to control what context the prediction is RTD is more sample-efficient than MLM because the former conditioned on, as illustrated in"
106,Large Language Models A Survey.pdf,"cause the former conditioned on, as illustrated in Fig 6. The pre-trained model isdefinedoverallinputtokensratherthanjustthesmallsubset can be fine-tuned for both natural language understanding and being masked out, as illustrated in Fig 4. generation tasks. Fig. 4: A comparison between replaced token detection and masked language modeling. Courtesy of [46]. Fig. 6: Overview of unified LM pre-training. The model XLMs [47] extended BERT to cross-lingual language parameters are shared across the L"
107,Large Language Models A Survey.pdf,"ingual language parameters are shared across the LM objectives (i.e., bidirec- models using two methods: (1) a unsupervised method that tionalLM,unidirectionalLM,andsequence-to-sequenceLM). only relies on monolingual data, and (2) a supervised method Courtesy of [49]. that leverages parallel data with a new cross-lingual language model objective, as illustrated in Fig 5. XLMs had obtained state-of-the-art results on cross-lingual classification, unsuper- 2)Decoder-only PLMs: Two of the most wide"
108,Large Language Models A Survey.pdf,"unsuper- 2)Decoder-only PLMs: Two of the most widely used visedandsupervisedmachinetranslation,atthetimetheywere decoder-only PLMs are GPT-1 and GPT-2, developed by proposed. OpenAI. These models lay the foundation to more powerful LLMs subsequently, i.e., GPT-3 and GPT-4. Therearealsoencoder-onlylanguagemodelsthatleverage the advantages of auto-regressive (decoder) models for model GPT-1 [50] demonstrates for the first time that good trainingandinference.TwoexamplesareXLNetandUNILM. performance"
109,Large Language Models A Survey.pdf,"inference.TwoexamplesareXLNetandUNILM. performanceoverawiderangeofnaturallanguagetaskscanbe XLNet [48] is based on Transformer-XL, pre-trained using a obtained by Generative Pre-Training (GPT) of a decoder-only generalized autoregressive method that enables learning bidi- Transformer model on a diverse corpus of unlabeled text in a rectional contexts by maximizing the expected likelihood over self-supervised learning fashion (i.e., next word/token predic-
tion), followed by discriminative fine-t"
110,Large Language Models A Survey.pdf,"n predic-
tion), followed by discriminative fine-tuning on each specific B. Large Language Model Families downstream task (with much fewer samples), as illustrated in Large language models (LLMs) mainly refer to Fig 7. GPT-1 paves the way for subsequent GPT models, with transformer-based PLMs that contain tens to hundreds each version improving upon the architecture and achieving ofbillionsofparameters.ComparedtoPLMsreviewedabove, better performance on various language tasks. LLMsarenotonlymuchl"
111,Large Language Models A Survey.pdf,"nce on various language tasks. LLMsarenotonlymuchlargerinmodelsize,butalsoexhibit stronger language understanding and generation and emergent abilities that are not present in smaller-scale models. In what follows, we review three LLM families: GPT, LLaMA, and PaLM, as illustrated in Fig 8. 1)The GPT Family: Generative Pre-trained Transform- ers (GPT) are a family of decoder-only Transformer-based language models, developed by OpenAI. This family con- sists of GPT-1, GPT-2, GPT-3, InstrucGPT, Ch"
112,Large Language Models A Survey.pdf," con- sists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4, CODEX, and WebGPT. Although early GPT models, such as GPT-1 and GPT-2, are open-source, recent models, such as GPT-3 and GPT-4, are close-source and can only be accessed Fig.7:High-leveloverviewofGPTpretraining,andfine-tuning via APIs. GPT-1 and GPT-2 models have been discussed in steps. Courtesy of OpenAI. the early PLM subsection. We start with GPT-3 below. GPT-3 [56] is a pre-trained autoregressive language model with 175 billion"
113,Large Language Models A Survey.pdf,"ned autoregressive language model with 175 billion parameters. GPT-3 is widely considered as the first LLM in that it not only is much larger than previous GPT-2 [51] shows that language models are able to learn PLMs, but also for the first time demonstrates emergent toperformspecificnaturallanguagetaskswithoutanyexplicit abilitiesthatarenotobservedinprevioussmallerPLMs.GPT- supervisionwhentrainedonalargeWebTextdatasetconsisting 3 shows the emergent ability of in-context learning, which of milli"
114,Large Language Models A Survey.pdf,"ent ability of in-context learning, which of millions of webpages. The GPT-2 model follows the model means GPT-3 can be applied to any downstream tasks without designs of GPT-1 with a few modifications: Layer normal- any gradient updates or fine-tuning, with tasks and few-shot ization is moved to the input of each sub-block, additional demonstrations specified purely via text interaction with the layernormalizationisaddedafterthefinalself-attentionblock, model. GPT-3 achieved strong performance "
115,Large Language Models A Survey.pdf,"onblock, model. GPT-3 achieved strong performance on many NLP initialization is modified to account for the accumulation on tasks, including translation, question-answering, and the cloze the residual path and scaling the weights of residual layers, tasks, as well as several ones that require on-the-fly reasoning vocabulary size is expanded to 50,25, and context size is or domain adaptation, such as unscrambling words, using a increased from 512 to 1024 tokens. novel word in a sentence, 3-digit "
116,Large Language Models A Survey.pdf,"to 1024 tokens. novel word in a sentence, 3-digit arithmetic. Fig 9 plots the performanceofGPT-3asafunctionofthenumberofexamples 3)Encoder-DecoderPLMs: In[52],Raffleetal.showsthat in in-context prompts. almost all NLP tasks can be cast as a sequence-to-sequence generationtask.Thus,anencoder-decoderlanguagemodel,by CODEX [57], released by OpenAI in March 2023, is a design, is a unified model in that it can perform all natural general-purpose programming model that can parse natural language under"
117,Large Language Models A Survey.pdf,"amming model that can parse natural language understanding and generation tasks. Representative language and generate code in response. CODEX is a de- encoder-decoder PLMs we will review below are T5, mT5, scendant of GPT-3, fine-tuned for programming applications MASS, and BART. on code corpora collected from GitHub. CODEX powers Microsoft’s GitHub Copilot. T5[52]isaText-to-TextTransferTransformer(T5)model, WebGPT[58]isanotherdescendantofGPT-3,fine-tunedto where transfer learning is effectively"
118,Large Language Models A Survey.pdf,"ine-tunedto where transfer learning is effectively exploited for NLP via an answer open-ended questions using a text-based web browser, introductionofaunifiedframeworkinwhichallNLPtasksare facilitating users to search and navigate the web. Specifically, castasatext-to-textgenerationtask.mT5[53]isamultilingual WebGPT is trained in three steps. The first is for WebGPT variant of T5, which is pre-trained on a new Common Crawl- to learn to mimic human browsing behaviors using human based dataset con"
119,Large Language Models A Survey.pdf,"n browsing behaviors using human based dataset consisting of texts in 101 languages. demonstration data. Then, a reward function is learned to predict human preferences. Finally, WebGPT is refined to MASS (MAsked Sequence to Sequence pre-training) [54] optimize the reward function via reinforcement learning and adopts the encoder-decoder framework to reconstruct a sen- rejection sampling. tence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked f"
120,Large Language Models A Survey.pdf,"he encoder takes a sentence with randomly masked fragment To enable LLMs to follow expected human instructions, (several consecutive tokens) as input, and the decoder predicts InstructGPT [59] is proposed to align language models with the masked fragment. In this way, MASS jointly trains the user intent on a wide range of tasks by fine-tuning with encoder and decoder for language embedding and generation, humanfeedback.Startingwithasetoflabeler-writtenprompts respectively. and prompts submitted "
121,Large Language Models A Survey.pdf,"rittenprompts respectively. and prompts submitted through the OpenAI API, a dataset of labeler demonstrations of the desired model behavior is BART [55] uses a standard sequence-to-sequence transla- collected. Then GPT-3 is fine-tuned on this dataset. Then, a tionmodelarchitecture.Itispre-trainedbycorruptingtextwith dataset of human-ranked model outputs is collected to further an arbitrary noising function, and then learning to reconstruct fine-tune the model using reinforcement learning. The me"
122,Large Language Models A Survey.pdf,"une the model using reinforcement learning. The method the original text. is known Reinforcement Learning from Human Feedback
WizardLM Giraffe WebGPT Tulu InstructGPT Guanaco CODEX Long LLaMA PaLM2 GPT3.5 Turbo Mistral Med-PaLM PaLM code-davinci Gorilla text-davinci Stable Beluga2 Med-PaLM2 PaLM-E GPT4 Turbo GPT4 Vision Vigogne Code LLaMA GPT3 U-PaLM FLAN-PaLM GPT2 Koala Baize GPT4 GPT1 Vicuna Alpaca GP T GPT Family LLaMA 1/2 Family PaLM Family Fig. 8: Popular LLM Families. launchofChatGPT(ChatG"
123,Large Language Models A Survey.pdf,"ig. 8: Popular LLM Families. launchofChatGPT(ChatGenerativePre-trainedTransformer) [60] on November 30, 2022. ChatGPT is chatbot that enables users to steer a conversation to complete a wide range of tasks such as question answering, information seeking, text summarization, and more. ChatGPT is powered by GPT-3.5 (and later by GPT-4), a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. GPT-4 [33] is the latest and most powerful L"
124,Large Language Models A Survey.pdf,"onse. GPT-4 [33] is the latest and most powerful LLM in the GPT family. Launched in March, 2023, GPT-4 is a multi- modal LLM in that it can take image and text as inputs and Fig. 9: GPT-3 shows that larger models make increasingly produce text outputs. While still less capable than humans efficient use of in-context information. It shows in-context in some of the most challenging real-world scenarios, GPT-4 learning performance on a simple task requiring the model to exhibits human-level perform"
125,Large Language Models A Survey.pdf,"equiring the model to exhibits human-level performance on various professional and remove random symbols from a word, both with and without academicbenchmarks,includingpassingasimulatedbarexam a natural language task description. Courtesy of [56]. with a score around the top 10% of test takers, as shown in Fig11.LikeearlyGPTmodels,GPT-4wasfirstpre-trainedto predict next tokens on large text corpora, and then fine-tuned withRLHFtoalignmodelbehaviorswithhuman-desiredones. (RLHF), as shown in 10. T"
126,Large Language Models A Survey.pdf,"rswithhuman-desiredones. (RLHF), as shown in 10. The resultant InstructGPT models have shown improvements in truthfulness and reductions in 2)TheLLaMAFamily: LLaMAisacollectionoffounda- toxic output generation while having minimal performance tion language models, released by Meta. Unlike GPT models, regressions on public NLP datasets. LLaMA models are open-source, i.e., model weights are released to the research community under a noncommercial license. Thus, the LLaMA family grows rapidly as th"
127,Large Language Models A Survey.pdf,"icense. Thus, the LLaMA family grows rapidly as these models are widely used by many research groups to develop betteropen-sourceLLMstocompetetheclosed-sourceonesor todeveloptask-specificLLMsformission-criticalapplications. ThefirstsetofLLaMAmodels[32]wasreleasedinFebru- ary 2023, ranging from 7B to 65B parameters. These models are pre-trained on trillions of tokens, collected from publicly availabledatasets.LLaMAusesthetransformerarchitectureof GPT-3,withafewminorarchitecturalmodifications,incl"
128,Large Language Models A Survey.pdf,"GPT-3,withafewminorarchitecturalmodifications,including (1) using a SwiGLU activation function instead of ReLU, (2) using rotary positional embeddings instead of absolute positional embedding, and (3) using root-mean-squared layer- Fig. 10: The high-level overview of RLHF. Courtesy of [59]. normalization instead of standard layer-normalization. The open-source LLaMA-13B model outperforms the proprietary GPT-3 (175B) model on most benchmarks, making it a good The most important milestone of LLM d"
129,Large Language Models A Survey.pdf,"ng it a good The most important milestone of LLM development is the baseline for LLM research.
collected from ShareGPT. Preliminary evaluation using GPT- 4 as a evaluator shows that Vicuna-13B achieves more than 90% quality of OpenAI’s ChatGPT, and Google’s Bard while outperformingothermodelslikeLLaMAandStanfordAlpaca in more than 90% of cases. 13 shows the relative response quality of Vicuna and a few other well-known models by GPT-4.AnotheradvantageofVicuna-13Bisitsrelativelimited computationa"
130,Large Language Models A Survey.pdf,"ntageofVicuna-13Bisitsrelativelimited computationaldemandformodeltraining.Thetrainingcostof Vicuna-13B is merely $300. Fig. 11: GPT-4 performance on academic and professional Fig. 13: Relative Response Quality of Vicuna and a few other exams, compared with GPT 3.5. Courtesy of [33]. well-known models by GPT-4. Courtesy of Vicuna Team. LikeAlpacaandVicuna,theGuanacomodels[63]arealso InJuly2023,Meta,inpartnershipwithMicrosoft,released finetunedLLaMAmodelsusinginstruction-followingdata.But the LLaM"
131,Large Language Models A Survey.pdf,"Amodelsusinginstruction-followingdata.But the LLaMA-2 collection [61], which include both foundation the finetuning is done very efficiently using QLoRA such languagemodelsandChatmodelsfinetunedfordialog,known that finetuning a 65B parameter model can be done on a as LLaMA-2 Chat. The LLaMA-2 Chat models were reported single48GBGPU.QLoRAback-propagatesgradientsthrough to outperform other open-source models on many public a frozen, 4-bit quantized pre-trained language model into Low benchmarks. F"
132,Large Language Models A Survey.pdf," pre-trained language model into Low benchmarks. Fig 12 shows the training process of LLaMA-2 RankAdapters(LoRA).ThebestGuanacomodeloutperforms Chat. The process begins with pre-training LLaMA-2 using all previously released models on the Vicuna benchmark, publicly available online data. Then, an initial version of reaching 99.3% of the performance level of ChatGPT while LLaMA-2 Chat is built via supervised fine-tuning. Subse- only requiring 24 hours of fine-tuning on a single GPU. quently, the "
133,Large Language Models A Survey.pdf,"ours of fine-tuning on a single GPU. quently, the model is iteratively refined using RLHF, rejection Koala [64] is yet another instruction-following language samplingandproximalpolicyoptimization.IntheRLHFstage, modelbuiltonLLaMA,butwithaspecificfocusoninteraction the accumulation of human feedback for revising the reward datathatincludeuserinputsandresponsesgeneratedbyhighly model is crucial to prevent the reward model from being capable closed-source chat models such as ChatGPT. The changed to"
134,Large Language Models A Survey.pdf,"source chat models such as ChatGPT. The changed too much, which could hurt the stability of LLaMA Koala-13B model performs competitively with state-of-the-art model training. chat models according to human evaluation based on real- world user prompts. Mistral-7B [65] is a 7B-parameter language model engi- neered for superior performance and efficiency. Mistral-7B outperformsthebestopen-source13Bmodel(LLaMA-2-13B) across all evaluated benchmarks, and the best open-source 34Bmodel(LLaMA-34B)inreas"
135,Large Language Models A Survey.pdf,"and the best open-source 34Bmodel(LLaMA-34B)inreasoning,mathematics,andcode generation. This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectivelyhandlesequencesofarbitrarylengthwithareduced inference cost. TheLLaMAfamilyisgrowingrapidly,asmoreinstruction- Fig. 12: Training of LLaMA-2 Chat. Courtesy of [61]. following models have been built on LLaMA or LLaMA- 2, including Code LLaMA [66], Gorilla [67], Giraffe [68], Vigogne [69], Tulu "
136,Large Language Models A Survey.pdf,"], Gorilla [67], Giraffe [68], Vigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable Alpaca[62]isfine-tunedfromtheLLaMA-7Bmodelusing Beluga2 [72], just to name a few. 52K instruction-following demonstrations generated in the 3)The PaLM Family: The PaLM (Pathways Language style of self-instruct using GPT-3.5 (text-davinci-003). Alpaca Model) family are developed by Google. The first PaLM is very cost-effective for training, especially for academic model[31]wasannouncedinApril2022andremainedpri"
137,Large Language Models A Survey.pdf,"mic model[31]wasannouncedinApril2022andremainedprivate research. On the self-instruct evaluation set, Alpaca performs until March 2023. It is a 540B parameter transformer-based similarly to GPT-3.5, despite that Alpaca is much smaller. LLM. The model is pre-trained on a high-quality text corpus TheVicunateamhasdevelopeda13Bchatmodel,Vicuna- consisting of 780 billion tokens that comprise a wide range 13B, by fine-tuning LLaMA on user-shared conversations of natural language tasks and use cases. P"
138,Large Language Models A Survey.pdf,"sations of natural language tasks and use cases. PaLM is pre-trained
on 6144 TPU v4 chips using the Pathways system, which [77]. Med-PaLM 2 scored up to 86.5% on the MedQA enables highly efficient training across multiple TPU Pods. dataset (i.e., a benchmark combining six existing open ques- PaLM demonstrates continued benefits of scaling by achiev- tion answering datasets spanning professional medical exams, ing state-of-the-art few-shot learning results on hundreds of research, and consumer qu"
139,Large Language Models A Survey.pdf,"g results on hundreds of research, and consumer queries), improving upon Med-PaLM language understanding and generation benchmarks. PaLM- by over 19% and setting a new state-of-the-art. 540B outperforms not only state-of-the-art fine-tuned models on a suite of multi-step reasoning tasks, but also on par with C. Other Representative LLMs humans on the recently released BIG-bench benchmark. In addition to the models discussed in the previous sub- The U-PaLM models of 8B, 62B, and 540B scales are s"
140,Large Language Models A Survey.pdf,"he U-PaLM models of 8B, 62B, and 540B scales are sections, there are other popular LLMs which do not belong continuallytrainedonPaLMwithUL2R,amethodofcontinue to those three model families, yet they have achieved great training LLMs on a few steps with UL2’s mixture-of-denoiser performance and have pushed the LLMs field forward. We objective [73]. An approximately 2x computational savings briefly describe these LLMs in this subsection. rate is reported. FLAN: In [78], Wei et al. explored a simpl"
141,Large Language Models A Survey.pdf,"ported. FLAN: In [78], Wei et al. explored a simple method for U-PaLM is later instruction-finetuned as Flan-PaLM [74]. improving the zero-shot learning abilities of language models. Compared to other instruction finetuning work mentioned They showed that instruction tuning language models on a above, Flan-PaLM’s finetuning is performed using a much collection of datasets described via instructions substantially larger number of tasks, larger model sizes, and chain-of- improves zero-shot perform"
142,Large Language Models A Survey.pdf,"el sizes, and chain-of- improves zero-shot performance on unseen tasks. They take thoughtdata.Asaresult,Flan-PaLMsubstantiallyoutperforms a 137B parameter pretrained language model and instruction previous instruction-following models. For instance, Flan- tuneitonover60NLPdatasetsverbalizedvianaturallanguage PaLM-540B, which is instruction-finetuned on 1.8K tasks, instruction templates. They call this instruction-tuned model outperforms PaLM-540B by a large margin (+9.4% on av- FLAN. Fig 15 prov"
143,Large Language Models A Survey.pdf," by a large margin (+9.4% on av- FLAN. Fig 15 provides a comparison of instruction tuning erage). The finetuning data comprises 473 datasets, 146 task with pretrain–finetune and prompting. categories, and 1,836 total tasks, as illustrated in Fig 14. Fig. 15: comparison of instruction tuning with pre- train–finetune and prompting. Courtesy of [78]. Gopher: In [79], Rae et al. presented an analysis of Transformer-basedlanguagemodelperformanceacrossawide rangeofmodelscales—frommodelswithtensofmilli"
144,Large Language Models A Survey.pdf,"awide rangeofmodelscales—frommodelswithtensofmillionsof parametersuptoa280billionparametermodelcalledGopher. Fig.14:Flan-PaLMfinetuningconsistof473datasetsinabove These models were evaluated on 152 diverse tasks, achieving task categories. Courtesy of [74]. state-of-the-art performance across the majority. The number of layers, the key/value size, and other hyper-parameters of different model sizes are shown in Fig 16. PaLM-2 [75] is a more compute-efficient LLM with bet- ter multilingual and re"
145,Large Language Models A Survey.pdf,"te-efficient LLM with bet- ter multilingual and reasoning capabilities, compared to its predecessor PaLM. PaLM-2 is trained using a mixture of objectives. Through extensive evaluations on English, multi- lingual, and reasoning tasks, PaLM-2 significantly improves the model performance on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference than PaLM. Fig. 16: Model architecture details of Gopher with different Med-PaLM [76] is a domai"
146,Large Language Models A Survey.pdf," of Gopher with different Med-PaLM [76] is a domain-specific PaLM, and is de- number of parameters. Courtesy of [78]. signed to provide high-quality answers to medical questions. Med-PaLM is finetuned on PaLM using instruction prompt tuning, a parameter-efficient method for aligning LLMs to new domains using a few exemplars. Med-PaLM obtains very T0: In [80], Sanh et al. developed T0, a system for easily encouraging results on many healthcare tasks, although it is mapping any natural language ta"
147,Large Language Models A Survey.pdf,"ks, although it is mapping any natural language tasks into a human-readable stillinferiortohumanclinicians.Med-PaLM2improvesMed- prompted form. They converted a large set of supervised PaLM via med-domain finetuning and ensemble prompting datasets, each with multiple prompts with diverse wording.
These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. Then, a T0 encoder-decoder model is developed to consume textual inputs and produces target re"
148,Large Language Models A Survey.pdf,"d to consume textual inputs and produces target responses. The model is trained on a multitask mixture of NLP datasets partitioned into different tasks. ERNIE 3.0: In [81], Sun et al. proposed a unified frame- worknamedERNIE3.0forpre-traininglarge-scaleknowledge enhanced models. It fuses auto-regressive network and auto- encoding network, so that the trained model can be easily tai- Fig. 18: Retro architecture. Left: simplified version where a lored for both natural language understanding and ge"
149,Large Language Models A Survey.pdf,"red for both natural language understanding and generation sequence of length n = 12 is split into l = 3 chunks of size tasksusingzero-shotlearning,few-shotlearningorfine-tuning. m = 4. For each chunk, we retrieve k = 2 neighbours of r = They have trained ERNIE 3.0 with 10 billion parameters 5 tokens each. The retrieval pathway is shown on top. Right: on a 4TB corpus consisting of plain texts and a large-scale Details of the interactions in the CCA operator. Causality is knowledge graph. Fig 17 "
150,Large Language Models A Survey.pdf,"CA operator. Causality is knowledge graph. Fig 17 illustrates the model architecture of maintainedasneighboursofthefirstchunkonlyaffectthelast Ernie 3.0. token of the first chunk and tokens from the second chunk. Courtesy of [82]. Fig.17:High-levelmodelarchitectureofERNIE3.0.Courtesy of [81]. Fig. 19: GLaM model architecture. Each MoE layer (the RETRO:In[82],Borgeaudetal.enhancedauto-regressive bottom block) is interleaved with a Transformer layer (the language models by conditioning on document"
151,Large Language Models A Survey.pdf,"r (the language models by conditioning on document chunks re- upper block). Courtesy of [84]. trievedfromalargecorpus,basedonlocalsimilaritywithpre- cedingtokens.Usinga2-trillion-tokendatabase,theRetrieval- Enhanced Transformer (Retro) obtains comparable perfor- mance to GPT-3 and Jurassic-1 [83] on the Pile, despite using Theyshowedthatfine-tuningwithannotateddataandenabling 25% fewer parameters. As shown in Fig 18, Retro combines the model to consult external knowledge sources can lead to a fr"
152,Large Language Models A Survey.pdf,"onsult external knowledge sources can lead to a frozen Bert retriever, a differentiable encoder and a chunked significant improvements towards the two key challenges of cross-attentionmechanismtopredicttokensbasedonanorder safety and factual grounding. of magnitude more data than what is typically consumed during training. OPT: In [86], Zhang et al. presented Open Pre-trained Transformers(OPT),asuiteofdecoder-onlypre-trainedtrans- GLaM: In [84], Du et al. proposed a family of LLMs formers rangin"
153,Large Language Models A Survey.pdf,"Du et al. proposed a family of LLMs formers ranging from 125M to 175B parameters, which they named GLaM (Generalist Language Model), which use a share with researchers. The OPT models’ parameters are sparsely activated mixture-of-experts architecture to scale the shown in 20 model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillionparameters,whichisapproximately7xlargerthanGPT- 3. It consumes only 1/3 of the energy used to "
154,Large Language Models A Survey.pdf,"PT- 3. It consumes only 1/3 of the energy used to train GPT-3 and requireshalfofthecomputationflopsforinference,whilestill achieving better overall zero, one and few-shot performance across 29 NLP tasks. Fig 19 shows the high-level architecture of GLAM. LaMDA: In [85], Thoppilan et al. presented LaMDA, a family of Transformer-based neural language models special- Fig. 20: Different OPT Models’ architecture details. Courtesy ized for dialog, which have up to 137B parameters and are of [86]. pre-t"
155,Large Language Models A Survey.pdf," have up to 137B parameters and are of [86]. pre-trainedon1.56Twordsofpublicdialogdataandwebtext.
Chinchilla:In[2],Hoffmannetal.investigatedtheoptimal model size and number of tokens for training a transformer language model under a given compute budget. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they found that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for e"
156,Large Language Models A Survey.pdf,"of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. They tested this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and Fig. 21: Sparrow pipeline relies on human participation to 4% more more data. continually expand a training set. Courtesy of [90]. Galactica: In [87], Taylor et al. introduced Galactica, a largelanguagemodelth"
157,Large Language Models A Survey.pdf,"t al. introduced Galactica, a largelanguagemodelthatcanstore,combineandreasonabout scientific knowledge. They trained on a large scientific corpus effective. They proposed Mixture-of-Denoisers (MoD), a pre- ofpapers,referencematerial,knowledgebasesandmanyother trainingobjectivethatcombinesdiversepre-trainingparadigms sources.Galacticaperformedwellonreasoning,outperforming together. This framework is known as Unifying Language Chinchilla on mathematical MMLU by 41.3% to 35.7%, and Learning (UL2)."
158,Large Language Models A Survey.pdf,"atical MMLU by 41.3% to 35.7%, and Learning (UL2). An overview of UL2 pretraining paradigm PaLM 540B on MATH with a score of 20.4% versus 8.8%. is shown in Fig 21. CodeGen: In [88], Nijkamp et al. trained and released a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open sourced the training library JAX- FORMER. They showed the utility of the trained model by demonstrating that it is competitive with the previous sta"
159,Large Language Models A Survey.pdf,"ating that it is competitive with the previous state-of- the-art on zero-shot Python code generation on HumanEval. They further investigated the multi-step paradigm for program synthesis, where a single program is factorized into multi- ple prompts specifying sub-problems. They also constructed an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. AlexaTM: In [89], Soltan et al. demonstrated that mul- Fig. "
160,Large Language Models A Survey.pdf,"n [89], Soltan et al. demonstrated that mul- Fig. 22: An overview of UL2 pretraining paradigm. Courtesy tilingual large-scale sequence-to-sequence (seq2seq) models, of [92]. pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various task. They trained a BLOOM:In[93],Scaoetal.presentedBLOOM,a176B- 20 billion parameter multilingual seq2seq model called Alexa parameter open-access language model designed a"
161,Large Language Models A Survey.pdf,"xa parameter open-access language model designed and built Teacher Model (AlexaTM 20B) and showed that it achieves thanks to a collaboration of hundreds of researchers. BLOOM state-of-the-art (SOTA) performance on 1-shot summarization is a decoder-only Transformer language model trained on the tasks, outperforming a much larger 540B PaLM decoder ROOTS corpus, a dataset comprising hundreds of sources in model. AlexaTM consist of 46 encoder layers, 32 decoder 46 natural and 13 programming language"
162,Large Language Models A Survey.pdf," 32 decoder 46 natural and 13 programming languages (59 in total). An layers, 32 attention heads, and d model =4096. overview of BLOOM architecture is shown in Fig 23. Sparrow: In [90], Glaese et al. presented Sparrow, an information-seekingdialogueagenttrainedtobemorehelpful, correct, and harmless compared to prompted language model baselines.Theyusedreinforcementlearningfromhumanfeed- back to train their models with two new additions to help human raters judge agent behaviour. The high-level p"
163,Large Language Models A Survey.pdf,"man raters judge agent behaviour. The high-level pipeline of Sparrow model is shown in Fig 21. Minerva: In [91], Lewkowycz et al. introduced Minerva, alargelanguagemodelpretrainedongeneralnaturallanguage dataandfurthertrainedontechnicalcontent,totackleprevious LLM struggle with quantitative reasoning (such as solving mathematics, science, and engineering problems). Fig. 23: An overview of BLOOM architecture. Courtesy of [93]. MoD: In [92], Tay et al. presented a generalized and unified perspecti"
164,Large Language Models A Survey.pdf," al. presented a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be GLM: In [94], Zeng et al. introduced GLM-130B, a
bilingual (English and Chinese) pre-trained language model FuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B with 130 billion parameters. It was an attempt to open-source [122]. a 100B-scale model at least as good as GPT-3 (davinci) an"
165,Large Language Models A Survey.pdf,"scale model at least as good as GPT-3 (davinci) and Fig 24 provides an overview of some of the most repre- unveil how models of such a scale can be successfully pre- sentative LLM frameworks, and the relevant works that have trained. contributed to the success of LLMs and helped to push the Pythia: In [95], Biderman et al. introduced Pythia, a suite limits of LLMs. of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public"
166,Large Language Models A Survey.pdf,"size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the III. HOWLLMSAREBUILT 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. In this section, we first review the popular architectures usedforLLMs,andthendiscussdataandmodelingtechniques Orca: In [96], Mukherjee et al. develop Orca, a 13-billion ranging from data preparation, tokenization, to pre-training, parameter model that learns to imitate the "
167,Large Language Models A Survey.pdf,"ining, parameter model that learns to imitate the reasoning process instruction tuning, and alignment. of large foundation models. Orca learns from rich signals fromGPT-4includingexplanationtraces;step-by-stepthought Once the model architecture is chosen, the major steps processes; and other complex instructions, guided by teacher involved in training an LLM includes: data preparation (col- assistance from ChatGPT. lection, cleaning, deduping, etc.), tokenization, model pre- training (in a self-"
168,Large Language Models A Survey.pdf,"c.), tokenization, model pre- training (in a self-supervised learning fashion), instruction StarCoder: In [97], Li et al. introduced StarCoder and tuning, and alignment. We will explain each of them in a StarCoderBase. They are 15.5B parameter models with 8K separate subsection below. These steps are also illustrated in context length, infilling capabilities and fast large-batch in- Fig 25. ference enabled by multi-query attention. StarCoderBase is trained on one trillion tokens sourced from The"
169,Large Language Models A Survey.pdf,"is trained on one trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories A. Dominant LLM Architectures with inspection tools and an opt-out process. They fine-tuned StarCoderBaseon35BPythontokens,resultinginthecreation ThemostwidelyusedLLMarchitecturesareencoder-only, of StarCoder. They performed the most comprehensive evalu- decoder-only,andencoder-decoder.Mostofthemarebasedon ation of Code LLMs to date and showed that StarCoderBase Transformer ("
170,Large Language Models A Survey.pdf,"o date and showed that StarCoderBase Transformer (as the building block). Therefore we also review outperformseveryopenCodeLLMthatsupportsmultiplepro- the Transformer architecture here. gramming languages and matches or outperforms the OpenAI code-cushman-001 model. 1)Transformer: inaground-breakingwork[44],Vaswani et al. proposed the Transformer framework, which was orig- KOSMOS: In [98], Huang et al. introduced KOSMOS-1, inally designed for effective parallel computing using GPUs. a Multimodal"
171,Large Language Models A Survey.pdf,"ective parallel computing using GPUs. a Multimodal Large Language Model (MLLM) that can per- The heart of Transformer is the (self-)attention mechanism, ceive general modalities, learn in context (i.e., few-shot), and which can capture long-term contextual information much follow instructions (i.e. zero-shot). Specifically, they trained more effectively using GPUs than the recurrence and convo- KOSMOS-1 from scratch on web-scale multi-modal corpora, lution mechanisms. Fig 26 provides a high-leve"
172,Large Language Models A Survey.pdf,"ra, lution mechanisms. Fig 26 provides a high-level overview of includingarbitrarilyinterleavedtextandimages,image-caption transformerwork.Inthissectionweprovideanoverviewofthe pairs,andtextdata.ExperimentalresultsshowthatKOSMOS- main elements and variants, see [44], [123] for more details. 1achievesimpressiveperformanceon(i)languageunderstand- ing, generation, and even OCR-free NLP (directly fed with The Transformer language model architecture, originally document images), (ii) perception-langu"
173,Large Language Models A Survey.pdf,"originally document images), (ii) perception-language tasks, including proposed for machine translation, consists of an encoder and multimodal dialogue, image captioning, visual question an- a decoder. The encoder is composed of a stack of N = 6 swering, and (iii) vision tasks, such as image recognition with identical Transformer layers. Each layer has two sub-layers. descriptions (specifying classification via text instructions). The first one is a multi-head self-attention layer, and the other"
174,Large Language Models A Survey.pdf,"s a multi-head self-attention layer, and the other one is a simple position-wise fully connected feed-forward Gemini: In [99], Gemini team introduced a new family of network. The decoder is composed of a stack of 6 identical multimodal models, that exhibit promising capabilities across layers.Inadditiontothetwosub-layersineachencoderlayer, image, audio, video, and text understanding. Gemini family the decoder has a third sub-layer, which performs multi-head includes three versions: Ultra for hig"
175,Large Language Models A Survey.pdf," multi-head includes three versions: Ultra for highly-complex tasks, Pro attention over the output of the encoder stack. The attention forenhancedperformanceanddeployabilityatscale,andNano functioncanbedescribedasmappingaqueryandasetofkey- for on-device applications. Gemini architecture is built on top value pairs to an output, where the query, keys, values, and ofTransformerdecoders,andistrainedtosupport32kcontext output are all vectors. The output is computed as a weighted length (via using ef"
176,Large Language Models A Survey.pdf,"put is computed as a weighted length (via using efficient attention mechanisms). sum of the values, where the weight assigned to each value SomeoftheotherpopularLLMframeworks(ortechniques is computed by a compatibility function of the query with the used for efficient developments of LLMs) includes Inner- corresponding key. Instead of performing a single attention Monologue [100], Megatron-Turing NLG [101], LongFormer function with d dimensional keys, values and queries, model [102], OPT-IML [10"
177,Large Language Models A Survey.pdf,"keys, values and queries, model [102], OPT-IML [103], MeTaLM [104], Dromedary [105], it is found to be beneficial to linearly project the queries, Palmyra [106], Camel [107], Yalm [108], MPT [109], ORCA- keys and values h with different, learned linear projections to 2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2 d , d and d dimensions, respectively. Positional encoding is k k v [113], Zephyr [114], Grok [115], Qwen [116], Mamba [30], incorporated to fuse information about the relativ"
178,Large Language Models A Survey.pdf,"incorporated to fuse information about the relative or absolute Mixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119], position of the tokens in the sequence.
Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our #parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way for their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language m"
179,Large Language Models A Survey.pdf,"er, BERT, GPT-1), as well as some small language models. ♣ shows entities that serve not only as models but also as approaches. ♦ shows only approaches. 2)Encoder-Only: For this family, at each stage, the atten- mask word replaces. Encoder-decoder models are best suited tion layers can access all the words in the initial sentence. for tasks about generating new sentences conditioned on a The pre-training of these models usually consist of some- given input, such as summarization, translation, or"
180,Large Language Models A Survey.pdf,"iven input, such as summarization, translation, or generative how corrupting a given sentence (for instance, by masking question answering. random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are great B. Data Cleaning for tasks requiring an understanding of the full sequence, such as sentence classification, named entity recognition, and Data quality is crucial to the performance of language extractive question answering. One prominent e"
181,Large Language Models A Survey.pdf,"age extractive question answering. One prominent encoder only models trained on them. Data cleaning techniques such as model is BERT (Bidirectional Encoder Representations from filtering, deduplication, are shown to have a big impact on Transformers), proposed in [24]. the model performance. 3)Decoder-Only: For these models, at each stage, for any As an example, in Falcon40B [124], Penedo et al. showed word,theattentionlayerscanonlyaccessthewordspositioned thatproperlyfilteredanddeduplicatedwebd"
182,Large Language Models A Survey.pdf,"positioned thatproperlyfilteredanddeduplicatedwebdataalonecanlead before that in the sentence. These models are also sometimes to powerful models; even significantly outperforming models calledauto-regressivemodels.Thepretrainingofthesemodels fromthestate-of-the-arttrainedonThePile.Despiteextensive is usually formulated as predicting the next word (or token) filtering, they were able to obtain five trillion tokens from in the sequence. The decoder-only models are best suited for CommonCrawl. The"
183,Large Language Models A Survey.pdf,"r-only models are best suited for CommonCrawl. They also released an extract of 600 billion tasks involving text generation. GPT models are prominent tokens from our REFINEDWEB dataset, and 1.3/7.5B param- example of this model category. eters language models trained on it. 27 shows the Refinement process of CommonCrawl data by this work. 4)Encoder-Decoder: These models use both encoder and decoder,andaresometimescalledsequence-to-sequencemod- 1)Data Filtering: Data filtering aims to enhance the"
184,Large Language Models A Survey.pdf,"Data Filtering: Data filtering aims to enhance the qual- els.Ateachstage,theattentionlayersoftheencodercanaccess ity of training data and the effectiveness of the trained LLMs. all the words in the initial sentence, whereas the attention Common data filtering techniques include: layersofthedecoderonlyaccessesthewordspositionedbefore a given word in the input. These models are usually pre- Removing Noise: refers to eliminating irrelevant or noisy trained using the objectives of encoder or decoder"
185,Large Language Models A Survey.pdf,"trained using the objectives of encoder or decoder models, but data that might impact the model’s ability to generalize well. usually involve something a bit more complex. For instance, As an example, one can think of removing false information some models are pretrainedby replacingrandom spansof text fromthetrainingdata,tolowerthechanceofmodelgenerating (that can contain several words) with a single mask special false responses. Two mainstream approaches for quality filter- word, and the object"
186,Large Language Models A Survey.pdf,"pproaches for quality filter- word, and the objective is then to predict the text that this ing includes: classifier-based, and heuristic-based frameworks.
How LLMs Are Built? Data Filtering Removing Noise Handling Outliers Data Cleaning Addressing Imbalances Text Preprocessing Deduplication BytePairEncoding Tokenizations WordPieceEncoding SentencePieceEncoding Absolute Positional Embeddings Relative Positional Embeddings Positional Encoding Rotary Position Embeddings Relative Positional Bias En"
187,Large Language Models A Survey.pdf,ry Position Embeddings Relative Positional Bias Encoder-Only Decoder-Only LLM Architectures Encoder-Decoder ... Masked Language Modeling Causal Language Modeling Model Pre-training Next Sentence Prediction Mixture of Experts Supervised Fine-tuning General Fine-tuning Fine-tuning and Instruction Tuning Multi-turn Instructions Instruction Following Supervised learning Reinforcement Learning from Human Feedback Alignment Direct Preference Optimization Kahneman-Tversky Optimization Greedy Search Dec
188,Large Language Models A Survey.pdf,"on Kahneman-Tversky Optimization Greedy Search Decoding Strategies Beam Search Top-k Sampling Top-p Sampling Optimized Training Zero Redundancy Optimizer Receptance Weighted Key Value Cost-Effective Training/Inference, Low-Rank Adaption Adaptation & Compression Knowledge Distillation Quantization Fig. 25: This figure shows different components of LLMs.
biasesinthemodeltrainingprocessandreducethediversity,as the model may learn from the same examples multiple times, potentially leading to overfit"
189,Large Language Models A Survey.pdf,"les multiple times, potentially leading to overfitting on those particular instances. Some works [125] have shown that de-duplication improves models’ ability to generalize to new, unseen data. The de-duplication process is particularly important when dealing with large datasets, as duplicates can unintentionally inflate the importance of certain patterns or characteristics. This is especially relevant in NLP tasks, where diverse and representative training data is crucial for building robust la"
190,Large Language Models A Survey.pdf,"ve training data is crucial for building robust lan- guage models. The specific de-duplication method can vary based on the nature of the data and the requirements of the particular languagemodelbeingtrained.Itmayinvolvecomparingentire data points or specific features to identify and eliminate du- plicates. At the document level, existing works mainly rely on the overlap ratio of high-level features (e.g. n-grams overlap) between documents to detect duplicate samples. C. Tokenizations Fig.26:Hig"
191,Large Language Models A Survey.pdf,"ect duplicate samples. C. Tokenizations Fig.26:High-leveloverviewoftransformerwork.Courtesyof Tokenization referes to the process of converting a se- [44]. quence of text into smaller parts, known as tokens. While the simplest tokenization tool simply chops text into tokens based on white space, most tokenization tools rely on a word dictionary. However, out-of-vocabulary (OOV) is a problem in this case because the tokenizer only knows words in its dictionary. To increase the coverage of diction"
192,Large Language Models A Survey.pdf,"ts dictionary. To increase the coverage of dictionaries, popular tokenizers used for LLMs are based on sub-words, which can be combined to form a large number of words, including the words unseen in training data or words in different languages. In what follows, we describe three popular tokenizers. 1)BytePairEncoding: BytePairEncoding is originally a type of data compression algorithm that uses frequent patterns atbyteleveltocompressthedata.Bydefinition,thisalgorithm mainly tries to keep the fr"
193,Large Language Models A Survey.pdf,"finition,thisalgorithm mainly tries to keep the frequent words in their original form Fig. 27: Subsequent stages of Macrodata Refinement remove and break down ones that are not common. This simple nearly 90% of the documents originally in CommonCrawl. paradigm keeps the vocabulary not very large, but also good Courtesy of [124]. enough to represent common words at the same time. Also morphologicalformsofthefrequentwordscanberepresented very well if suffix or prefix is also commonly presented in "
194,Large Language Models A Survey.pdf,"if suffix or prefix is also commonly presented in the training data of the algorithm. Handling Outliers: Identifying and handling outliers or 2)WordPieceEncoding: Thisalgorithmismainlyusedfor anomalies in the data to prevent them from disproportionately very well-known models such as BERT and Electra. At the influencing the model. beginningoftraining,thealgorithmtakesallthealphabetfrom Addressing Imbalances: Balancing the distribution of thetrainingdatatomakesurethatnothingwillbeleftasUNK classe"
195,Large Language Models A Survey.pdf,"ingdatatomakesurethatnothingwillbeleftasUNK classes or categories in the dataset to avoid biases and ensure orunknownfromthetrainingdataset.Thiscasehappenswhen fair representation. This is specially useful for responsible the model is given an input that can not be tokenized by the model training and evaluation. tokenizer.Itmostlyhappensincaseswheresomecharactersare not tokenizable by it. Similar to BytePairEncoding, it tries to Text Preprocessing: Cleaning and standardizing text data maximizeth"
196,Large Language Models A Survey.pdf,"g: Cleaning and standardizing text data maximizethelikelihoodofputtingallthetokensinvocabulary by removing stop words, punctuation, or other elements that based on their frequency. may not contribute significantly to the model’s learning. 3)SentencePieceEncoding: Although both tokenizers de- Dealing with Ambiguities: Resolving or excluding am- scribedbeforearestrongandhavemanyadvantagescompared biguous or contradictory data that might confuse the model to white-space tokenization, they still tak"
197,Large Language Models A Survey.pdf," model to white-space tokenization, they still take assumption of during training. This can help the model to provide more words being always separated by white-space as granted. This definite and reliable answers. assumptionisnotalwaystrue,infactinsomelanguages,words 2)Deduplication: De-duplication refers to the process of can be corrupted by many noisy elements such as unwanted removing duplicate instances or repeated occurrences of the spaces or even invented words. SentencePieceEncoding trie"
198,Large Language Models A Survey.pdf,"or even invented words. SentencePieceEncoding tries same data in a dataset. Duplicate data points can introduce to address this issue.
D. Positional Encoding In Autoregressive Language Modeling framework, given a sequence of n tokens x , ..., x , the model tries to predict 1 n 1)Absolute Positional Embeddings: (APE) [44] has been next token x (and sometimes next sequence of tokens) in n+1 used in the original Transformer model to preserve the infor- an auto-regressive fashion. One popular loss f"
199,Large Language Models A Survey.pdf,"or- an auto-regressive fashion. One popular loss function in this mationofsequenceorder.Therefore,thepositionalinformation case is the log-likelihood of predicted tokens as shown in Eq of words is added to the input embeddings at the bottom of 2 boththeencoderanddecoderstacks.Therearevariousoptions N (cid:88) for positional encodings, either learned or fixed. In the vanilla L (x)= p(x |x ,...,x ) (1) ALM i+n i i+n−1 Transformer, sine and cosine functions are employed for this i=1 purpose. The ma"
200,Large Language Models A Survey.pdf,"unctions are employed for this i=1 purpose. The main drawback of using APE in Transformers Given the auto-regressive nature of this framework, the is the restriction to a certain number of tokens. Additionally, decoder-only models are naturally better suited to learn how APEfailstoaccountfortherelativedistancesbetweentokens. to accomplish these task. 2)Relative Positional Embeddings: (RPE) [126] involves In Masked Language Modeling, some words are masked extending self-attention to take into acc"
201,Large Language Models A Survey.pdf,"e masked extending self-attention to take into account the pairwise links in a sequence and the model is trained to predict the masked between input elements. RPE is added to the model at two words based on the surrounding context. Sometimes people levels: first as an additional component to the keys, and refer to this approach as denoising autoencoding, too. If we subsequently as a sub-component of the values matrix. This denote the masked/corrupted samples in the sequence x, as x ̃, approach l"
202,Large Language Models A Survey.pdf,"pted samples in the sequence x, as x ̃, approach looks at the input as a fully-connected graph with then the training objective of this approach can be written as: labelsanddirectededges.Inthecaseoflinearsequences,edges can capture information about the relative position differences (cid:88) N L (x)= p(x ̃|x\x ̃) (2) between input elements. A clipping distance, represented as k MLM 2 ≤ k ≤ n−4, specifies the maximum limit on relative lo- i=1 cations. This allows the model to make reasonable pred"
203,Large Language Models A Survey.pdf,"ons. This allows the model to make reasonable predictions And more recently, Mixture of Experts (MoE) [130], for sequence lengths that are not part of the training data. [131] have become very popular in LLM space too. MoEs 3)Rotary Position Embeddings: Rotary Positional Em- enable models to be pre-trained with much less compute, bedding (RoPE) [127] tackles problems with existing ap- which means one can dramatically scale up the model or proaches.Learnedabsolutepositionalencodingscanlackgen- da"
204,Large Language Models A Survey.pdf,"s.Learnedabsolutepositionalencodingscanlackgen- dataset size with the same compute budget as a dense model. eralizability and meaningfulness, particularly when sentences MoE consists of two main elements: Sparse MoE layers, are short. Moreover, current methods like T5’s positional which are used instead of dense feed-forward network (FFN) embedding face challenges with constructing a full attention layers, and have a certain number of “experts” (e.g. 8), in matrix between positions. RoPE uses a "
205,Large Language Models A Survey.pdf,"e.g. 8), in matrix between positions. RoPE uses a rotation matrix to which each expert is a neural network. In practice, the experts encode the absolute position of words and simultaneously in- areFFNs,buttheycanalsobemorecomplexnetworks.Agate cludes explicit relative position details in self-attention. RoPE network or router, that determines which tokens are sent to brings useful features like flexibility with sentence lengths, a which expert. It is worth noting that, one can send a token decre"
206,Large Language Models A Survey.pdf,"t is worth noting that, one can send a token decrease in word dependency as relative distances increase, to more than one expert. How to route a token to an expert and the ability to improve linear self-attention with relative is one of the big decisions when working with MoEs - the position encoding. GPT-NeoX-20B, PaLM, CODEGEN, and router is composed of learned parameters and is pretrained at LLaMA are among models that take advantage of RoPE in the same time as the rest of the network. Fig 29"
207,Large Language Models A Survey.pdf,"n the same time as the rest of the network. Fig 29 provides an their architectures. illustration of a Switch Transformer encoder block, which are used in MoE. 4)Relative Positional Bias: The concept behind this type of positional embedding is to facilitate extrapolation during F. Fine-tuning and Instruction Tuning inferenceforsequenceslongerthanthoseencounteredintrain- ing.In[128]Pressetal.proposedAttentionwithLinearBiases Early language models such as BERT trained using self- (ALiBi). Instead o"
208,Large Language Models A Survey.pdf,"uch as BERT trained using self- (ALiBi). Instead of simply adding positional embeddings to supervision as explained in section III-E were not able to wordembeddings,theyintroducedabiastotheattentionscores performspecifictasks.Inorderforthefoundationmodeltobe of query-key pairs, imposing a penalty proportional to their usefulitneededtobefine-tunedtoaspecifictaskwithlabeled distance. In the BLOOM model, ALiBi is leveraged. data (so-called supervised fine-tuning or SFT for short). For example,inthe"
209,Large Language Models A Survey.pdf,"d fine-tuning or SFT for short). For example,intheoriginalBERTpaper[24],themodelwasfine- tunedto11differenttasks.WhilemorerecentLLMsnolonger E. Model Pre-training require fine-tuning to be used, they can still benefit from task or data-specific fine-tuning. For example, OpenAI reports that Pre-training is the very first step in large language model themuchsmallerGPT-3.5TurbomodelcanoutperformGPT-4 training pipeline, and it helps LLMs to acquire fundamental when fine-tuned with task specific data"
210,Large Language Models A Survey.pdf,"undamental when fine-tuned with task specific data 2. language understanding capabilities, which can be useful in a wide range of language related tasks. During pre-training, the Fine-tuning does not need to be performed to a single LLM is trained on a massive amount of (usually) unlabeled task though, and there are different approaches to multi-task texts, usually in a self-supervised manner. There are different fine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one approaches used for "
211,Large Language Models A Survey.pdf,"l. [132]). Fine-tuning to one approaches used for pre-training like next sentence prediction or more tasks is known to improve results and reduce the [24], two most common ones include, next token prediction complexity of prompt engineering, and it can serve as an (autoregressive language modeling), and masked language modeling. 2https://platform.openai.com/docs/guides/fine-tuning
(a) Absolute Positional Embeddings [129] (b) Relative Positional Embeddings (c) Rotary Positional Embedding [127] (d"
212,Large Language Models A Survey.pdf,"mbeddings (c) Rotary Positional Embedding [127] (d) Relative Positional Bias [128] Fig. 28: Various positional encodings are employed in LLMs. Instructions[134]includenotonlythetaskdefinitionbutother components such as positive/negative examples or things to avoid. The specific approach and instruction datasets used to instruction-tune an LLM varies, but, generally speaking, in- struction tuned models outperform their original foundation models they are based on. For example, InstructGPT [59] ou"
213,Large Language Models A Survey.pdf,"hey are based on. For example, InstructGPT [59] outperforms GPT-3 on most benchmarks. The same is true for Alpaca [62] when compared to LLaMA. Self-Instruct [135], proposed by Wang et al. is also a popular approach along this line, in which they introduced a frameworkforimprovingtheinstruction-followingcapabilities Fig. 29: : Illustration of a Switch Transformer encoder block. of pre-trained language models by bootstrapping their own They replaced the dense feed forward network (FFN) layer gener"
214,Large Language Models A Survey.pdf,"d the dense feed forward network (FFN) layer generations. Their pipeline generates instructions, input, and present in the Transformer with a sparse Switch FFN layer output samples from a language model, then filters invalid or (light blue). . Courtesy of [131]. similaronesbeforeusingthemtofinetunetheoriginalmodel. G. Alignment alternative to retrieval augmented generation. Furthermore, AIAlignmentistheprocessofsteeringAIsystemstowards there are other reasons why it might be advisable to fine-tu"
215,Large Language Models A Survey.pdf,"other reasons why it might be advisable to fine-tune. human goals, preferences, and principles. LLMs, pre-trained Forexample,onemightwanttofine-tunetoexposethemodel for word prediction, often exhibit unintended behaviors. For to new or proprietary data that it has not been exposed to example, they might generate contents that are toxic, harmful, during pre-training. misleading and biased. An important reason to fine-tune LLMs is to align the Instruction tuning, discussed above, gets LLMs a step "
216,Large Language Models A Survey.pdf,"ruction tuning, discussed above, gets LLMs a step responsestotheexpectationshumanswillhavewhenproviding closertobeingaligned.However,inmanycases,itisimportant instructionsthroughprompts.Thisistheso-calledinstruction toincludefurtherstepstoimprovethealignmentofthemodel tuning [133]. We dive into the details of how to design andavoidunintendedbehaviors3.Wereviewthemostpopular and engineer prompts in section IV-B, but in the context of instruction tuning, it is important to understand that the 3Acc"
217,Large Language Models A Survey.pdf,"uning, it is important to understand that the 3According to very recent research by Ethayarajh et al. [136], further instruction is a prompt that specifies the task that the LLM alignment besides SFT mainly improves models of at least 7B parameters. should accomplish. Instruction tuning datasets such as Natural Forsmallermodels,SFTissufficient.
approaches to alignment in this subsection. y ). Fig 31 shows a high-level comparison between KTO and w other alignment approaches discussed above. RLHF "
218,Large Language Models A Survey.pdf," other alignment approaches discussed above. RLHF (reinforcement learning from humanfeedback) and RLAIF (reinforcement learning from AI feedback) are two popular approaches. RLHF uses a reward model to learn alignment from human feedback. This reward model, after being tuned, is able to rate different outputs and score them accordingtotheiralignmentpreferencesgivenbyhumans.The reward model gives feedback to the original LLM and this feedbackisusedtotunetheLLMfurther[137].Reinforcement learningfr"
219,Large Language Models A Survey.pdf,"dtotunetheLLMfurther[137].Reinforcement learningfromAIfeedbackontheotherhand,directlyconnects a pretrained and well-aligned model to the LLM and helps it to learn from larger and more aligned models [138]. In another recent work (known as DPO) [139], Rafailov Fig. 31: LLM alignment involves supervised finetuning fol- et al. discussed that RLHF is a complex and often unstable lowed by optimizing a human-centered loss (HALO). How- procedure,andtriedtoaddressthiswithanewapproach.They ever, the pair"
220,Large Language Models A Survey.pdf,"dtoaddressthiswithanewapproach.They ever, the paired preferences that existing approaches need are leveraged a mapping between reward functions and optimal hard-to-obtain. In contrast, KTO uses a far more abundant policies to show that this constrained reward maximization kind of data, making it much easier to use in the real world. problemcanbeoptimizedexactlywithasinglestageofpolicy Courtesy of [136]. training, essentially solving a classification problem on the human preference data. The resu"
221,Large Language Models A Survey.pdf,"ion problem on the human preference data. The resulting algorithm, which they called Direct Preference Optimization (DPO), is stable, per- formant,andcomputationallylightweight,eliminatingtheneed H. Decoding Strategies for fitting a reward model, sampling from the LM during fine- Decodingreferstotheprocessoftextgenerationusingpre- tuning, or performing significant hyperparameter tuning. They trained LLMs. Given an input prompt, the tokenizer translates observedthatfine-tuningwithDPOexceedsRLHF’s"
222,Large Language Models A Survey.pdf,"slates observedthatfine-tuningwithDPOexceedsRLHF’sabilityto each token in the input text into a corresponding token ID. controlsentimentofgenerationsandimprovesresponsequality Then, the language model uses these token IDs as input and in summarization. Fig 30 shows the high-level comparison predicts the next most likely token (or a sequence of tokens). between DPO vs RLHF. Finally, the model generates logits, which are converted to probabilities using a softmax function. Different decoding strat"
223,Large Language Models A Survey.pdf,"using a softmax function. Different decoding strategies have been proposed. Some of the most popular ones are greedy search, beam search, as well as different sample techniques such as top-K, top-P (Nucleus sampling). 1)Greedy Search: Greedy search takes the most probable Fig.30:DPOoptimizesforhumanpreferenceswhileavoiding tokenateachstepasthenexttokeninthesequence,discarding reinforcement learning. Existing methods for fine-tuning lan- allotherpotentialoptions.Asyoucanimagine,thisisasimple guag"
224,Large Language Models A Survey.pdf,"otentialoptions.Asyoucanimagine,thisisasimple guage models with human feedback first fit a reward model approach and can loose a lot of temporal consistency and to a dataset of prompts and human preferences over pairs of coherency. It only considers the most probable token at each responses, and then use RL to find a policy that maximizes step, without considering the overall effect on the sequence. the learned reward. In contrast, DPO directly optimizes for This property makes it fast, but it a"
225,Large Language Models A Survey.pdf,"ptimizes for This property makes it fast, but it also means that it can miss the policy best satisfying the preferences with a simple classi- outonbettersequencesthatmighthaveappearedwithslightly fication objective, without an explicit reward function or RL. less probable next tokens. Courtesy of [139]. 2)BeamSearch: Unlikegreedysearchthatonlyconsiders the next most probable token, beam search takes into account the N most likely tokens, where N denotes the number of EvenmorerecentlyEthayarajhet"
226,Large Language Models A Survey.pdf,"denotes the number of EvenmorerecentlyEthayarajhetal.proposedanewalign- beams. This procedure is repeated until a predefined maxi- ment approach called the Kahneman-Tversky Optimization mum sequence length is reached or an end-of-sequence token (KTO)[136].Unlikeexistingstate-of-the-artapproaches,KTO appears. At this point, the sequence of tokens (AKA “beam”) does not require paired preference data (x, y , y ), and it with the highest overall score is chosen as the output. For w l only needs (x,y"
227,Large Language Models A Survey.pdf,"e is chosen as the output. For w l only needs (x,y) and knowledge of whether y is desirable or example for beam size of 2 and maximum length of 5, undesirable. KTO-aligned models are shown to be good or the beam search needs to keep track of 25 = 32 possible better than DPO-aligned models at scales from 1B to 30B, sequences.Soitismorecomputationallyintensivethangreedy despite not using paired preferences. KTO is also far easier to search. useintherealworldthanpreferenceoptimizationmethods,as 3)T"
228,Large Language Models A Survey.pdf,"erealworldthanpreferenceoptimizationmethods,as 3)Top-k Sampling: Top-k sampling is a technique that thekindofdataitneedsisfarmoreabundant.Asanexample, uses the probability distribution generated by the language everyretailcompanyhasalotofcustomerinteractiondataand model to select a token randomly from the k most likely whether that interaction was successful (e.g., purchase made) options. orunsuccessful(e.g.,nopurchasemade).However,Theyhave little to no counterfactual data (i.e., what would have"
229,Large Language Models A Survey.pdf,"e to no counterfactual data (i.e., what would have made Suppose we have 6 tokens (A, B, C, D, E, F) and k=2, an unsuccessful customer interaction y into a successful one and P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)= l
12.5%. In top-k sampling, tokens C, D, E, F are disregarded, RWKV: In [141], Peng et al. proposed a novel model and the model outputs A 60% of the time, and B, 40% of architecture, Receptance Weighted Key Value (RWKV), that the time. This approach ensures that we prioritize"
230,Large Language Models A Survey.pdf,"the time. This approach ensures that we prioritize the most combines the efficient parallelizable training of Transformers probable tokens while introducing an element of randomness withtheefficientinferenceofRNNs.Theirapproachleverages in the selection process. alinearattentionmechanismandallowsthemtoformulatethe model as either a Transformer or an RNN, which parallelizes The randomness is usually introduced via the concept of computations during training and maintains constant compu- temperatu"
231,Large Language Models A Survey.pdf,"g training and maintains constant compu- temperature.ThetemperatureTisaparameterthatrangesfrom tational and memory complexity during inference, leading to 0to1,whichaffectstheprobabilitiesgeneratedbythesoftmax the first non-transformer architecture to be scaled to tens of function, making the most likely tokens more influential. In billions of parameters. RWKV architecture is shown in Fig practice, it simply consists of dividing the input logits by 32.TheTimeComplexitycomparisonofRWKVwithdiffere"
232,Large Language Models A Survey.pdf,"by 32.TheTimeComplexitycomparisonofRWKVwithdifferent temperature value: exi/T softmax(x )= (3) i (cid:80) exj/T j A low temperature setting significantly alters the proba- bility distribution (and is commonly used in text generation to control the level of “creativity” in the generated output), while a large temperature prioritizes the tokens with higher probabilities. Top-k is a creative way of sampling, and can be used along with beam search. The sequence chosen by top- k sampling may not be t"
233,Large Language Models A Survey.pdf,"he sequence chosen by top- k sampling may not be the sequence with highest probability in beam search. But it’s important to remember that highest scores do not always lead to more realistic or meaningful sequences. 4)Top-p Sampling: Top-p sampling, also known as Nu- cleus sampling, takes a slightly different approach from top-k sampling. Instead of selecting the top k most probable tokens, nucleussamplingchoosesacutoffvaluepsuchthatthesumof the probabilities of the selected tokens exceeds p. Th"
234,Large Language Models A Survey.pdf,"probabilities of the selected tokens exceeds p. This forms a“nucleus”oftokensfromwhichtorandomlychoosethenext token. In other words, in top-p sampling the language model examines the most probable tokens in descending order and Fig. 32: RWKV architecture. Courtesy of [141]. keeps adding them to the list until the sum of probabilities surpasses the threshold p. As you can imagine, this could be betterspeciallyforscenariosinwhichtop-ktokensdonothave Transformers are provided in Fig 33. a large pro"
235,Large Language Models A Survey.pdf,"e Transformers are provided in Fig 33. a large probability mass. Unlike top-k sampling, the number of tokens included in the nucleus sampling is not fixed. This variability often results in a more diverse and creative output, making nucleus sampling popular for text generation related tasks. I. Cost-Effective Training/Inference/Adaptation/Compression In this part, we review some of the popular approaches used for more cost-friendly (and compute-friendly) training and usage of LLMs. Fig.33:TimeCo"
236,Large Language Models A Survey.pdf,"riendly) training and usage of LLMs. Fig.33:TimeComplexitycomparisonofRWKVwithdifferent 1)Optimized Training: There are many frameworks de- Transformers. Here T denotes the sequence length, d the veloped for optimized training of LLMs, here we introduce feature dimension, and c is MEGA’s chunk size of quadratic some of the prominent ones. attention. Courtesy of [141]. ZeRO: In [140], Rajbhandari et al. developed a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly impro"
237,Large Language Models A Survey.pdf,"Optimizer (ZeRO), to optimize memory, vastly improving training speed of LLMs while 2)Low-Rank Adaption (LoRA): Low-Rank Adaptation is increasingthemodelsizethatcanbeefficientlytrained.ZeRO a popular and lightweight training technique that significantly eliminates memory redundancies in data- and model-parallel reduces the number of trainable parameters, and is based training while retaining low communication volume and high on a crucial insight that the difference between the fine- computationa"
238,Large Language Models A Survey.pdf,"that the difference between the fine- computational granularity, allowing one to scale the model tuned weights for a specialized task and the initial pre-trained sizeproportionaltothenumberofdeviceswithsustainedhigh weights often exhibits “low intrinsic rank” - meaning that efficiency. it can be approximated well by a low rank matrix [142].
Fig. 35: A generic knowledge distillation framework with student and teacher (Courtesy of [144]). Fig. 34: An illustration of LoRA reparametrizan. Only A and"
239,Large Language Models A Survey.pdf,"An illustration of LoRA reparametrizan. Only A and B trained during this process. Courtesy of [142]. Knowledge can be transferred by different forms of learn- ing: response distillation, feature distillation, and API distilla- Training with LoRA is much faster, memory-efficient, and tion. Response distillation is concerned only with the outputs producessmallermodelweights(afewhundredMBs),thatare of the teacher model and tries to teach the student model easier to store and share. One property of "
240,Large Language Models A Survey.pdf, model easier to store and share. One property of low-rank matrices how to exactly or at least similarly perform (in the sense of is that they can be represented as the product of two smaller prediction) as the teacher. Feature distillation not only uses matrices.Thisrealizationleadstothehypothesisthatthisdelta the last layer but also intermediate layers as well to create a between fine-tuned weights and initial pre-trained weights can betterinnerrepresentationforthestudentmodel.Thishelpsthe be 
241,Large Language Models A Survey.pdf,"rrepresentationforthestudentmodel.Thishelpsthe be represented as the matrix product of two much smaller smaller model to have a similar representation as the teacher matrices. By focusing on updating these two smaller matrices model. rather than the entire original weight matrix, computational efficiency can be substantially improved. API distillation is the process of using an API (typically from an LLM provider such as OpenAI) to train smaller Specifically, for a pre-trained weight matrix W 0 "
242,Large Language Models A Survey.pdf,"Specifically, for a pre-trained weight matrix W 0 ∈ Rd×k, models. In the case of LLMs, it is used to train the model LoRA constrains its update by representing the latter with fromthedirectoutputofthelargermodelwhichmakesitvery a low-rank decomposition W 0 + ∆W = W 0 + BA, where similar to response distillation. Many concerns are raised by B ∈Rd×r , A∈Rr×k, and the rank r ≪min(d,k). During thistypeofdistillationbecauseincaseswherethemodelitself training, W 0 is frozen and does not receive gradie"
243,Large Language Models A Survey.pdf,"raining, W 0 is frozen and does not receive gradient updates, isnotopenlyavailable,a(usually)paidAPIisexposedforend while A and B contain trainable parameters. It is worth users.Ontheotherhand,whileuserspayforeachcall,howto mentioning that both W 0 and ∆W =BA are multiplied with use the predictions is limited, for example, OpenAI prohibits thesameinput,andtheirrespectiveoutputvectorsaresummed usage of its API to create LLMs that later will be used to coordinate-wise. For h = W 0 x, their modifie"
244,Large Language Models A Survey.pdf,"d to coordinate-wise. For h = W 0 x, their modified forward pass compete with it. The main value in such case is training data. yields: h=W x+∆Wx=W x+BAx. Usually a random 0 0 Gaussian initialization is used for A, and zero initialization 4)Quantization: deep learning in its core, is a set of for B, so ∆W = BA is zero at the beginning of training. mathematical functions applied to matrices, with a specific They then scale ∆Wx by αr, where α is a constant in r. This precision for model weights. R"
245,Large Language Models A Survey.pdf,"constant in r. This precision for model weights. Reducing the precision of the reparametrization is illustrated in Figure 34 weights can be used to reduce the size of the model and also make it faster. As an example, Float-32 operations compared It is worth mentioning that LoRA can be applied to any a to Int-8 operations are slower. This process, which is called subset of weight matrices in a neural network to reduce the quantization, can be applied in different phases. Main ap- number of traina"
246,Large Language Models A Survey.pdf,"ied in different phases. Main ap- number of trainable parameters. In the Transformer architec- proaches for model quantization can be categorized as: post ture,therearefourweightmatricesintheself-attentionmodule training quantization and quantization-aware training. Post- (W , W , W , W ), and two in the MLP module. Most of trainingquantizationisconcernedwithquantizedtrainedmod- q k v o the time, LoRA is focused on adapting the attention weights els in two well-known methods: dynamic and static."
247,Large Language Models A Survey.pdf,"els in two well-known methods: dynamic and static. Dynamic only for downstream tasks, and freezes the MLP modules, so post-training quantization computes the range of quantization they are not trained in downstream tasks both for simplicity ontheruntimeandisslowercomparedtostatic.Quantization- and parameter-efficiency. aware training adds quantization criteria into training, and a quantized model is trained and optimized during training 3)Knowledge Distillation: Knowledge distillation is the pro"
248,Large Language Models A Survey.pdf,ge Distillation: Knowledge distillation is the process. This approach ensures that the end model will have process of learning from a larger model [143]. Earlier days of goodperformanceandalsodoesnotneedtobequantizedafter best-performingmodelsreleasehaveproventhatthisapproach training. isveryusefulevenifitisusedinanAPIdistillationapproach. Itisalsoreferredtoasanapproachtodistilltheknowledgeof IV. HOWLLMSAREUSEDANDAUGMENTED not a single model but in fact multiple models into a smaller one. Creati
249,Large Language Models A Survey.pdf,"in fact multiple models into a smaller one. Creating smaller models by this approach yields smaller Once the LLMs are trained, we can use them to generate modelsizesthatcanbeusedevenonedgedevices.Knowledge desired outputs for a variety of tasks. LLMs can be used distillation as shown in Fig 35, illustrates a general setup of directly through basic prompting. However, in order to exploit this training scheme. their full potential or to address some of the shortcomings,
weneedtoaugmentthemodelsthr"
250,Large Language Models A Survey.pdf,"e of the shortcomings,
weneedtoaugmentthemodelsthroughsomeexternalmeans. 1) IntrinsicHallucinations:Thesedirectlyconflictwith In this section we first provide a brief overview of the main the source material, introducing factual inaccuracies shortcoming of LLMs, with a deeper look at the issue of or logical inconsistencies. hallucination.Wethendescribehowpromptingandsomeaug- 2) Extrinsic Hallucinations: These, while not contra- mentation approaches can not only address those limitations dicting,"
251,Large Language Models A Survey.pdf,"es can not only address those limitations dicting, are unverifiable against the source, encom- but also be used to augment the capabilities of LLMs going passing speculative or unconfirmable elements. as far as turning an LLM into a full-blown AI agent with the ability to interface with the external world. The definition of ’source’ in LLM contexts varies with the task. In dialogue-based tasks, it refers to ’world knowledge’, whereas in text summarization, it pertains to the input text A. LLM li"
252,Large Language Models A Survey.pdf,"arization, it pertains to the input text A. LLM limitations itself. This distinction plays a crucial role in evaluating and interpretinghallucinations.Theimpactofhallucinationsisalso ItisimportanttorememberthatLLMsaretrainedtopredict highly context-dependent. For instance, in creative endeavors a token. While fine-tuning and alignment improves their per- like poem writing, hallucinations might be deemed acceptable formanceandaddsdifferentdimensionstotheirabilities,there or even beneficial. are s"
253,Large Language Models A Survey.pdf,"nstotheirabilities,there or even beneficial. are still some important limitations that come up, particularly if they are used naively. Some of them include the following: LLMs, trained on diverse datasets including the internet, books, and Wikipedia, generate text based on probabilistic • They don’t have state/memory. LLMs on their own models without an inherent understanding of truth or falsity. cannot remember even what was sent to them in the Recent advancements like instruct tuning and Reinf"
254,Large Language Models A Survey.pdf,"Recent advancements like instruct tuning and Reinforcement previous prompt. That is an important limitation for Learning from Human Feedback (RLHF) have attempted to manyoftheusescasesthatrequiresomeformofstate. steerLLMstowardsmorefactualoutputs,butthefundamental probabilistic nature and its inherent limitations remain. A • Theyarestochastic/probabilistic.Ifyousendthesame recent study, “Sources of Hallucination by Large Language prompttoanLLMseveraltimes,youarelikelytoget Models on Inference Ta"
255,Large Language Models A Survey.pdf,"eraltimes,youarelikelytoget Models on Inference Tasks” [146], highlights two key aspects different responses. While there are parameters, and contributing to hallucinations in LLMs: the veracity prior and in particular the temperature, to limit the variability the relative frequency heuristic, underscoring the complexities in the response, this is an inherent property of their inherent in LLM training and output generation. training that can create issues. Effective automated measurement of hall"
256,Large Language Models A Survey.pdf,"te issues. Effective automated measurement of hallucinations in • They have stale information and, on their own, don’t LLMs requires a combination of statistical and model-based haveaccesstoexternaldata.AnLLMonitsowndoes metrics. notevenknowaboutthecurrenttimeordayanddoes nothaveaccesstoanyinformationthatwasnotpresent Statistical Metrics: in its training set. • MetricslikeROUGE[147]andBLEU[148]arecom- • They are generally very large. This means that many monforassessingtextsimilarity,focusingoni"
257,Large Language Models A Survey.pdf,"hat many monforassessingtextsimilarity,focusingonintrinsic costly GPU machines are needed for training and hallucinations. serving. In some cases, largest models have poor SLAs, particularly in terms of latency. • Advanced metrics such as PARENT [149], PARENT- T [150], and Knowledge F1 [151] are utilized when • They hallucinate. LLMs do not have a notion of structured knowledge sources are available. These ”truth” and they have usually been trained on a mix metrics, while effective, have limitat"
258,Large Language Models A Survey.pdf,"ed on a mix metrics, while effective, have limitations in capturing of good and bad content. They can produce very syntactic and semantic nuances. plausible but untruthful answers. Model-Based Metrics: While the previous limitations can all become important for some applications, it is worth for us to dive a bit into the • IE-Based Metrics: Utilize Information Extraction last one, hallucinations, since it has gathered a lot of interest models to simplify knowledge into relational tuples, over th"
259,Large Language Models A Survey.pdf,"simplify knowledge into relational tuples, over the past few months and it has also sparked many of the then compare these with the source. prompt approaches and LLM augmentation methods we will later describe. • QA-Based Metrics: Assess the overlap between gen- erated content and the source through a question- Hallucination: In the realm of Large Language Models answering framework (see [152]). (LLMs), the phenomenon of ”hallucinations” has garnered significant attention. Defined in the literat"
260,Large Language Models A Survey.pdf,"ered significant attention. Defined in the literature, notably in the • NLI-BasedMetrics:UseNaturalLanguageInference ”Survey of Hallucination in Natural Language Generation” datasets to evaluate the truthfulness of a generated paper [145], hallucination in an LLM is characterized as hypothesis based on a given premise (see [153]). ”the generation of content that is nonsensical or unfaithful to the provided source.” This terminology, although rooted in • Faithfulness Classification Metrics: Offer"
261,Large Language Models A Survey.pdf,"ed in • Faithfulness Classification Metrics: Offer a refined psychological parlance, has been appropriated within the field assessment by creating task-specific datasets for a of artificial intelligence. nuanced evaluation (see [154]). Hallucinations in LLMs can be broadly categorized into Despite advances in automated metrics, human judgment two types: remains a vital piece. It typically involves two methodologies:
How LLMs Are Used and Augmented Statistical Metrics Automated metrics IE-Based M"
262,Large Language Models A Survey.pdf,d Statistical Metrics Automated metrics IE-Based Metrics Hallucination Model-Based Metrics QA-Based Metrics Hallucination Quantification NLI-Based Metrics Scoring Human judgment Comparative Analysis A) LLM limitations Prompt Design and Engineering 1) Chain of Thought 2) Tree of Thought 5) Expert Prompting 7) Rails 8) Automatic Prompt Engineering Zero-Shot CoT 3) Self-Consistency 6) Chains Topical Rails Prompt Generation Manual CoT Fact-Checking Rails Prompt Scoring 4) Reflection Jailbreaking Rai
263,Large Language Models A Survey.pdf,ails Prompt Scoring 4) Reflection Jailbreaking Rails Refinement and Iteration B) Using LLMs Components of a RAG RAG Tools a) RAG-aware prompting techniques Retrieval Generation LangChain Meltano Augmentation LlamaIndex Cohere Coral HayStack Flowise AI B) Augmenting LLMs through external knowledge - RAG a) Tool-aware prompting techniques C) Using External Tools Functionality of an LLM-based agent Prompt engineering techniques for agents Tool Access and Utilization Reasoning without Observation De
264,Large Language Models A Survey.pdf,s and Utilization Reasoning without Observation Decision Making Reason and Act Dialog-Enabled Resolving Agents D) LLM Agents Fig. 36: How LLMs Are Used and Augmented. 1) Scoring: Human evaluators rate the level of halluci- Maintainingandanalyzingatrackingsetofhallucina- nation within a predefined scale. tions is essential for ongoing model improvement. 2) Comparative Analysis: Evaluators compare gener- • Prompt Engineering and Metaprompt Design. Many ated content against baseline or ground-truth
265,Large Language Models A Survey.pdf,"Many ated content against baseline or ground-truth refer- of the advanced prompt techniques described in IV-B ences, adding an essential layer of subjective assess- such as Retrieval Augmented Generation directly ad- ment. dress hallucination risks. FactScore[155]isarecentexampleofametricthatcanbe • Model Selection and Configuration for Hallucination used both for human and model-based evaluation. The metric Mitigation. For exemple, larger models with lower breaksanLLMgenerationinto“atomicfacts”"
266,Large Language Models A Survey.pdf," with lower breaksanLLMgenerationinto“atomicfacts”.Thefinalscore temperature settings usually perform better. Also, is computed as the sum of the accuracy of each atomic fact, techniques such as RLHF or domain-sepcific fine- givingeachofthemequalweight.Accuracyisabinarynumber tuning can mitigate hallucination risks. that simply states whether the atomic fact is supported by the source. The authors implement different automation strategies that use LLMs to estimate this metric. B. Using LLMs: Pro"
267,Large Language Models A Survey.pdf,"e LLMs to estimate this metric. B. Using LLMs: Prompt Design and Engineering Finally,mitigatinghallucinationsinLLMsisamultifaceted A prompt in generative AI models is the textual input challenge, requiring tailored strategies to suit various applica- provided by users to guide the model’s output. This could tions. Those include: rangefromsimplequestionstodetaileddescriptionsorspecific tasks. Prompts generally consist of instructions, questions, • Product Design and User Interaction Strategies su"
268,Large Language Models A Survey.pdf," Product Design and User Interaction Strategies such input data, and examples. In practice, to elicit a desired as use case design, structuring the input/output, or response from an AI model, a prompt must contain either providing mechanisms for user feedback. instructions or questions, with other elements being optional. • Data Management and Continuous Improvement. Advanced prompts involve more complex structures, such as
”chain of thought” prompting, where the model is guided to such examples"
269,Large Language Models A Survey.pdf,"mpting, where the model is guided to such examples of step by step reasoning by hand is hard and follow a logical reasoning process to arrive at an answer. error prone. That is where automatic CoT [157] comes into play. Prompt engineering is a rapidly evolving discipline that shapes the interactions and outputs of LLMs and other gen- 2)Tree of Thought (ToT): The Tree of Thought (ToT) erative AI models. The essence of prompt engineering lies in [158] prompting technique is inspired by the concept"
270,Large Language Models A Survey.pdf,"58] prompting technique is inspired by the concept of crafting the optimal prompt to achieve a specific goal with considering various alternative solutions or thought processes a generative model. This process is not only about instructing before converging on the most plausible one. ToT is based themodelbutalsoinvolvessomeunderstandingofthemodel’s on the idea of branching out into multiple ”thought trees” capabilities and limitations, and the context within which it where each branch represents"
271,Large Language Models A Survey.pdf,"ntext within which it where each branch represents a different line of reasoning. operates. This method allows the LLM to explore various possibilities and hypotheses, much like human cognitive processes where Prompt engineering transcends the mere construction of multiple scenarios are considered before determining the most prompts;itrequiresablendofdomainknowledge,understand- likely one. ing of the AI model, and a methodical approach to tailor prompts for different contexts. This might involve"
272,Large Language Models A Survey.pdf,"prompts for different contexts. This might involve creating AcriticalaspectofToTistheevaluationofthesereasoning templates that can be programmatically modified based on a paths. As the LLM generates different branches of thought, givendatasetorcontext.Forexample,generatingpersonalized each is assessed for its validity and relevance to the query. responses based on user data might use a template that is This process involves real-time analysis and comparison of dynamically filled with relevant us"
273,Large Language Models A Survey.pdf," comparison of dynamically filled with relevant user information. the branches, leading to a selection of the most coherent and logical outcome. Furthermore, prompt engineering is an iterative and ex- ploratory process, akin to traditional machine learning prac- ToT is particularly useful in complex problem-solving tices such as model evaluation or hyperparameter tuning. The scenarios where a single line of reasoning might not suffice. rapidgrowthofthisfieldsuggestsitspotentialtorevolutionize It"
274,Large Language Models A Survey.pdf,"hofthisfieldsuggestsitspotentialtorevolutionize It allows LLMs to mimic a more human-like problem-solving certainaspectsofmachinelearning,movingbeyondtraditional approach, considering a range of possibilities before arriving methods like feature or architecture engineering. On the other at a conclusion. This technique enhances the model’s ability hand, traditional engineering practices such as version con- tohandleambiguity,complexity,andnuancedtasks,makingit trol and regression testing need to "
275,Large Language Models A Survey.pdf,"asks,makingit trol and regression testing need to be adapted to this new a valuable tool in advanced AI applications. paradigmjustliketheywereadaptedtoothermachinelearning approaches [156]. 3)Self-Consistency: Self-Consistency [159] utilizes an ensemble-based method, where the LLM is prompted to gen- In the following paragraphs we detail some of the most erate multiple responses to the same query. The consistency interesting and popular prompt engineering approaches. amongtheseresponsesservesasa"
276,Large Language Models A Survey.pdf,"gineering approaches. amongtheseresponsesservesasanindicatoroftheiraccuracy 1)Chain of Thought (CoT): The Chain of Thought (CoT) and reliability. technique, initially described in the paper “Chain-of-Thought TheSelf-Consistencyapproachisgroundedintheprinciple Prompting Elicits Reasoning in Large Language Models”[34] that if an LLM generates multiple, similar responses to the by Google researchers, represents a pivotal advancement in same prompt, it is more likely that the response is accurate. p"
277,Large Language Models A Survey.pdf,"it is more likely that the response is accurate. prompt engineering for Large Language Models (LLMs). This method involves asking the LLM to tackle a query mul- This approach hinges on the understanding that LLMs, while tiple times, each time analyzing the response for consistency. proficient in token prediction, are not inherently designed for This technique is especially useful in scenarios where factual explicit reasoning. CoT addresses this by guiding the model accuracy and precision are par"
278,Large Language Models A Survey.pdf,"y guiding the model accuracy and precision are paramount. through essential reasoning steps. The consistency of responses can be measured using vari- CoT is based on making the implicit reasoning process of ous methods. One common approach is to analyze the overlap LLMs explicit. By outlining the steps required for reasoning, in the content of the responses. Other methods may include the model is directed closer to a logical and reasoned output, comparing the semantic similarity of responses or "
279,Large Language Models A Survey.pdf,comparing the semantic similarity of responses or employing especially in scenarios demanding more than simple informa- more sophisticated techniques like BERT-scores or n-gram tion retrieval or pattern recognition. overlaps. These measures help in quantifying the level of CoT prompting manifests in two primary forms: agreement among the responses generated by the LLM. 1) Zero-Shot CoT: This form involves instructing the Self-Consistency has significant applications in fields LLM to “think step 
280,Large Language Models A Survey.pdf,"ificant applications in fields LLM to “think step by step”, prompting it to de- where the veracity of information is critical. It is particularly construct the problem and articulate each stage of relevant in scenarios like fact-checking, where ensuring the reasoning. accuracy of information provided by AI models is essential. 2) Manual CoT: A more complex variant, it requires By employing this technique, prompt engineers can enhance providing step-by-step reasoning examples as tem- the trustwor"
281,Large Language Models A Survey.pdf,"ep-by-step reasoning examples as tem- the trustworthiness of LLMs, making them more reliable for plates for the model. While yielding more effective tasks that require high levels of factual accuracy. results, it poses challenges in scalability and mainte- 4)Reflection: Reflection [160] involves prompting LLMs nance. to assess and potentially revise their own outputs based on Manual CoT is more effective than zero-shot. However, reasoning about the correctness and coherence of their re- the effe"
282,Large Language Models A Survey.pdf,"he correctness and coherence of their re- the effectiveness of this example-based CoT depends on the sponses. The concept of Reflection centers on the ability of choice of diverse examples, and constructing prompts with LLMs to engage in a form of self-evaluation. After generating
an initial response, the model is prompted to reflect on its 8)Automatic Prompt Engineering (APE): Automatic own output, considering factors like factual accuracy, logical Prompt Engineering (APE) [163] focuses on auto"
283,Large Language Models A Survey.pdf,"cal Prompt Engineering (APE) [163] focuses on automating the consistency,andrelevance.Thisintrospectiveprocesscanlead process of prompt creation for Large Language Models to the generation of revised or improved responses. (LLMs). APE seeks to streamline and optimize the prompt designprocess,leveragingthecapabilitiesofLLMsthemselves A key aspect of Reflection is the LLM’s capacity for to generate and evaluate prompts. APE involves using LLMs self-editing. By evaluating its initial response, the "
284,Large Language Models A Survey.pdf,"-editing. By evaluating its initial response, the model can in a self-referential manner where the model is employed identifypotentialerrorsorareasofimprovement.Thisiterative to generate, score, and refine prompts. This recursive use of processofgeneration,reflection,andrevisionenablestheLLM LLMs enables the creation of high-quality prompts that are torefineitsoutput,enhancingtheoverallqualityandreliability more likely to elicit the desired response or outcome. of its responses. Themethodologyof"
285,Large Language Models A Survey.pdf,"nse or outcome. of its responses. ThemethodologyofAPEcanbebrokendownintoseveral 5)ExpertPrompting: ExpertPrompting[161]enhancesthe key steps: capabilities of Large Language Models (LLMs) by simulating theresponsesofexpertsinvariousfields.Thismethodinvolves • Prompt Generation: The LLM generates a range of prompting the LLMs to assume the role of an expert and re- potential prompts based on a given task or objective. spond accordingly, providing high-quality, informed answers. A key strategy with"
286,Large Language Models A Survey.pdf,"igh-quality, informed answers. A key strategy within Expert Prompting is the multi-expert • Prompt Scoring: Each generated prompt is then approach. The LLM is prompted to consider responses from evaluated for its effectiveness, often using criteria multiple expert perspectives, which are then synthesized to like clarity, specificity, and likelihood of eliciting the form a comprehensive and well-rounded answer. This tech- desired response. nique not only enhances the depth of the response but als"
287,Large Language Models A Survey.pdf,"ot only enhances the depth of the response but also • Refinement and Iteration: Based on these evalua- incorporates a range of viewpoints, reflecting a more holistic tions,promptscanberefinedanditeratedupon,further understanding of the subject matter. enhancing their quality and effectiveness. 6)Chains: Chains refer to the method of linking multiple componentsinasequencetohandlecomplextaskswithLarge C. Augmenting LLMs through external knowledge - RAG Language Models (LLMs). This approach involve"
288,Large Language Models A Survey.pdf," RAG Language Models (LLMs). This approach involves creating a series of interconnected steps or processes, each contributing One of the main limitations of pre-trained LLMs is their to the final outcome. The concept of Chains is based on lack of up-to-date knowledge or access to private or use- the idea of constructing a workflow where different stages case-specific information. This is where retrieval augmented or components are sequentially arranged. Each component in generation (RAG) comes i"
289,Large Language Models A Survey.pdf,"ranged. Each component in generation (RAG) comes into the picture [164]. RAG, illus- a Chain performs a specific function, and the output of one trated in figure 37, involves extracting a query from the input serves as the input for the next. This end-to-end arrangement prompt and using that query to retrieve relevant information allows for more complex and nuanced processing, as each from an external knowledge source (e.g. a search engine or a stage can be tailored to handle a specific aspect o"
290,Large Language Models A Survey.pdf,"tage can be tailored to handle a specific aspect of the task. knowledge graph, see figure 38 ). The relevant information is Chains can vary in complexity and structure, depending on thenaddedtotheoriginalpromptandfedtotheLLMinorder the requirements. In “PromptChainer: Chaining Large Lan- for the model to generate the final response. A RAG system guage Model Prompts through Visual Programming” [162], includes three important components: Retrieval, Generation, theauthorsnotonlydescribethemainchall"
291,Large Language Models A Survey.pdf," Generation, theauthorsnotonlydescribethemainchallengesindesigning Augmentation [165]. chains, but also describe a visual tool to support those tasks. a)RAG-aware prompting techniques: Because of the 7)Rails: Rails in advanced prompt engineering refer to importance of RAG to build advanced LLM systems, several a method of guiding and controlling the output of Large RAG-aware prompting techniques have been developed re- Language Models (LLMs) through predefined rules or tem- cently.Onesuchtechniq"
292,Large Language Models A Survey.pdf,"ugh predefined rules or tem- cently.OnesuchtechniqueisForward-lookingActiveRetrieval plates. This approach is designed to ensure that the model’s Augmented Generation (FLARE) responsesadheretocertainstandardsorcriteria,enhancingthe Forward-looking Active Retrieval Augmented Generation relevance, safety, and accuracy of the output. The concept of (FLARE) [168] enhances the capabilities of Large Language Rails involves setting up a framework or a set of guidelines Models (LLMs) by iteratively comb"
293,Large Language Models A Survey.pdf,"et of guidelines Models (LLMs) by iteratively combining prediction and in- that the LLM must follow while generating responses. These formation retrieval. FLARE represents an evolution in the guidelines are typically defined using a modeling language or use of retrieval-augmented generation, aimed at improving the templates known as Canonical Forms, which standardize the accuracy and relevance of LLM responses. way natural language sentences are structured and delivered. FLARE involves an iterat"
294,Large Language Models A Survey.pdf,"structured and delivered. FLARE involves an iterative process where the LLM Rails can be designed for various purposes, depending on actively predicts upcoming content and uses these predictions the specific needs of the application: as queries to retrieve relevant information. This method con- • Topical Rails: Ensure that the LLM sticks to a trastswithtraditionalretrieval-augmentedmodelsthattypically particular topic or domain. retrieveinformationonceandthenproceedwithgeneration.In FLARE, this "
295,Large Language Models A Survey.pdf,"ononceandthenproceedwithgeneration.In FLARE, this process is dynamic and ongoing throughout the • Fact-Checking Rails: Aimed at minimizing the gen- generationphase.InFLARE,eachsentenceorsegmentgener- eration of false or misleading information. atedbytheLLMisevaluatedforconfidence.Iftheconfidence • JailbreakingRails:PreventtheLLMfromgenerating levelisbelowacertainthreshold,themodelusesthegenerated responses that attempt to bypass its own operational content as a query to retrieve relevant informa"
296,Large Language Models A Survey.pdf,"al content as a query to retrieve relevant information, which is constraints or guidelines. then used to regenerate or refine the sentence. This iterative
Fig. 37: An example of synthesizing RAG with LLMs for question answering application [166]. examples, the LLM decides to call an external Q&A tool, a calculator, and a Wikipedia Search Engine More recently, researchersatBerkeleyhavetrainedanewLLMcalledGorilla [67] that beats GPT-4 at the use of APIs, a specific but quite general tool. a)Tool-a"
297,Large Language Models A Survey.pdf," APIs, a specific but quite general tool. a)Tool-awarepromptingtechniques: Similarlytowhat was described with RAG, several tool-aware prompting ap- proaches have been developed to make usage of tools more scalable.ApopulartechniqueisthesocalledAutomaticMulti- step Reasoning and Tool-use (ART). Fig. 38: This is one example of synthesizing the KG as a AutomaticMulti-stepReasoningandTool-use(ART)[170] retriever with LLMs [167]. is a prompt engineering technique that combines automated chain of thou"
298,Large Language Models A Survey.pdf,"ng technique that combines automated chain of thought prompting with the use of external tools. ARTrepresents aconvergence ofmultiple promptengineering strategies, enhancing the ability of Large Language Models process ensures that each part of the response is informed by (LLMs) to handle complex tasks that require both reasoning the most relevant and current information available. and interaction with external data sources or tools. FormoredetailsonRAGframeworkanditsrelevantworks, ART involves "
299,Large Language Models A Survey.pdf,"lsonRAGframeworkanditsrelevantworks, ART involves a systematic approach where, given a task we refer the readers to this survey of retrieval augmented and input, the system first identifies similar tasks from a task generations [165]. library. These tasks are then used as examples in the prompt, guiding the LLM on how to approach and execute the current D. Using External Tools task.Thismethodisparticularlyeffectivewhentasksrequirea Retrieving information from an external knowledge source combina"
300,Large Language Models A Survey.pdf,"ormation from an external knowledge source combinationofinternalreasoningandexternaldataprocessing asdescribedaboveisonlyoneofthepotentialwaystoaugment or retrieval. an LLM. More generally, an LLM can access any number of external tools (e.g. an API to a service) to augment its E. LLM Agents functionality. In that regards, RAG can be seen as a specific TheideaofAIagentshasbeenwell-exploredinthehistory instance of the broader category of the so called ”tools”. of AI. An agent is typically an auto"
301,Large Language Models A Survey.pdf,"lled ”tools”. of AI. An agent is typically an autonomous entity that can Tools in this context are external functions or services that perceive the environment using its sensors, make a judgment LLMs can utilize. These tools extend the range of tasks an basedonthestateitcurrentlyis,andaccordinglyactbasedon LLMcanperform,frombasicinformationretrievaltocomplex the actions that are available to it. interactions with external databases or APIs. In the context of LLMs, an agent refers to a system bas"
302,Large Language Models A Survey.pdf,"e context of LLMs, an agent refers to a system based In the paper ”Toolformer: Language Models Can Teach on a specialized instantiation of an (augmented) LLM that ThemselvestoUseTools”[169],theauthorsgobeyondsimple is capable of performing specific tasks autonomously. These tool usage by training an LLM to decide what tool to use agents are designed to interact with users and environment to when, and even what parameters the API needs. Tools include make decisions based on the input and the inte"
303,Large Language Models A Survey.pdf,"ude make decisions based on the input and the intended goal of two different search engines, or a calculator. In the following the interaction. Agents are based on LLMs equipped with the
abilitytoaccessandusetools,andtomakedecisionsbasedon oruncertain,allowingtheLLM-basedagenttomaintainahigh thegiveninput.Theyaredesignedtohandletasksthatrequire level of performance and reliability. a degree of autonomy and decision-making, typically beyond Reason and Act (ReAct)[176] prompts LLMs to generate sim"
304,Large Language Models A Survey.pdf," and Act (ReAct)[176] prompts LLMs to generate simple response generation. not only verbal reasoning but also actionable steps, thus The functionalities of a generic LLM-based agent include: enhancing the model’s dynamic problem-solving capabilities. ReAct is grounded in the principle of integrating reasoning • Tool Access and Utilization: Agents have the capabil- withaction.Inthisapproach,theLLMispromptedtoalternate ity to access external tools and services, and to utilize between generating re"
305,Large Language Models A Survey.pdf,"and services, and to utilize between generating reasoning traces (explanations) and taking these resources effectively to accomplish tasks. actions (steps or commands) in an interleaved manner. This approachallowsthemodeltodynamicallyreasonaboutaprob- • Decision Making: They can make decisions based on lem, and propose and take concrete actions simultaneously. the input, context, and the tools available to them, often employing complex reasoning processes. Dialog-Enabled Resolving Agents (DERA) "
306,Large Language Models A Survey.pdf,"processes. Dialog-Enabled Resolving Agents (DERA) [177] are spe- cializedAIagentsthatcanengageindialogue,resolvequeries, As an example, an LLM that has access to a function (or and make decisions based on interactive exchanges. DERA anAPI)suchasweatherAPI,canansweranyquestionrelated is developed based on the idea of utilizing multiple agents to the weather of the specific place. In other words, it can use withinadialogcontext,eachwithspecificrolesandfunctions. APIs to solve problems. Furthermore"
307,Large Language Models A Survey.pdf,"sandfunctions. APIs to solve problems. Furthermore, if that LLM has access TheseagentscanincludeResearchers,whogatherandanalyze to an API that allows to make purchases, a purchasing agent information, and Deciders, who make final judgments based can be built to not only have capabilities to read information on the information provided. This division of roles allows for from the external world, but also act on it [171]. a well-organized and efficient approach to problem-solving and decision-makin"
308,Large Language Models A Survey.pdf,"ent approach to problem-solving and decision-making. DERA is particularly advantageous in Fig. 40 shows another example of LLM-based agents for scenarios requiring complex decision-making and problem- conversational information seeking [36], where an LLM is solving, such as those in medical diagnostics or customer ser- augmented with a set of plug-and-play modules, including vice. The collaborative and interactive nature of DERA agents a working memory that tracks the dialog state, a policy that"
309,Large Language Models A Survey.pdf,"memory that tracks the dialog state, a policy that allows them to handle intricate queries with a level of depth makes an execution plan for the task and selects next system and nuance that single-agent systems might struggle with. action, an action executor that performs an action selected by Moreover, this approach aligns well with human decision- the policy (consolidating evidence from external knowledge, making processes, making AI reasoning more relatable and or prompting the LLM to generat"
310,Large Language Models A Survey.pdf,"more relatable and or prompting the LLM to generate responses), and a utility trustworthy. that accesses the alignment of the LLM’s responses with user expectations or specific business requirements, and generate V. POPULARDATASETSFORLLMS feedback to improve agent performance. Large language models exhibit promising accomplish- FormoredetailsonLLM-basedAIagentsseerecentsurvey ments, but the main question that arises is how effectively [172], [173], [174]. they function and how their performance "
311,Large Language Models A Survey.pdf,"], [174]. they function and how their performance can be assessed in specific tasks or applications. a)Prompt engineering techniques for agents: Like RAG and Tools, prompt engineering techniques that specif- The evaluation of LLMs poses particular challenges due ically address the needs of LLM-based agents have been to the evolving landscape of their applications. The original developed. Three such examples are Reasoning without Ob- intent behind developing LLMs was to boost the performance serv"
312,Large Language Models A Survey.pdf," developing LLMs was to boost the performance servation (ReWOO), Reason and Act (ReAct), and Dialog- of NLP tasks such as translation, summarization, question- Enabled Resolving Agents (DERA). answering, and so on [178]. However, it is evident today that these models are finding utility across diverse domains Reasoning without Observation (ReWOO) [175] aims to including code generation and finance. Moreover, the eval- decouplereasoningfromdirectobservations.ReWOOoperates uation of LLMs encompass"
313,Large Language Models A Survey.pdf,"bservations.ReWOOoperates uation of LLMs encompasses several critical considerations byenablingLLMstoformulatecomprehensivereasoningplans such as fairness and bias, fact-checking, and reasoning. In or meta-plans without immediate reliance on external data this section, we outline the commonly used benchmarks for or tools. This approach allows the agent to create a struc- assessing LLMs. These benchmarks are categorized based on tured framework for reasoning that can be executed once the training"
314,Large Language Models A Survey.pdf,"r reasoning that can be executed once the training or evaluating the LLM Capabilities. necessary data or observations are available. In ReWOO, the LLM initially develops a plan (a series of steps) that outlines A. Datasets for Basic Tasks: language model- how to approach and solve a given problem. This meta- ing/understanding/generation planning phase is crucial as it sets the stage for the agent to process information once it becomes available. The execution This section provides an overview of"
315,Large Language Models A Survey.pdf,"The execution This section provides an overview of the benchmarks and phasetheninvolvesintegratingactualdataorobservationsinto datasets suited to evaluate the basic abilities of LLMs. the pre-specified plan, leading to coherent and contextually relevant responses. ReWOO offers significant advantages in • NaturalQuestions[179]isaQAdatasetthatconsists terms of token efficiency and robustness to tool failure. It of real anonymized, aggregated queries submitted to enables LLMs to handle tasks where "
316,Large Language Models A Survey.pdf,"s submitted to enables LLMs to handle tasks where immediate access to the Google search engine as questions. An annotator external data is not available, relying instead on a well- is presented with a question along with a Wikipedia structured reasoning framework. This method is particularly page from the top 5 search results, and annotates a advantageous in scenarios where data retrieval is costly, slow, longanswer(typicallyaparagraph)andashortanswer
Fig. 39: HuggingGPT: An agent-based approach"
317,Large Language Models A Survey.pdf,"nswer
Fig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]] task description, a code solution, and three automated test cases. • HumanEval [182] is a dataset for code generation task. This dataset consists of 164 hand-crafted pro- gramming challenges. Each challenge is accompanied byafunctionsignature,docstring,codebody,andmul- tiple unit tests. The main intuition behind developing thisdatasetistoguaranteetheexclusionofitscontents from training datasets"
318,Large Language Models A Survey.pdf,"eetheexclusionofitscontents from training datasets for code generation models. • APPS [183] is designed for code generation task focusing on the Python programming language. The APPSdatasetcontainsacollectionof232,444Python programs.Eachprograminthedatasethasanaverage Fig. 40: A LLM-based agent for conversational information of 18 lines of Python code. Additionally, APPS offers seeking. Courtesy of [36]. access to a repository of 10,000 unique programming exercises, each with text-based problem "
319,Large Language Models A Survey.pdf,"ogramming exercises, each with text-based problem descriptions. The final aspect to highlight is that the it includes test cases. (oneormoreentities)ifpresentonthepage,ormarks • WikiSQL[184]iscraftedforcodegenerationtaskand null if no long/short answer is present. it has 87,726 carefully labeled pairs of SQL queries • MMLU [180] is intended to evaluate the knowl- and corresponding natural language questions from edge gained in zero-shot and few-shot scenarios. That Wikipedia tables. The SQL quer"
320,Large Language Models A Survey.pdf,"hot scenarios. That Wikipedia tables. The SQL queries comprise three means that MMLU assesses both the general knowl- subsets: test sets (17,284 examples), development edgeandproblem-solvingabilityofamodel.Itcovers (9,145 examples), and training (61,297 examples). 57 subjects in STEM, humanities, social sciences, and other areas. The benchmark varies in complexity, • TriviaQA [185] is designed for QA task. This ranging from elementary to advanced professional. dataset comprises more than 650,000"
321,Large Language Models A Survey.pdf," professional. dataset comprises more than 650,000 question- It is worth mentioning that the main contribution of answer-evidence triples. There are 95,000 question- this dataset is for multi-task language understanding, answerpairsinthisdataset,eachauthoredbytriviaen- question answering, and arithmetic reasoning. thusiasts and supported by an average of six indepen- dentlysourcedevidencedocuments.Thesedocuments • MBPP [181] stands for “Mostly Basic Python Prob- are automatically acquired from W"
322,Large Language Models A Survey.pdf,"sic Python Prob- are automatically acquired from Wikipedia or broader lems” and provides a benchmark for evaluating the web search results. The dataset is categorized into performance of models designed for code generation. two segments, including those with authentic answers The benchmark encompasses 974 short Python pro- from Wikipedia and web domains, and verified sets grams including a wide range of topics, including embodythe accuratelyansweredquestions alongwith fundamental programming con"
323,Large Language Models A Survey.pdf,"redquestions alongwith fundamental programming concepts and standard li- their associated documents from both Wikipedia and brary usage, and more. Each challenge comprises a online.
Fig. 41: Dataset applications. • RACE [186] suits for reading comprehension task. is the synthesis of RACE-M and RACE-H. This dataset is based on English tests completed by Chinesestudentsfrommiddleschoolandhighschool, • SQuAD [187] stands for “Stanford Question Answer- aged 12 to 18, and it contains roughly 28,000 t"
324,Large Language Models A Survey.pdf,"r- aged 12 to 18, and it contains roughly 28,000 texts ing Dataset” and is a crowdsourced reading compre- and 100,000 questions rigorously prepared by human hension dataset based on Wikipedia articles. It has specialists, primarily English instructors. This dataset approximately 100,000 question-answer pairs con- contains a wide range of subjects that were purpose- nected to more than 500 articles. The answers to fully chosen to assess students’ comprehension and these questions are typically te"
325,Large Language Models A Survey.pdf,"comprehension and these questions are typically text fragments or spans reasoning abilities. This dataset is available in three taken from the corresponding reading passages. The subgroups: RACE-M, RACE-H, and RACE. RACE- questions may be unanswerable in some cases. The M refers to the middle school examinations, whereas dataset is divided into three sets: an 80% training set, RACE-Hdenotesthehighschooltests.Finally,RACE a 10% development set, and a 10% hidden test set.
Fig. 42: Datasets license"
326,Large Language Models A Survey.pdf,"d a 10% hidden test set.
Fig. 42: Datasets licensed under different licenses. • BoolQ [188] is a yes/no question-answering dataset • GSM8K [190] is designed to evaluate the model’s where the goal is reading comprehension task. BoolQ abilityformulti-stepmathematicalreasoning.GSM8K includes 15,942 examples. Each example is a triplet includes8.5Klinguisticallydiversegradeschoolmath that includes a question, a relevant paragraph, and wordproblemswrittenbyhumans.Thedatasetissplit the solution. Althou"
327,Large Language Models A Survey.pdf,"tenbyhumans.Thedatasetissplit the solution. Although the main intuition behind into two sets: a training set with 7.5K problems, this dataset is for reading comprehension, it can be and a test set with 1K problems. These problems used for reasoning, natural language inference, and need 2 to 8 steps to be solved. Solutions mainly question-answering tasks. are a series of elementary calculations using basic arithmetic operations. • MultiRC [189] is another dataset that fits reading comprehension t"
328,Large Language Models A Survey.pdf," another dataset that fits reading comprehension task. MultiRC contains brief para- • MATH [191] enables to assess how well models can graphs as well as multi-sentence questions that can solve math problems. MATH dataset hast 12, 500 be answered using the information in the paragraph. problems from high school math competitions. Each The paragraphs in this dataset come from a variety problem in the dataset has a step-by-step solution and of sources, including news, fiction, historical texts, a f"
329,Large Language Models A Survey.pdf,"es, including news, fiction, historical texts, a final answer enclosed in a box. The problems cover Wikipedia articles, discussions on society and law, a wide range of topics and have different levels of elementary school science textbooks, and 9/11 re- complexity. There are seven subjects in total. Further- ports. Each question has many response choices, with more, the difficulty of each problem is rated based one or more of them being correct. Answering the on the AoPS standards on a scale fro"
330,Large Language Models A Survey.pdf,"Answering the on the AoPS standards on a scale from ′1′ to ′5′. A questions requires reasoning across several sentences. ′1′ shows the easiest problems in a subject, while ′5′ MultiRC dataset encompasses around 6,000 multi- represents the most difficult. In terms of formatting, sentencequestionsgatheredfromover800paragraphs. allproblemsandsolutionsarepresentedusingLATEX On average, each question offers about two valid and the Asymptote vector graphics language. answer alternatives out of a total"
331,Large Language Models A Survey.pdf,"phics language. answer alternatives out of a total of five. • HellaSwag [192] is designed to assess commonsense reasoning in LLMs. This benchmark includes 70,000 B. Datasets for Emergent: ICL, reasoning (CoT), instruction multiple-choice questions. Each question is derived following from one of two domains: ActivityNet or WikiHow, and presents four answer choices regarding what This section centers on the benchmarks and datasets em- might happen in the following situation. The correct ployed to "
332,Large Language Models A Survey.pdf,"in the following situation. The correct ployed to evaluate the emergent abilities of LLMs. answer provides an actual statement describing the
upcoming event, but the three wrong answers are C. Datasets for Augmented: using external knowledge/tools created to confuse machines. This section focuses on datasets designed for the aug- mented abilities of LLMs. • AI2 Reasoning Challenge (ARC) [193] is used for commonsense reasoning. This benchmark encom- • HotpotQA [198] is designed to cover a diverse"
333,Large Language Models A Survey.pdf,"m- • HotpotQA [198] is designed to cover a diverse and passes 7,787 science examination questions. These explainable question-answering dataset that necessi- questions are in English, and most of them are set tatesmulti-hopreasoning.Thisdatasetisderivedfrom up in a multiple-choice format. The questions have theEnglishWikipedia.Itconsistsofroughly113,000 been divided into two groups: a Challenge Set with questions. Each question in the dataset comes with 2,590 difficult questions and an Easy Set "
334,Large Language Models A Survey.pdf,"es with 2,590 difficult questions and an Easy Set with 5,197 two paragraphs, called gold paragraphs, from two questions. Each collection has also been pre-divided Wikipedia articles. Also, there is a list of sentences into Train, Development, and Test subsets. in those paragraphs that crowdworkers have picked as important for answering the question. • PIQA [194] is intended to evaluate the language representations on their knowledge of physical com- • ToolQA [199] is a question answering benchma"
335,Large Language Models A Survey.pdf,"om- • ToolQA [199] is a question answering benchmark monsense. In this dataset, the focus is on everyday to evaluate LLMs’ ability to use external tools for situations with a preference for uncommon solutions. answering questions. Thecentraltaskisamultiple-choicequestionanswer- • GPT4Tools serves as an instructional dataset, gener- ing, where a question (q) is provided along with two ated by instructing advanced teachers (such as Chat- potential solutions (s1,s2). Then, the best solution is GPT)"
336,Large Language Models A Survey.pdf,"solutions (s1,s2). Then, the best solution is GPT), with instructions conditioned on visual content chosen by whether a model or a human. For each and tool descriptions. This process results in the question, only one of the solutions is the correct generation of instructions related to the use of tools. answer. There are three versions of this dataset. The first version comprises 71,000 instruction-following data • SIQA[195]providesaframeworkforevaluatingmod- pointsutilizedtofine-tunetheGPT4Tool"
337,Large Language Models A Survey.pdf,"valuatingmod- pointsutilizedtofine-tunetheGPT4Toolsmodel.The els’ ability for commonsense reasoning about social next version consists of manually cleaned instruction situations. SIQA dataset has 38,000 multiple-choice data used for validation, covering instructions related questions designed to assess emotional and social to the tools from the first version. The last version is intelligence in everyday circumstances. This dataset cleaned instruction data used for testing and includes covers a w"
338,Large Language Models A Survey.pdf,"tion data used for testing and includes covers a wide variety of social scenarios. In SIQA, instructions related to some tools that are not present the potential answers is a mixture of human-selected in the first version. responses and machine-generated ones that have been filtered through adversarial processes. VI. PROMINENTLLMS’PERFORMANCEON • OpenBookQA (OBQA) [196] is a new kind of BENCHMARKS question-answering dataset where answering its ques- In this section we first provide an overview o"
339,Large Language Models A Survey.pdf,"es- In this section we first provide an overview of some of tions requires additional common and commonsense popular metrics used for evaluating the performance of LLMs knowledge not contained in the book and rich text under different scenarios. We then look at the performance comprehension. This dataset includes around 6,000 of prominent large language models on some of the popular multiple-choice questions. Each question is linked to datasets and benchmarks. one core fact, as well as an additi"
340,Large Language Models A Survey.pdf,"nd benchmarks. one core fact, as well as an additional collection of over 6000 facts. The questions were developed using a multi-stage crowdsourcing and expert filter- A. Popular Metrics for Evaluating LLMs ing procedure. OpenBookQA questions are difficult Evaluating the performance of generative language models because they need multi-hop reasoning with limited depends on the underlying task they are going to be used for. background. Tasks that are mostly about selecting a choice out of given o"
341,Large Language Models A Survey.pdf,"are mostly about selecting a choice out of given ones (such as sentiment analysis), can be seen as simple as • TruthfulQA [197] is designed specifically to eval- classification and their performance can be evaluated using uate the truthfulness of language models in gen- classification metrics. Metrics such as accuracy, precision, erating answers to questions. This dataset includes recall,F1,etcareapplicableinthiscase.Itisalsoimportantto 817 questions, written by authors, from 38 different noteth"
342,Large Language Models A Survey.pdf,"ions, written by authors, from 38 different notethattheanswersgeneratedbythemodelforspecifictasks categories,includinghealth,law,finance,andpolitics. suchasmulti-choicequestionansweringarealwayseitherTrue These questions are purposefully designed to chal- orFalse.Iftheanswerisnotinasetofoptions,itcanbeseen lengehumanresponders,astheymaycontaincommon as False as well. misunderstandings that lead to incorrect answers. However,sometasksthatarepurelyopen-endedtextgener- • OPT-IML Bench [103] is a co"
343,Large Language Models A Survey.pdf,"open-endedtextgener- • OPT-IML Bench [103] is a comprehensive bench- ationcannotbeevaluatedinthesamewayasforcategorization. mark for Instruction Meta-Learning. It covers 2000 Different metrics are required for the specific purpose of the NLPtasksfrom8existingbenchmarks.TheOPT-IML evaluation. Code generation is a very different case in open- Benchconsistsofatrainingsetwith17.9Mexamples, ended generative evaluations. The generated code must pass adevsetwith145Ksamples,andatestsetwith321K the test "
344,Large Language Models A Survey.pdf,"evsetwith145Ksamples,andatestsetwith321K the test suite but on the other hand, it is also important samples. to understand if a model is capable of generating different
TABLE II: LLM Datasets Overview. BenchmarkName EvaluationMetric Leaderboard Source paperswithcode HumanEval PASS@k Link Link Link MBPP PASS@k,Accuracy - Link Link APPS PASS@k,Accuracy - Link Link WikiSQL Accuracy - Link Link CoNaLa BLEU Link Link CodeParrot PASS@k - Link - HellaSwag Accuracy Link Link Link AI2 Reasoning Accuracy "
345,Large Language Models A Survey.pdf,"ag Accuracy Link Link Link AI2 Reasoning Accuracy Link Link Link Challenge(ARC) BoolQ Accuracy - Link Link MultiRC F1-score,Accuracy - Link Link CNN/DailyMail[200] Accuracy - Link - SQuAD F1-score,EM Link Link Link RACE Accuracy - Link Link CNN/DailyMail[201] ROUGE - Link Link Drop F1-score,EM Link Link Link QuAC F1-score,HEQ-Q,HEQ-D Link Link Link TriviaQA EM,F1-score,Accuracy Link Link Link NaturalQuestions EM,F1-score,Accuracy Link Link Link StrategyQA Accuracy,Recall@10,SARI Link Link Link C"
346,Large Language Models A Survey.pdf,"trategyQA Accuracy,Recall@10,SARI Link Link Link CoQA F1-score Link Link Link XSum ROUGE - Link Link SAMSum ROUGE - - Link WikiSum ROUGE - Link - DialogSum ROUGE - Link Link TruthfulQA MC1,MC2,%true,%info,BLEURT Link Link Link MMLU Accuracy Link Link Link GSM8K Accuracy Link Link Link PIQA Accuracy Link Link Link SIQA Accuracy Link Link Link OpenBookQA(OBQA) Accuracy Link Link Link HotpotQA EM,F1-score,JointEM,JointF1-score, Link Link Link MATH Accuracy - Link Link CommonsenseQA Accuracy Link Li"
347,Large Language Models A Survey.pdf,"ccuracy - Link Link CommonsenseQA Accuracy Link Link Link NaturalInstructions ROUGE-L,Human Link Link Link BIG-bench Accuracy,Average - Link Link Successrate,Precision,Recall,Incorrect ToolTalk - Link Link actionrate,Percentoffailingerrortypes MetaTool Accuracy,Precision,Recall,F1-score - Link Link SuccessfulRateofThought,Successful GPT4Tools RateofAction,SuccessfulRateofAr- - Link Link guments,SuccessRate Correctness,ROUGE,Error(APIHallu- cination, Has Exception, Invalid Input API-Bank - Link L"
348,Large Language Models A Survey.pdf,"on, Has Exception, Invalid Input API-Bank - Link Link Parameters,FalseAPICallFormat,API Call,MissInputParameters) Alpaca-CoT - - Link Link solutions as a code, what is the probability of selecting the M correctoneamongthem.Pass@kisaverygoodmetricinthis EM = (5) case. It works in this manner that given a problem, different N solutionsascodearegenerated.Theyaretestedforcorrectness using different functionality tests. Afterward, from generated Human equivalence score (HEQ) on the other hand, is an "
349,Large Language Models A Survey.pdf," equivalence score (HEQ) on the other hand, is an nsolutions,andtherespectivecnumberofthembeingcorrect alternative to F1 score [203]. HEQ-Q represents the precision equation 4 provides the final value. of individual questions, wherein an answer is deemed correct ifthemodel’sF1scoresurpassestheaveragehumanF1score. Likewise, HEQ-D denotes the precision of each dialogue; it is (cid:34) (cid:0)n−c(cid:1)(cid:35) deemed accurate when all questions within the dialogue meet the criteria of HEQ [182]. p"
350,Large Language Models A Survey.pdf,"hin the dialogue meet the criteria of HEQ [182]. pass@k := E 1− k (4) (cid:0)n(cid:1) Problems k Evaluationofothergenerativetaskssuchasmachinetrans- lation are based on metrics such as Rouge and BLEU. These scores work well when there is a reference text as ground Exact match (EM) is another metric that is mostly con- truth (such as translation) and a hypothesis that is generated cerned with exact matches from (pre-defined) answers. It by the generative model, in our case the LLM. These scores c"
351,Large Language Models A Survey.pdf,"erative model, in our case the LLM. These scores counts a prediction as correct if it exactly matches one of are mostly used for cases where the goal is to detect the more than one desired reference text token by token. In some similarity of the answer and ground truth in a computation cases,itcanbethesameasaccuracyandtheequation5shows manner. In a computation manner, it meant that nothing more themathematicaldefinition.HereMistotalnumberofcorrect thanN-Gramswouldbeused.However,metricssuchasBERT"
352,Large Language Models A Survey.pdf,"t thanN-Gramswouldbeused.However,metricssuchasBERT- answers and N is the total number of questions [202]. Score are also good for these cases but they are also heavily
TABLE III: LLM categories and respective definitions. Classification Category Description Small Numberofparameters≤1B Medium 1B<Numberofparameters≤10B Size Large 10B<Numberofparameters≤100B VeryLarge 100B<Numberofparameters Foundationmodel Pretrainedlanguagemodel Type Instructionmodel Pretrainedandinstructionfine-tunedlanguagemode"
353,Large Language Models A Survey.pdf,"del Pretrainedandinstructionfine-tunedlanguagemodel Chatmodel Pretrained,instructionfine-tuned,andchatfine-tunedlanguagemodel Originalmodel AnoriginalmodelreleasedwitheitherFoundation,Instruction,orChatmodel Origin Tunedmodel Fine-tunedversionofanoriginalmodel Publiclyavailable Modelandweightsareavailableduetorequesttowithoutrequest Availability Publiclyunavailable Modelandweightsarenotpubliclyavailable TABLE IV: Different LLM categorization. Model Size #Params(B) Type Availability Origin Davinc"
354,Large Language Models A Survey.pdf,el Size #Params(B) Type Availability Origin Davinci-002 VeryLarge 175 Instruction Unavailable Tuned Davinci-003 VeryLarge 175 Instruction Unavailable Tuned GPT3.5-turbo Large 20 Chat Unavailable Tuned Falcon7B Medium 7 Foundation Public Original Alpaca Large 13 Chat Public Tuned Pythia7B Medium 7 Foundation Public Original Pythia12B Large 12 Foundation Public Original LLAMA7B Medium 7 Chat Public Original LLAMA27B Medium 7 Chat Public Tuned LLAMA27B Medium 7 Foundation Public Original Vicuna13B 
355,Large Language Models A Survey.pdf,"27B Medium 7 Foundation Public Original Vicuna13B Large 13 Foundation Public Tuned Vicuna7B Medium 7 Foundation Public Tuned Claude Large 93 Chat Unavailable Original Claude2 VeryLarge 137 Chat Unavailable Original erroneous because another model is used to judge. Still, even we use is their primary use case. We consider each LLM to today, evaluating purely generated content is very hard and beeither: Foundation model(pretrained languagemodel with no completely fitting metric is not found, metri"
356,Large Language Models A Survey.pdf,"h no completely fitting metric is not found, metrics are either no instruction fine-tuning and chat fine-tuning), Instruction looking for simplistic features such as N-Gram, SkipGram, model (pretrained language model with only instruction fine- etc,ortheyaremodelswithunknownaccuracyandpreciseness tuning), and Chat model (pretrained language model with [204]. instruction and chat fine-tuning). Apart from all the catego- rization described, another category is required to distinguish Generativeeva"
357,Large Language Models A Survey.pdf," category is required to distinguish Generativeevaluationmetricsarealsoanothertypeofeval- between original models and tuned ones. Original models are uation metric for LLMs that use another LLM for evaluating those that have been released as a foundation model or a fine- the answer. However, depending on the task itself, evaluation tuned one. Tuned models are those that grasped the original can be possible in this way or not. Another dependency model and tuned it with different datasets or even "
358,Large Language Models A Survey.pdf,odel and tuned it with different datasets or even different that makes generative evaluation error-prone is reliance on trainingapproaches.Itisalsogoodtonotethatoriginalmodels the prompt itself. RAGAS is one of the good examples that are usually foundation models that have been fine-tuned on incorporate the usage of generative evaluation. specific datasets or even different approaches. Availability of themodelweightsregardlessofthelicenseisanothercategory Various benchmarks and leaderboards have
359,Large Language Models A Survey.pdf,rcategory Various benchmarks and leaderboards have been proposed in our classification. Models that have their weights publicly to address the most challenging question in the world of available (even through request) are noted as Public models large language models: Which one is better? However not while others are noted as Private. Table III shows all of these a simple answer can address this question. The answer de- definitions and abbreviations used in the rest of the article. pends on vario
360,Large Language Models A Survey.pdf,"ns used in the rest of the article. pends on various aspects of large language models. Section V Figure 43 illustrate these visually. shows the categorical presentation of different tasks and the most important datasets in each category. We will follow the According to the provided categorizations, we can catego- same categorization and provide a comparison based on each rizeandlabeleachnotableLLMasshownintableIV.Ascan category. After providing comparison for each category, we be seen from this "
361,Large Language Models A Survey.pdf,"omparison for each category, we be seen from this table, models categorized as very large are will provide a broad overview of aggregated performance by also unavailable as well. averaging the reported performance metric on different tasks. B. LLMs’ Performance on Different Tasks Evaluating different LLMs can be seen also from different perspectives. For example, a LLM with a drastically fewer Commonsense reasoning is one of the important capabili- number of parameters is not completely comparab"
362,Large Language Models A Survey.pdf,"i- number of parameters is not completely comparable to one ties each model can obtain. This capability denotes the ability withalargernumberofparameters.Fromthisperspective,we of the model to use prior knowledge in combination with will categorize LLMs in four categories as well: small (less reasoningskills.InthecaseofHellaSwagforexample,finding thanorequalto1billionparameters),medium(between1and the continuation of text is challenging because the given text 10billion),large(between10and100bill"
363,Large Language Models A Survey.pdf,"he given text 10billion),large(between10and100billion),andverylarge contains a partial part of the story while the given choices (more than 100 billion). Another classification for the LLMs as continuation are tricky to select, and without having prior
Parameters Large Language Models Availability ytilanigirO Type Large LM 10B < # of params <100B Medium LM 1B < # of params <10B Very Large LM 100B < # of params Small LM Pretrained model with no instruction or chat fine-tuning. Fine tuned models t"
364,Large Language Models A Survey.pdf,struction or chat fine-tuning. Fine tuned models that are originally # of params <1B Example: MPT-7B based on original models. Example: Alpaca (based on LLaMA) Foundation Tuned Pretrained model that is also fine-tuned on Fine tuning Instruction Exa in m s p tr l u e c : t M io P n T fo -7 ll B ow -in in s g tr . uct Original Chat Pretrained model that is also fine-tuned on chat. Example: MPT-7B-chat Original models that are not fine tuned or based on any other pretrained model. Example: LLaMA Mo
365,Large Language Models A Survey.pdf,d on any other pretrained model. Example: LLaMA Model weights are NOT publicly Model weights are publicly released released and is NOT available. Private Public and is available. Example: GPT-4 Example: LLaMA Fig. 43: LLM categorizations. knowledgeabouttheworlditisnotpossible.Thisspecifickind FromtheresultspresentedinTableVitisclearthatGPT-4 of reasoning deserves high attention because it is related to achieves best results for HellaSwag while Davinci-003 is best utilizing previous knowledge wit
366,Large Language Models A Survey.pdf,"vinci-003 is best utilizing previous knowledge with open text-described scenes modelforOBQA.ItisalsogoodtonotethatresultsforOBQA or facts. As can be seen from table V not just Unavailable arenotreportedforallofthemodelsandpossiblydavinci-003 models but also Public ones can achieve good results on is not the best model achieving highest results on OBQA. various tests. TABLE V: Commonsense reasoning comparison. Notallmodelsreporttheirperformanceonalldatasets,and Model OBQA HellaSwag because of tha"
367,Large Language Models A Survey.pdf,"lldatasets,and Model OBQA HellaSwag because of that, the number of models for which performance Davinci-003 51 83.4 is reported in different tables varies. Falcon7B 44.4 76.3 Alpaca 43.4 73.9 Pythia7B 37.2 64 Pythia12B 43.2 68.1 LLAMA7B 42.4 73 Dolly6B 41.2 67.6 Dolly12B 40.4 71 TABLE VI: Symbolic reasoning comparison. Alpaca7B 43.4 73.9 AlpacaLora7B 42.6 74 Model Cobjects Penguins GPT-J6.7B 38.2 66.2 GPT-NeoX 26 33.56 LLama7B 42.4 73 OPT66B 31.2 28.08 LLama13B 42.2 76.2 BloombergGPT 34.8 37.67 "
368,Large Language Models A Survey.pdf," 28.08 LLama13B 42.2 76.2 BloombergGPT 34.8 37.67 Pythia6.7B 37.2 64 BLOOM176B 36.8 40.41 Pythia12B 38 67.3 PaLM540B 38 44.5 StableLMTuned 33.4 53.6 Gopher-280B 49.2 40.6 Koala13B 42.8 72.6 Chinchilla-70B 59.7 48.7 Mosaicmpt-7B 42.6 76.3 PaLM2 61.2 65.8 LLAMA270B - 87.33 LLAMA65B - 86.09 Falcon40B - 85.3 Falcon180B - 88.86 MPTInstruct30B - 84.31 MPTInstruct7B - 77.91 Worldknowledgeismostlyaboutgeneralknowledgeques- Yi6B - 76.42 tions,forexample,inWikifactdatasetquestionssuchas”Who Yi34B - 85.69 "
369,Large Language Models A Survey.pdf,"nWikifactdatasetquestionssuchas”Who Yi34B - 85.69 GPT-4 - 95.3 istheauthorofaspecificwell-knownbook”canbefoundand GeminiUltra - 87.8 references are also provided. Table VII shows the results.
TABLE VII: World knowledge comparison. TABLE IX: Arithmetic reasoning comparison. Model TriviaQA NaturalQ WebQ ARC Model GSM8k MATH BLOOM - - - 32.9 GeminiUltra 94.4 53.2 BLOOM176B - - - 50.85 GPT-4 87.1 42.5 BloombergGPT - - - 48.63 GeminiPro 86.5 32.6 Chinchilla - 35.5 - - ToRA70B 84.3 49.7 Codex+REPLUG 7"
370,Large Language Models A Survey.pdf,chilla - 35.5 - - ToRA70B 84.3 49.7 Codex+REPLUG 76.8 44.7 - - MathCoder-L-70B 83.9 - GAL120B - - - 67.9 MetaMath70B 82.3 26 GLaM62B/64E 75.8 32.5 15.5 50.3 MuggleMATH70B 82.3 - Gopher - 28.2 - - MathCoder-CL-34B 81.7 45.2 GPT-3175B 71.2 29.9 41.5 85.2 ToRA-Code34B 80.7 50.8 GPT-4 - - - 96.4 MetaMath-Mistral-7B 77.7 - GPT-NeoX - - - 45.39 Arithmo2-Mistral-7B 76.4 - LLaMA13B - - - 52.7 ToRA-Code13B 75.8 48.1 LLaMA270B 85 33 - - Arithmo-Mistral-7B 74.7 - LLaMA33B - 24.9 - 57.8 MathCoder-CL-13B 74.
371,Large Language Models A Survey.pdf,74.7 - LLaMA33B - 24.9 - 57.8 MathCoder-CL-13B 74.1 35.9 LLaMA65B 72.6 39.9 - - MuggleMATH13B 74 - LLaMA7B - - - 47.6 CodeT5+ 73.8 - Mistral7B 69.9 28.8 - 55.5 KwaiYiiMath13B 73.3 - Neo-6B - 13.7 - - ToRA-Code7B 72.6 44.6 OPT - - - 31.1 MathCoder-L-13B 72.6 29.9 OPT66B - - - 44.54 MetaMath13B 71 22.5 OPT-175B - - - 43.94 LLaMA65B 69.7 10.6 OPT-175B - - - 25.6 MuggleMATH7B 68.4 - PaLM2-L 86.1 37.5 28.2 95.1 MathCoder-CL-7B 67.8 23.3 PaLM2-M 81.7 32 26.9 64.9 MetaMath7B 66.4 19.4 PaLM2-S 75.2 25.3
372,Large Language Models A Survey.pdf,"2 26.9 64.9 MetaMath7B 66.4 19.4 PaLM2-S 75.2 25.3 21.8 59.6 RFT70B 64.8 - PaLM-540B 81.4 39.6 43.5 87.1 MathCoder-L-7B 64.2 - phi-1.5-web1.3B - - - 44.9 Orca2-13B 59.14 - SparseGPT - - - 38.99 U-PaLM 58.5 - SparseGPT - - - 39.85 PaLM-540B 58.1 8.8 SparseGPT - - - 41.3 LLaMA270B 56.8 - RFT13B 55.3 - LLaMA33B 53.1 7.1 Mistral7B 52.2 13.1 RFT7B 51.2 - Forsomespecificuse-casemodels,itishighlydemandedto LLaMA65B 50.9 20.5 have coding and code-generation capability. Table VIII shows Orca2-7B 47.23 - "
373,Large Language Models A Survey.pdf,ion capability. Table VIII shows Orca2-7B 47.23 - the results of different models on coding capability. Text-davinci-002 40.7 19.1 LLaMA33B 35.6 3.9 GPT-Neo-2.7B 19.5 - LLaMA7B 18.1 2.9 TABLE VIII: Coding capability comparison. PaLM540B 17.9 8.8 LLaMA13B 17.8 3.9 LLaMA7B 11 2.9 Model HumanEval GPT-Neo-125M 7.5 - GeminiUltra 74.4 PaLM8B 4.1 1.5 GeminiPro 67.7 GPT-2 - 5.4 GPT-4 67 GPT-3175B - 5.2 WizardCoder15B 57.3 PaLM62B - 4.4 phi-11.3B 50.6 GPT-3-13B - 3 CodeLlama 48.8 LLaMA7B 11 2.9 GPT-3.5 4
374,Large Language Models A Survey.pdf,-3-13B - 3 CodeLlama 48.8 LLaMA7B 11 2.9 GPT-3.5 48.1 PaLM8B - 1.5 OctoCoder 46.2 phi-1-small 45 PaLM2-S 37.6 Largelanguagemodelsinsomecasesarehallucinatingan- InstructCodeT5+16B 35 Mistral7B 30.5 swerssimplybecausetheyarenext-tokenpredictionmachines. LLaMA2 29.9 Hallucination is one of the important factors in measuring phi-1-base 29 how much a large language model is trustworthy and reliable. Codex-12B 28.81 PaLM540B 26.2 Measuringhallucinationontheotherhandisalsonoteasyasit CodeT5+2B 24.2 see
375,Large Language Models A Survey.pdf,"ontheotherhandisalsonoteasyasit CodeT5+2B 24.2 seems because each fact can be written in different styles and LLaMA65B 23.7 even the smallest changes in writing make it hard to detect. LLaMA33B 21.7 PaLM62B 15.9 It is fair to assume if any particular LLM is more capable LLaMA13B 15.8 to detect hallucination of false information in text, it is also LaMDA137B 14 more trustworthy. HaluEval is one of the datasets that aims to MIM-350M 13.7 LLaMA7B 10.5 measurehallucinationinthisfield[205].Evaluation"
376,Large Language Models A Survey.pdf,".5 measurehallucinationinthisfield[205].Evaluationcanalsobe PaLM8B 3.6 performed by another model judging the response with regard to the actual answer [206]. Table X shows the evaluation of different models based on these datasets. Arithmetic reasoning is another challenging reasoning ca- VII. CHALLENGESANDFUTUREDIRECTIONS pabilitytoachieve.GSM8Kforexamplecontainsgradeschool mathematicalquestionswithrespecttotheiranswers.TableIX As we have seen in the previous sections, large language provides "
377,Large Language Models A Survey.pdf,"in the previous sections, large language provides an insight for different model comparisons. models have achieved impressive results in the past 1-2 years.
TABLE X: Hallucination evaluation Model HHEM HaluEvalQA HaluEvalDialogue HaluEvalSum. HaluEvalGeneral GPT4 97 - - - - GPT4Turbo 97 - - - - GPT3.5Turbo 96.5 62.59 72.4 58.53 79.44 Davinci002 - 60.05 60.81 47.77 80.42 Davinci003 - 49.65 68.37 48.07 80.4 GPT-3 - 49.21 50.02 51.23 72.72 GoogleGeminiPro 95.2 - - - - Llama270B 94.9 - - - - Llama27"
378,Large Language Models A Survey.pdf,iniPro 95.2 - - - - Llama270B 94.9 - - - - Llama27B 94.4 49.6 43.99 49.55 20.46 Llama213B 94.1 - - - - Cohere-Chat 92.5 - - - - Cohere 91.5 - - - - Claude2 91.5 69.78 64.73 57.75 75 Claude1 67.6 64.83 53.76 73.88 MicrosoftPhi2 91.5 - - - - GooglePalm2(beta) 91.4 - - - - Mixtral8x7B 90.7 - - - - AmazonTitanExpress 90.6 - - - - Mistral7B 90.6 - - - - GooglePalm2Chat(beta) 90 - - - - GooglePalm2 87.9 - - - - GooglePalm2Chat 72.8 - - - - ChatGLM - 47.93 44.41 48.57 30.92 Falcon - 39.66 29.08 42.71 1
379,Large Language Models A Survey.pdf,".93 44.41 48.57 30.92 Falcon - 39.66 29.08 42.71 18.98 Vicuna - 60.34 46.35 45.62 19.48 Alpaca - 6.68 17.55 20.63 9.54 At the same time this is still a new and extremely active GRU, seq2seq, but Transformers have been the dominant researchareawherethepaceofinnovationisincreasingrather approach since its inception. As described earlier, attention is thanslowingdown.Asinanyotherevolvingareathough,there themainmechanismdrivingtransformers.Morerecently,there are still numerous challenges ahead. Here"
380,Large Language Models A Survey.pdf,"ly,there are still numerous challenges ahead. Here we briefly mention has been promising research in alternative approaches that are someofthechallengesandmainactiveareaswhichareknown being labelled as post-attention. so far. It is worth noting that LLM challenges are discussed An important class of such class of post-attention models in details in a work by Kaddour et al. [207]. arethesocalledStateSpaceModels(SSMs).Whilethenotion of State Space Models has a long history in machine learning, A. "
381,Large Language Models A Survey.pdf,"Models has a long history in machine learning, A. Smaller and more efficient Language Models itshouldbenotedthatinthecontextoflanguagemodels,SSM This is a survey on large language models, and there isusuallyusedinreferencetothenewerStructureStateSpace has been an initial push towards ”larger is better” that has Model architecture or S4 for short (see Gu et al. [29]). Some clearly been rewarded with ever larger models like GPT- recent models in this category are Mamba [30], Hyena [210], 4 getting"
382,Large Language Models A Survey.pdf,"is category are Mamba [30], Hyena [210], 4 getting better accuracy and performance in benchmarks. and Striped Hyena [211]. However, those large models are costly and inefficient in While all of those models are very competitive in terms of several dimensions (e.g. high latency). In response to all of performance in leaderboards and efficiency, they also address this, there is a current research trend to come up with Small an important challenge in more traditional attention-based Language Models"
383,Large Language Models A Survey.pdf,"n more traditional attention-based Language Models (SLMs) as a cost-effective alternative to architectures: the lack of support for larger context windows. LLMs, particularly when used on specific tasks that might not require the full generality of larger models. Prominent works Having a good answer to many prompts requires context. in this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2 Forexample,theresponseto”Recommendsomegoodmovies from Microsoft. for me” requires a lot of context ab"
384,Large Language Models A Survey.pdf,"om Microsoft. for me” requires a lot of context about ”me” as well as what movies are available and which ones I have not watched. More generally, we should expect many research efforts in Context length is especially important for RAG, where large this area of how to train smaller and more efficient models. portionsoftextmightberetrievedandinjectedintotheprompt Techniques such as parameter-efficient fine-tuning (PEFT), for generation (see section IV-C. teacher/student, and other forms of distil"
385,Large Language Models A Survey.pdf,"n IV-C. teacher/student, and other forms of distillation – see section III-I – will continue to be used to build a smaller model out The longer the context length, the more tokens we can of larger ones. squeeze into the context. The more information the model has access to, the better its response will be. But on the other B. New Post-attention Architectural Paradigms hand, with very long context, it would be hard for the model toremembereverythingandefficientlyprocessalltheinforma- Transformerb"
386,Large Language Models A Survey.pdf,"ngandefficientlyprocessalltheinforma- Transformerblockshavebeenacrucialandconstantpartof tion. Attention-based models are highly inefficient for longer mostofcurrentLLMframeworks,andit’sabigquestionmark contexts and that is why we should expect more research in how much longer this architecture will be in vogue, and what different mechanisms that enable processing longer contexts will be the next big architectural break-through in the field of and generally come up with more efficient architectu"
387,Large Language Models A Survey.pdf,"d generally come up with more efficient architectures. deeplearning(andNLP).SinceAlexNetin2012,wehaveseen many architectures go in and out of fashion, including LSTM, That being said, new architectures might not only propose
alternatives for the attention mechanism but rather rethink the LLM-based systems are already starting to replace ma- whole Transformer architecture. As an early example of this, chine learning systems that were until recently using other Monarch Mixer [212] proposes a new a"
388,Large Language Models A Survey.pdf,"y using other Monarch Mixer [212] proposes a new architecture that uses approaches. As a clear example of this, LLMs are now being the same sub-quadratic primitive that achieves high hardware deployed to better understand people preference and interests, efficiencyonGPUs–Monarchmatrices–alongbothsequence and provide more personalized interactions, whether in cus- length and model dimension. tomer service, content recommendation, or other applications. This involves better understanding of user p"
389,Large Language Models A Survey.pdf,"ions. This involves better understanding of user preferences, and On the other end of the spectrum, it is worth mentioning analyzingtheirpastinteractionsandusingthemasthecontext. that there are some attention-compatible architectural mecha- We will continue to see research in the application and usage nisms that have been recently gaining steam and proving their of LLMs for not only personalization and recommendations, value in creating better and more powerful LLMs. Probably butmanyotherapplica"
390,Large Language Models A Survey.pdf,"d more powerful LLMs. Probably butmanyotherapplicationareasusingothermachinelearning the best example of such mechanism is Mixture of Experts techniques. (MoE).MoEshavebeenaroundinmachinelearningforyears, even before the Deep Learning Era [213], but they have been Finally, another important area of research we expect to gaining popularity since then, and particularly in the context gather increased attention is that of LLM-based agents and of Transformer models and LLMs. multi-agent systems [172"
391,Large Language Models A Survey.pdf,"nsformer models and LLMs. multi-agent systems [172], [173], [174]. The development of LLM systems with access to external tools and decision- In LLMs, MoEs allow to train an extremely large model making capabilities is both exciting and challenging. We will than is then only partially instantiated during inference seecontinuedresearchandprogressinthisimportantareathat when some of the experts are turned off wherever the gat- somearguecouldleadtoArtificialGeneralIntelligence(AGI). ing/weighting f"
392,Large Language Models A Survey.pdf,"rtificialGeneralIntelligence(AGI). ing/weighting function has a low weight assigned to them. As an example, the GLaM model has 1.2 trillion parameters, but during inference only 2 out of the 64 experts are used [84]. E. Security and Ethical/Responsible AI MoEs are nowadays an important component of the so- Ensuring the robustness and security of LLMs against called frontier LLMs (i.e. the most advanced and capable adversarial attacks and other vulnerabilities is a critical area models). GPT-4 it"
393,Large Language Models A Survey.pdf,"lnerabilities is a critical area models). GPT-4 itself is rumored to be based on a MoE of research [219]. As LLMs are increasingly deployed in real- architecture, and some of the best performing LLMs such as world applications, they need to be protected from potential Mixtral [117], are basically an MoE version of pre-existing threats, to prevent them being used to manipulate people or LLMs. spread mis-information. Finally, it is important to note that MoEs can be used as a Addressingethicalconc"
394,Large Language Models A Survey.pdf,"e that MoEs can be used as a AddressingethicalconcernsandbiasesinLLMsisanother componentofanyarchitectureregardlessofwhetheritisbased active area of research. Efforts are being made to ensure that on attention or not. In fact, MoEs have also been applied to LLMs are fair, unbiased, and capable of handling sensitive SSM-based LLMs like Mamba citepioro2024moemamba. We information responsibly. As LLMs are being used more and shouldcontinuetoseeMoE-drivenimprovementsinthefuture more by a large numbe"
395,Large Language Models A Survey.pdf,"rivenimprovementsinthefuture more by a large number of people on a daily basis, making regardless of the underlying architecture. sure they are unbiased and behave responsibly is crucial. C. Multi-modal Models VIII. CONCLUSION Future LLMs are expected to be multi-modal and handle a variety of data types, such as text, images, and videos, This paper present a survey of LLMs developed in the audio, in a unified manner. This opens up possibilities for past few years. We first provide an overview of"
396,Large Language Models A Survey.pdf,"or past few years. We first provide an overview of early pre- more diverse applications in fields like question answering, trained language models (e.g., as BERT), then review three content generation, creative arts, and healthcare, robotics, and popular LLM families (GPT, LLaMA, PaLM), and other beyond. There are already several prominent multi-modal representative LLMs. We then survey methods and techniques LLMsoutthere,including:LLAVA[214],LLAVA-Plus[215], of building, augmenting, and using L"
397,Large Language Models A Survey.pdf,"VA-Plus[215], of building, augmenting, and using LLMs. We review popular GPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is LLM datasets and benchmarks, and compare performance of expected to be continued. Evaluation of these models also is a a set of prominent models on public benchmarks. Finally, we newresearchtopic,especiallyconversationalgenerativevision present open challenges and future research directions. models [217]. Multi-modal LLMs can unlock huge potentials in a variety of "
398,Large Language Models A Survey.pdf,"l LLMs can unlock huge potentials in a variety of tasks, and there has already been a descent REFERENCES progress in this direction, which needs a dedicated paper to discuss all its details. [1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R.Child,S.Gray,A.Radford,J.Wu,andD.Amodei,“Scalinglaws forneurallanguagemodels,”arXivpreprintarXiv:2001.08361,2020. D. Improved LLM Usage and Augmentation techniques [2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, As we descri"
399,Large Language Models A Survey.pdf,"d, A. Mensch, E. Buchatskaya, T. Cai, As we described in sectionIV, many of the shortcomings E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al., “Training compute-optimal large language models,” arXiv and limitations of LLMs such as hallucination can be ad- preprintarXiv:2203.15556,2022. dressed through advanced prompt engineering, use of tools, [3] C.E.Shannon,“Predictionandentropyofprintedenglish,”Bellsystem or other augmentation techniques. We should expect not only tec"
400,Large Language Models A Survey.pdf,"entation techniques. We should expect not only technicaljournal,vol.30,no.1,pp.50–64,1951. continued, but accelerated research in this area. It is worth [4] F. Jelinek, Statistical methods for speech recognition. MIT press, mentioning that, in the specific case of software engineering, 1998. some works ([218]) tried to automatically eliminate this issue [5] C. Manning and H. Schutze, Foundations of statistical natural lan- from the overall software engineering workflow guageprocessing. MITpress,"
401,Large Language Models A Survey.pdf,"re engineering workflow guageprocessing. MITpress,1999.
[6] C.D.Manning,Anintroductiontoinformationretrieval. Cambridge models for natural language processing: A survey,” Science China universitypress,2009. TechnologicalSciences,vol.63,no.10,pp.1872–1897,2020. [7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, [29] A.Gu,K.Goel,andC.Re ́,“Efficientlymodelinglongsequenceswith B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language structuredstatespaces,”2022. models,”arXivpr"
402,Large Language Models A Survey.pdf,"guage structuredstatespaces,”2022. models,”arXivpreprintarXiv:2303.18223,2023. [30] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with [8] C.Zhou,Q.Li,C.Li,J.Yu,Y.Liu,G.Wang,K.Zhang,C.Ji,Q.Yan, selectivestatespaces,”arXivpreprintarXiv:2312.00752,2023. L.Heetal.,“Acomprehensivesurveyonpretrainedfoundationmod- [31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, els:Ahistoryfromberttochatgpt,”arXivpreprintarXiv:2302.09419, A.Roberts,P.Barham,H.W.Chung,C.Sutton,S.Gehrmannetal., 2"
403,Large Language Models A Survey.pdf,"rts,P.Barham,H.W.Chung,C.Sutton,S.Gehrmannetal., 2023. “Palm: Scaling language modeling with pathways,” arXiv preprint [9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre- arXiv:2204.02311,2022. train,prompt,andpredict:Asystematicsurveyofpromptingmethods [32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, in natural language processing,” ACM Computing Surveys, vol. 55, T.Lacroix,B.Rozie`re,N.Goyal,E.Hambro,F.Azharetal.,“Llama: no.9,pp.1–35,2023. Open and efficie"
404,Large Language Models A Survey.pdf,"retal.,“Llama: no.9,pp.1–35,2023. Open and efficient foundation language models,” arXiv preprint [10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, arXiv:2302.13971,2023. J. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint [33] OpenAI, “GPT-4 Technical Report,” https://arxiv.org/pdf/2303. arXiv:2301.00234,2022. 08774v3.pdf,2023. [11] J.HuangandK.C.-C.Chang,“Towardsreasoninginlargelanguage [34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, models:Asurvey,”arX"
405,Large Language Models A Survey.pdf,"huurmans, M. Bosma, b. ichter, models:Asurvey,”arXivpreprintarXiv:2212.10403,2022. F. Xia, E. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought [12] S. F. Chen and J. Goodman, “An empirical study of smoothing prompting elicits reasoning in large language models,” in techniques for language modeling,” Computer Speech & Language, Advances in Neural Information Processing Systems, S. Koyejo, vol.13,no.4,pp.359–394,1999. S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, [13] Y. Bengio, R. Ducharme"
406,Large Language Models A Survey.pdf,"ve, K. Cho, and A. Oh, [13] Y. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic Eds., vol. 35. Curran Associates, Inc., 2022, pp. 24824–24837. languagemodel,”Advancesinneuralinformationprocessingsystems, [Online]. Available: https://proceedings.neurips.cc/paper files/paper/ vol.13,2000. 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf [14] H. Schwenk, D. De ́chelotte, and J.-L. Gauvain, “Continuous space [35] G. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,"
407,Large Language Models A Survey.pdf," R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru, language models for statistical machine translation,” in Proceedings R. Raileanu, B. Rozie`re, T. Schick, J. Dwivedi-Yu, A. Celikyil- of the COLING/ACL 2006 Main Conference Poster Sessions, 2006, maz et al., “Augmented language models: a survey,” arXiv preprint pp.723–730. arXiv:2302.07842,2023. [15] T. Mikolov, M. Karafia ́t, L. Burget, J. Cernocky`, and S. Khudanpur, [36] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, “Recurrent"
408,Large Language Models A Survey.pdf," He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, “Recurrent neural network based language model.” in Interspeech, L. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try vol.2,no.3. Makuhari,2010,pp.1045–1048. again:Improvinglargelanguagemodelswithexternalknowledgeand automatedfeedback,”arXivpreprintarXiv:2302.12813,2023. [16] A. Graves, “Generating sequences with recurrent neural networks,” arXivpreprintarXiv:1308.0850,2013. [37] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.Narasimhan,andY.Cao, “React:Syn"
409,Large Language Models A Survey.pdf,"u,N.Du,I.Shafran,K.Narasimhan,andY.Cao, “React:Synergizingreasoningandactinginlanguagemodels,”arXiv [17] P.-S.Huang,X.He,J.Gao,L.Deng,A.Acero,andL.Heck,“Learning preprintarXiv:2210.03629,2022. deep structured semantic models for web search using clickthrough data,” in Proceedings of the 22nd ACM international conference on [38] D.E.Rumelhart,G.E.Hinton,R.J.Williamsetal.,“Learninginternal Information&KnowledgeManagement,2013,pp.2333–2338. representationsbyerrorpropagation,”1985. [18] J.Gao,C.Xion"
410,Large Language Models A Survey.pdf,"tationsbyerrorpropagation,”1985. [18] J.Gao,C.Xiong,P.Bennett,andN.Craswell,NeuralApproachesto [39] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14, ConversationalInformationRetrieval. SpringerNature,2023,vol.44. no.2,pp.179–211,1990. [19] I.Sutskever,O.Vinyals,andQ.V.Le,“Sequencetosequencelearning [40] M. V. Mahoney, “Fast text compression with neural networks.” in with neural networks,” Advances in neural information processing FLAIRSconference,2000,pp.230–234. systems,vol"
411,Large Language Models A Survey.pdf,"sing FLAIRSconference,2000,pp.230–234. systems,vol.27,2014. [41] T.Mikolov,A.Deoras,D.Povey,L.Burget,andJ.Cˇernocky`,“Strate- [20] K. Cho, B. Van Merrie ̈nboer, D. Bahdanau, and Y. Bengio, “On giesfortraininglargescaleneuralnetworklanguagemodels,”in2011 the properties of neural machine translation: Encoder-decoder ap- IEEEWorkshoponAutomaticSpeechRecognition&Understanding. proaches,”arXivpreprintarXiv:1409.1259,2014. IEEE,2011,pp.196–201. [21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. "
412,Large Language Models A Survey.pdf," Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dolla ́r, [42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/ J. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to ∼imikolov/rnnlm/ visual concepts and back,” in Proceedings of the IEEE conference [43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, oncomputervisionandpatternrecognition,2015,pp.1473–1482. andJ.Gao,“Deeplearning–basedtextclassification:acomprehensive [22] O. Vinyals, A. Toshe"
413,Large Language Models A Survey.pdf,"ification:acomprehensive [22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: review,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40, A neural image caption generator,” in Proceedings of the IEEE 2021. conference on computer vision and pattern recognition, 2015, pp. [44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. 3156–3164. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” [23] M.E.Peters,M.Neumann,M.Iyyer,M.Gardner,C.Clark,K.Lee, Ad"
414,Large Language Models A Survey.pdf,"ters,M.Neumann,M.Iyyer,M.Gardner,C.Clark,K.Lee, Advancesinneuralinformationprocessingsystems,vol.30,2017. and L. Zettlemoyer, “Deep contextualized word representations. corr [45] Z.Lan,M.Chen,S.Goodman,K.Gimpel,P.Sharma,andR.Soricut, abs/1802.05365(2018),”arXivpreprintarXiv:1802.05365,2018. “Albert:Alitebertforself-supervisedlearningoflanguagerepresen- [24] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-training tations,”arXivpreprintarXiv:1909.11942,2019. ofdeepbidirectionaltransformersforl"
415,Large Language Models A Survey.pdf,"09.11942,2019. ofdeepbidirectionaltransformersforlanguageunderstanding,”arXiv [46] K.Clark,M.-T.Luong,Q.V.Le,andC.D.Manning,“Electra:Pre- preprintarXiv:1810.04805,2018. trainingtextencodersasdiscriminatorsratherthangenerators,”arXiv [25] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis, preprintarXiv:2003.10555,2020. L.Zettlemoyer,andV.Stoyanov,“Roberta:Arobustlyoptimizedbert [47] G.LampleandA.Conneau,“Cross-linguallanguagemodelpretrain- pretrainingapproach,”arXivpreprintarXiv:1907.11692,2"
416,Large Language Models A Survey.pdf,"etrainingapproach,”arXivpreprintarXiv:1907.11692,2019. ing,”arXivpreprintarXiv:1901.07291,2019. [26] P.He,X.Liu,J.Gao,andW.Chen,“Deberta:Decoding-enhancedbert [48] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and withdisentangledattention,”arXivpreprintarXiv:2006.03654,2020. Q.V.Le,“Xlnet:Generalizedautoregressivepretrainingforlanguage [27] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, understanding,” Advances in neural information processing systems, A. Zhang"
417,Large Language Models A Survey.pdf,"in neural information processing systems, A. Zhang, L. Zhang et al., “Pre-trained models: Past, present and vol.32,2019. future,”AIOpen,vol.2,pp.225–250,2021. [49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, [28] X.Qiu,T.Sun,Y.Xu,Y.Shao,N.Dai,andX.Huang,“Pre-trained M. Zhou, and H.-W. Hon, “Unified language model pre-training for
natural language understanding and generation,” Advances in neural [70] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, informationproc"
418,Large Language Models A Survey.pdf," J. Hessel, T. Khot, K. R. Chandu, informationprocessingsystems,vol.32,2019. D.Wadden,K.MacMillan,N.A.Smith,I.Beltagyetal.,“Howfarcan camelsgo?exploringthestateofinstructiontuningonopenresources,” [50] A.Radford,K.Narasimhan,T.Salimans,I.Sutskeveretal.,“Improv- arXivpreprintarXiv:2306.04751,2023. inglanguageunderstandingbygenerativepre-training,”2018. [71] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski, [51] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskeveretal., and P. Miłos ́"
419,Large Language Models A Survey.pdf,"d,D.Luan,D.Amodei,I.Sutskeveretal., and P. Miłos ́, “Focused transformer: Contrastive training for context “Languagemodelsareunsupervisedmultitasklearners,”OpenAIblog, scaling,”arXivpreprintarXiv:2307.03170,2023. vol.1,no.8,p.9,2019. [72] D. Mahan, R. Carlow, L. Castricato, N. Cooper, [52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, and C. Laforte, “Stable beluga models.” [Online]. Y.Zhou,W.Li,andP.J.Liu,“Exploringthelimitsoftransferlearning Available: [https://huggingface.c"
420,Large Language Models A Survey.pdf,"transferlearning Available: [https://huggingface.co/stabilityai/StableBeluga2](https:// with a unified text-to-text transformer,” The Journal of Machine huggingface.co/stabilityai/StableBeluga2) LearningResearch,vol.21,no.1,pp.5485–5551,2020. [73] Y.Tay,J.Wei,H.W.Chung,V.Q.Tran,D.R.So,S.Shakeri,X.Gar- [53] L.Xue,N.Constant,A.Roberts,M.Kale,R.Al-Rfou,A.Siddhant, cia,H.S.Zheng,J.Rao,A.Chowdheryetal.,“Transcendingscaling A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained laws with "
421,Large Language Models A Survey.pdf,"5: A massively multilingual pre-trained laws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399, text-to-texttransformer,”arXivpreprintarXiv:2010.11934,2020. 2022. [54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked [74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, sequence to sequence pre-training for language generation,” arXiv Y.Li,X.Wang,M.Dehghani,S.Brahmaetal.,“Scalinginstruction- preprintarXiv:1905.02450,2019. finetunedlanguagemodels,”arXivpreprintarXiv"
422,Large Language Models A Survey.pdf,",2019. finetunedlanguagemodels,”arXivpreprintarXiv:2210.11416,2022. [55] M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy, [75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to- S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical sequencepre-trainingfornaturallanguagegeneration,translation,and report,”arXivpreprintarXiv:2305.10403,2023. comprehension,”arXivpreprintarXiv:1910.13461,2019. [76] K. S"
423,Large Language Models A Survey.pdf,"ion,”arXivpreprintarXiv:1910.13461,2019. [76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, [56] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, N.Scales,A.Tanwani,H.Cole-Lewis,S.Pfohletal.,“Largelanguage A.Neelakantan,P.Shyam,G.Sastry,A.Askelletal.,“Languagemod- modelsencodeclinicalknowledge,”arXivpreprintarXiv:2212.13138, elsarefew-shotlearners,”Advancesinneuralinformationprocessing 2022. systems,vol.33,pp.1877–1901,2020. [77] K. Singhal, T. Tu, J. Gottweis, R. Sayre"
424,Large Language Models A Survey.pdf,"020. [77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, [57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka- K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert- plan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., levelmedicalquestionansweringwithlargelanguagemodels,”arXiv “Evaluating large language models trained on code,” arXiv preprint preprintarXiv:2305.09617,2023. arXiv:2107.03374,2021. [78] J.Wei,M.Bosma,V.Y.Zhao,K.Guu,A.W.Yu,B.Lester,N"
425,Large Language Models A Survey.pdf,"78] J.Wei,M.Bosma,V.Y.Zhao,K.Guu,A.W.Yu,B.Lester,N.Du, [58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot C.Hesse,S.Jain,V.Kosaraju,W.Saundersetal.,“Webgpt:Browser- learners,”arXivpreprintarXiv:2109.01652,2021. assisted question-answering with human feedback,” arXiv preprint arXiv:2112.09332,2021. [79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J.Aslanides,S.Henderson,R.Ring,S.Youngetal.,“Scali"
426,Large Language Models A Survey.pdf,"J.Aslanides,S.Henderson,R.Ring,S.Youngetal.,“Scalinglanguage [59] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin, models: Methods, analysis & insights from training gopher,” arXiv C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language preprintarXiv:2112.11446,2021. models to follow instructions with human feedback,” Advances in Neural Information Processing Systems, vol. 35, pp. 27730–27744, [80] V.Sanh,A.Webson,C.Raffel,S.H.Bach,L.Sutawika,Z.Alyafeai, 2022. A. Chaffin, A. Sti"
427,Large Language Models A Survey.pdf,"ch,L.Sutawika,Z.Alyafeai, 2022. A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi- task prompted training enables zero-shot task generalization,” arXiv [60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https: preprintarXiv:2110.08207,2021. //openai.com/blog/chatgpt [81] Y.Sun,S.Wang,S.Feng,S.Ding,C.Pang,J.Shang,J.Liu,X.Chen, [61] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei, Y.Zhao,Y.Luetal.,“Ernie3.0:Large-scaleknowledgeenhancedpre- N. Bashlykov, S. Batra, P. B"
428,Large Language Models A Survey.pdf,"knowledgeenhancedpre- N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama training for language understanding and generation,” arXiv preprint 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2107.02137,2021. arXiv:2307.09288,2023. [82] S.Borgeaud,A.Mensch,J.Hoffmann,T.Cai,E.Rutherford,K.Mil- [62] R.Taori,I.Gulrajani,T.Zhang,Y.Dubois,X.Li,C.Guestrin,P.Liang, lican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark and T. B. Hashimoto, “Alpaca: A strong, rep"
429,Large Language Models A Survey.pdf," Clark and T. B. Hashimoto, “Alpaca: A strong, replicable instruction- et al., “Improving language models by retrieving from trillions of followingmodel,”StanfordCenterforResearchonFoundationMod- tokens,” in International conference on machine learning. PMLR, els.https://crfm.stanford.edu/2023/03/13/alpaca.html,vol.3,no.6, 2022,pp.2206–2240. p.7,2023. [83] O.Lieber,O.Sharir,B.Lenz,andY.Shoham,“Jurassic-1:Technical [63] T.Dettmers,A.Pagnoni,A.Holtzman,andL.Zettlemoyer,“Qlora:Ef- detailsandevaluat"
430,Large Language Models A Survey.pdf,"zman,andL.Zettlemoyer,“Qlora:Ef- detailsandevaluation,”WhitePaper.AI21Labs,vol.1,p.9,2021. ficientfinetuningofquantizedllms,”arXivpreprintarXiv:2305.14314, 2023. [84] N.Du,Y.Huang,A.M.Dai,S.Tong,D.Lepikhin,Y.Xu,M.Krikun, Y. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of [64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, languagemodelswithmixture-of-experts,”inInternationalConference andD.Song,“Koala:Adialoguemodelforacademicresearch,”Blog onMachineLearning. PMLR,"
431,Large Language Models A Survey.pdf,"foracademicresearch,”Blog onMachineLearning. PMLR,2022,pp.5547–5569. post,April,vol.1,2023. [85] R.Thoppilan,D.DeFreitas,J.Hall,N.Shazeer,A.Kulshreshtha,H.- [65] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot, T.Cheng,A.Jin,T.Bos,L.Baker,Y.Duetal.,“Lamda:Language D.d.l.Casas,F.Bressand,G.Lengyel,G.Lample,L.Saulnieretal., models for dialog applications,” arXiv preprint arXiv:2201.08239, “Mistral7b,”arXivpreprintarXiv:2310.06825,2023. 2022. [66] B.Roziere,J.Gehring,F.Gloeckle,S.Sootla,I.G"
432,Large Language Models A Survey.pdf,". [66] B.Roziere,J.Gehring,F.Gloeckle,S.Sootla,I.Gat,X.E.Tan,Y.Adi, [86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, J.Liu,T.Remez,J.Rapinetal.,“Codellama:Openfoundationmodels C. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained forcode,”arXivpreprintarXiv:2308.12950,2023. transformerlanguagemodels,”arXivpreprintarXiv:2205.01068,2022. [67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large [87] R.Taylor,M.Kardas,G.Cucurull,T.Scialom,A.Hartshorn,E.S"
433,Large Language Models A Survey.pdf,"ylor,M.Kardas,G.Cucurull,T.Scialom,A.Hartshorn,E.Sar- languagemodelconnectedwithmassiveapis,”2023. avia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large [68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and languagemodelforscience,”arXivpreprintarXiv:2211.09085,2022. S.Naidu,“Giraffe:Adventuresinexpandingcontextlengthsinllms,” [88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, arXivpreprintarXiv:2308.10882,2023. S. Savarese, and C. Xiong, “Codegen: An open"
434,Large Language Models A Survey.pdf,"2023. S. Savarese, and C. Xiong, “Codegen: An open large language [69] B. Huang, “Vigogne: French instruction-following and chat models,” model for code with multi-turn program synthesis,” arXiv preprint https://github.com/bofenghuang/vigogne,2023. arXiv:2203.13474,2022.
[89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, [110] A.Mitra,L.D.Corro,S.Mahajan,A.Codas,C.Simoes,S.Agarwal, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al., X. Chen, A. Razdaibiedina, E. J"
435,Large Language Models A Survey.pdf," Rumshisky et al., X. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi, “Alexatm 20b: Few-shot learning using a large-scale multilingual G. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2: seq2seqmodel,”arXivpreprintarXiv:2208.01448,2022. Teachingsmalllanguagemodelshowtoreason,”2023. [90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, [111] L.Gao,A.Madaan,S.Zhou,U.Alon,P.Liu,Y.Yang,J.Callan,and T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al., G. N"
436,Large Language Models A Survey.pdf,"L. Weidinger, M. Chadwick, P. Thacker et al., G. Neubig, “Pal: Program-aided language models,” in International “Improving alignment of dialogue agents via targeted human judge- ConferenceonMachineLearning. PMLR,2023,pp.10764–10799. ments,”arXivpreprintarXiv:2209.14375,2022. [112] Anthropic. claude. [Online]. Available: https://www.anthropic.com/ [91] A.Lewkowycz,A.Andreassen,D.Dohan,E.Dyer,H.Michalewski, news/introducing-claude V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al., ["
437,Large Language Models A Survey.pdf,"lone, C. Anil, I. Schlag, T. Gutman-Solo et al., [113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “Solving quantitative reasoning problems with language models,” “Codegen2: Lessons for training llms on programming and natural Advances in Neural Information Processing Systems, vol. 35, pp. languages,”arXivpreprintarXiv:2305.02309,2023. 3843–3857,2022. [114] L.Tunstall,E.Beeching,N.Lambert,N.Rajani,K.Rasul,Y.Belkada, [92] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Sc"
438,Large Language Models A Survey.pdf,". Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, S.Huang,L.vonWerra,C.Fourrier,N.Habibetal.,“Zephyr:Direct H.S.Zheng,N.Houlsby,andD.Metzler,“Unifyinglanguagelearning paradigms,”arXivpreprintarXiv:2205.05131,2022. distillationoflmalignment,”arXivpreprintarXiv:2310.16944,2023. [93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic ́, D. Hesslow, [115] X.team.Grok.[Online].Available:https://grok.x.ai/ R.Castagne ́,A.S.Luccioni,F.Yvon,M.Galle ́etal.,“Bloom:A176b- [116] J. Bai, S. Bai, S. Ya"
439,Large Language Models A Survey.pdf,"e ́etal.,“Bloom:A176b- [116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, parameter open-access multilingual language model,” arXiv preprint and J. Zhou, “Qwen-vl: A frontier large vision-language model with arXiv:2211.05100,2022. versatileabilities,”arXivpreprintarXiv:2308.12966,2023. [94] A.Zeng,X.Liu,Z.Du,Z.Wang,H.Lai,M.Ding,Z.Yang,Y.Xu, [117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/ W. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-traine"
440,Large Language Models A Survey.pdf,"ia et al., “Glm-130b: An open bilingual pre-trained mixtral-of-experts/ model,”arXivpreprintarXiv:2210.02414,2022. [118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei, [95] S.Biderman,H.Schoelkopf,Q.G.Anthony,H.Bradley,K.O’Brien, A. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative E.Hallahan,M.A.Khan,S.Purohit,U.S.Prashanth,E.Raffetal., languagemodelformultimodaldocumentunderstanding,”2023. “Pythia: A suite for analyzing large language models across train- ing and scal"
441,Large Language Models A Survey.pdf,"g large language models across train- ing and scaling,” in International Conference on Machine Learning. [119] D.Guo,Q.Zhu,D.Yang,Z.Xie,K.Dong,W.Zhang,G.Chen,X.Bi, PMLR,2023,pp.2397–2430. Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder: Whenthelargelanguagemodelmeetsprogramming–theriseofcode [96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and intelligence,”2024. A.Awadallah,“Orca:Progressivelearningfromcomplexexplanation tracesofgpt-4,”arXivpreprintarXiv:2306.02"
442,Large Language Models A Survey.pdf,"lanation tracesofgpt-4,”arXivpreprintarXiv:2306.02707,2023. [120] F.Wan,X.Huang,D.Cai,X.Quan,W.Bi,andS.Shi,“Knowledge fusionoflargelanguagemodels,”2024. [97] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M.Marone,C.Akiki,J.Li,J.Chimetal.,“Starcoder:maythesource [121] P.Zhang,G.Zeng,T.Wang,andW.Lu,“Tinyllama:Anopen-source bewithyou!”arXivpreprintarXiv:2305.06161,2023. smalllanguagemodel,”2024. [98] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, [122] C.Wu,Y.Gan,Y."
443,Large Language Models A Survey.pdf,"Hao, S. Singhal, S. Ma, T. Lv, [122] C.Wu,Y.Gan,Y.Ge,Z.Lu,J.Wang,Y.Feng,P.Luo,andY.Shan, L. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you “Llamapro:Progressivellamawithblockexpansion,”2024. need: Aligning perception with language models,” arXiv preprint [123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and arXiv:2302.14045,2023. M.Kazi,“Transformermodels:anintroductionandcatalog,”2023. [99] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut, [124] G. Pen"
444,Large Language Models A Survey.pdf,"aud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut, [124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, J.Schalkwyk,A.M.Dai,A.Hauthetal.,“Gemini:afamilyofhighly H.Alobeidli,B.Pannier,E.Almazrouei,andJ.Launay,“Therefined- capablemultimodalmodels,”arXivpreprintarXiv:2312.11805,2023. web dataset for falcon llm: outperforming curated corpora with web [100] W.Huang,F.Xia,T.Xiao,H.Chan,J.Liang,P.Florence,A.Zeng, data,andwebdataonly,”arXivpreprintarXiv:2306.01116,2023. J. Tompson, I. Mordatch, Y. Che"
445,Large Language Models A Survey.pdf,"v:2306.01116,2023. J. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue: [125] D.Hernandez,T.Brown,T.Conerly,N.DasSarma,D.Drain,S.El- Embodied reasoning through planning with language models,” arXiv Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al., preprintarXiv:2207.05608,2022. “Scaling laws and interpretability of learning from repeated data,” [101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, arXivpreprintarXiv:2205.10487,2022. J. Casper, Z. Liu, S. "
446,Large Language Models A Survey.pdf,"printarXiv:2205.10487,2022. J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti [126] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative et al., “Using deepspeed and megatron to train megatron-turing positionrepresentations,”arXivpreprintarXiv:1803.02155,2018. nlg 530b, a large-scale generative language model,” arXiv preprint arXiv:2201.11990,2022. [127] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En- [102] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: "
447,Large Language Models A Survey.pdf,"Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long- hanced transformer with rotary position embedding,” arXiv preprint documenttransformer,”arXivpreprintarXiv:2004.05150,2020. arXiv:2104.09864,2021. [103] S.Iyer,X.V.Lin,R.Pasunuru,T.Mihaylov,D.Simig,P.Yu,K.Shus- [128] O.Press,N.A.Smith,andM.Lewis,“Trainshort,testlong:Attention ter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language with linear biases enables input length extrapolation,” arXiv preprint model instruction meta "
448,Large Language Models A Survey.pdf,"apolation,” arXiv preprint model instruction meta learning through the lens of generalization,” arXiv:2108.12409,2021. arXivpreprintarXiv:2212.12017,2022. [129] G. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in [104] Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, languagepre-training,”arXivpreprintarXiv:2006.15595,2020. andF.Wei,“Languagemodelsaregeneral-purposeinterfaces,”arXiv [130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, preprintarXiv:2206"
449,Large Language Models A Survey.pdf,"rz, A. Davis, Q. Le, G. Hinton, preprintarXiv:2206.06336,2022. andJ.Dean,“Outrageouslylargeneuralnetworks:Thesparsely-gated [105] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, mixture-of-expertslayer,”arXivpreprintarXiv:1701.06538,2017. and C. Gan, “Principle-driven self-alignment of language mod- [131] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling els from scratch with minimal human supervision,” arXiv preprint to trillion parameter models with simple and effi"
450,Large Language Models A Survey.pdf," to trillion parameter models with simple and efficient sparsity,” The arXiv:2305.03047,2023. JournalofMachineLearningResearch,vol.23,no.1,pp.5232–5270, [106] W. E. team, “Palmyra-base Parameter Autoregressive Language 2022. Model,”https://dev.writer.com,2023. [132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson, [107] ——,“Camel-5binstructgpt,”https://dev.writer.com,2023. “Parameter-efficientmulti-taskfine-tuningfortransformersviashared [108] Yandex. Yalm. [Online]. Available: https://g"
451,Large Language Models A Survey.pdf,"[108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/ hypernetworks,”2021. YaLM-100B [133] S.Zhang,L.Dong,X.Li,S.Zhang,X.Sun,S.Wang,J.Li,R.Hu, [109] M.Teametal.,“Introducingmpt-7b:anewstandardforopen-source, T.Zhang,F.Wu,andG.Wang,“Instructiontuningforlargelanguage commerciallyusablellms,”2023. models:Asurvey,”2023.
[134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task and O. Abend, “q2: Evaluating factual consistency in knowledge- generalizationvianaturallanguagec"
452,Large Language Models A Survey.pdf,"cy in knowledge- generalizationvianaturallanguagecrowdsourcinginstructions,”arXiv groundeddialoguesviaquestiongenerationandquestionanswering,” preprintarXiv:2104.08773,2021. in Proceedings of the 2021 Conference on Empirical Methods in [135] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and H. Hajishirzi, “Self-instruct: Aligning language model with self andS.W.-t.Yih,Eds. OnlineandPuntaCana,DominicanRepublic: gener"
453,Large Language Models A Survey.pdf,"h,Eds. OnlineandPuntaCana,DominicanRepublic: generatedinstructions,”arXivpreprintarXiv:2212.10560,2022. AssociationforComputationalLinguistics,Nov.2021,pp.7856–7870. [Online].Available:https://aclanthology.org/2021.emnlp-main.619 [136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online]. Available: https://github.com/ContextualAI/HALOs/blob/main/assets/ [153] N.Dziri,H.Rashkin,T.Linzen,andD.Reitter,“Evaluatingattribution report.pdf in dialogue systems: The BEGIN benchmark,” Transactio"
454,Large Language Models A Survey.pdf,"dialogue systems: The BEGIN benchmark,” Transactions of the Association for Computational Linguistics, vol. 10, pp. 1066–1083, [137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and 2022.[Online].Available:https://aclanthology.org/2022.tacl-1.62 D. Amodei, “Deep reinforcement learning from human preferences,” Advancesinneuralinformationprocessingsystems,vol.30,2017. [154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim, Y.Liu,andD.Z.Hakkani-Tu ̈r,“Romewasbuiltin1776:Aca"
455,Large Language Models A Survey.pdf,"Y.Liu,andD.Z.Hakkani-Tu ̈r,“Romewasbuiltin1776:Acasestudy [138] H.Lee,S.Phatale,H.Mansoor,K.Lu,T.Mesnard,C.Bishop,V.Car- on factual correctness in knowledge-grounded response generation,” bune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from ArXiv,vol.abs/2110.05456,2021. humanfeedbackwithaifeedback,”arXivpreprintarXiv:2309.00267, [155] S.Min,K.Krishna,X.Lyu,M.Lewis,W.tauYih,P.W.Koh,M.Iyyer, 2023. L. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic [139] R. Rafailov, A"
456,Large Language Models A Survey.pdf,"actscore: Fine-grained atomic [139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and evaluationoffactualprecisioninlongformtextgeneration,”2023. C. Finn, “Direct preference optimization: Your language model is [156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, secretlyarewardmodel,”arXivpreprintarXiv:2305.18290,2023. V. Chaudhary, and M. Young, “Machine learning: The high interest [140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory credit "
457,Large Language Models A Survey.pdf,"asley, O. Ruwase, and Y. He, “Zero: Memory credit card of technical debt,” in SE4ML: Software Engineering for optimizationstowardtrainingtrillionparametermodels,”inSC20:In- MachineLearning(NIPS2014Workshop),2014. ternationalConferenceforHighPerformanceComputing,Networking, [157] Z.Zhang,A.Zhang,M.Li,andA.Smola,“Automaticchainofthought StorageandAnalysis. IEEE,2020,pp.1–16. promptinginlargelanguagemodels,”2022. [141] B.Peng,E.Alcaide,Q.Anthony,A.Albalak,S.Arcadinho,H.Cao, [158] S. Yao, D. Yu, J. "
458,Large Language Models A Survey.pdf,"lbalak,S.Arcadinho,H.Cao, [158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and X.Cheng,M.Chung,M.Grella,K.K.GVetal.,“Rwkv:Reinventing K. Narasimhan, “Tree of thoughts: Deliberate problem solving with rnnsforthetransformerera,”arXivpreprintarXiv:2305.13048,2023. largelanguagemodels,”2023. [142] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang, [159] P. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero- andW.Chen,“Lora:Low-rankadaptationoflargelanguagemodels,” reso"
459,Large Language Models A Survey.pdf,"ora:Low-rankadaptationoflargelanguagemodels,” resource black-box hallucination detection for generative large lan- arXivpreprintarXiv:2106.09685,2021. guagemodels,”2023. [143] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a [160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, neuralnetwork,”arXivpreprintarXiv:1503.02531,2015. and S. Yao, “Reflexion: Language agents with verbal reinforcement [144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: "
460,Large Language Models A Survey.pdf," J. Maybank, and D. Tao, “Knowledge distillation: learning,”2023. A survey,” International Journal of Computer Vision, vol. 129, pp. [161] S.J.Zhang,S.Florin,A.N.Lee,E.Niknafs,A.Marginean,A.Wang, 1789–1819,2021. K.Tyser,Z.Chin,Y.Hicke,N.Singh,M.Udell,Y.Kim,T.Buonassisi, [145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. A. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural eecscurriculumusinglargela"
461,Large Language Models A Survey.pdf,"allucination in natural eecscurriculumusinglargelanguagemodels,”2023. languagegeneration,”ACMComput.Surv.,vol.55,no.12,mar2023. [162] T.Wu,E.Jiang,A.Donsbach,J.Gray,A.Molina,M.Terry,andC.J. [Online].Available:https://doi.org/10.1145/3571730 Cai,“Promptchainer:Chaininglargelanguagemodelpromptsthrough [146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and visualprogramming,”2022. M.Steedman,“Sourcesofhallucinationbylargelanguagemodelson [163] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster,"
462,Large Language Models A Survey.pdf," [163] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and inferencetasks,”2023. J. Ba, “Large language models are human-level prompt engineers,” [147] C.-Y. Lin, “ROUGE: A package for automatic evaluation of 2023. summaries,”inTextSummarizationBranchesOut. Barcelona,Spain: [164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, Association for Computational Linguistics, Jul. 2004, pp. 74–81. N.Goyal,H.Ku ̈ttler,M.Lewis,W.Yih,T.Rockta ̈schel,S.Riedel,and [Online].Avai"
463,Large Language Models A Survey.pdf,"s,W.Yih,T.Rockta ̈schel,S.Riedel,and [Online].Available:https://aclanthology.org/W04-1013 D. Kiela, “Retrieval-augmented generation for knowledge-intensive [148] K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu,“Bleu:amethodfor NLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available: automatic evaluation of machine translation,” in Proceedings of the https://arxiv.org/abs/2005.11401 40thAnnualMeetingoftheAssociationforComputationalLinguistics, [165] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi"
464,Large Language Models A Survey.pdf,"5] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and P.Isabelle,E.Charniak,andD.Lin,Eds. Philadelphia,Pennsylvania, H.Wang,“Retrieval-augmentedgenerationforlargelanguagemodels: USA:AssociationforComputationalLinguistics,Jul.2002,pp.311– Asurvey,”arXivpreprintarXiv:2312.10997,2023. 318.[Online].Available:https://aclanthology.org/P02-1040 [166] A.W.Services.(Yearofpublication,e.g.,2023)Questionanswering [149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and using retr"
465,Large Language Models A Survey.pdf,"ui, A. Parikh, M.-W. Chang, D. Das, and using retrieval augmented generation with foundation models in W. Cohen, “Handling divergent reference texts when evaluating amazon sagemaker jumpstart. Accessed: Date of access, e.g., table-to-text generation,” in Proceedings of the 57th Annual Meeting December5,2023.[Online].Available:https://shorturl.at/dSV47 of the Association for Computational Linguistics, A. Korhonen, D. Traum, and L. Ma`rquez, Eds. Florence, Italy: Association [167] S.Pan,L.Luo,Y.Wa"
466,Large Language Models A Survey.pdf,"lorence, Italy: Association [167] S.Pan,L.Luo,Y.Wang,C.Chen,J.Wang,andX.Wu,“Unifyinglarge for Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online]. languagemodelsandknowledgegraphs:Aroadmap,”arXivpreprint Available:https://aclanthology.org/P19-1483 arXiv:2306.08302,2023. [150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful [168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, neural table-to-text generation with content-matching constraints,” J. Calla"
467,Large Language Models A Survey.pdf,"ation with content-matching constraints,” J. Callan, and G. Neubig, “Active retrieval augmented generation,” in Proceedings of the 58th Annual Meeting of the Association 2023. for Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter, [169] T.Schick,J.Dwivedi-Yu,R.Dess`ı,R.Raileanu,M.Lomeli,L.Zettle- and J. Tetreault, Eds. Online: Association for Computational moyer,N.Cancedda,andT.Scialom,“Toolformer:Languagemodels Linguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https: cantea"
468,Large Language Models A Survey.pdf," pp. 1072–1086. [Online]. Available: https: canteachthemselvestousetools,”2023. //aclanthology.org/2020.acl-main.101 [170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, [151] H.Song,W.-N.Zhang,J.Hu,andT.Liu,“Generatingpersonaconsis- andM.T.Ribeiro,“Art:Automaticmulti-stepreasoningandtool-use tentdialoguesbyexploitingnaturallanguageinference,”Proceedings forlargelanguagemodels,”2023. oftheAAAIConferenceonArtificialIntelligence,vol.34,no.05,pp. [171] Y.Shen,K.Song,X.Tan,D.Li,"
469,Large Language Models A Survey.pdf,"e,vol.34,no.05,pp. [171] Y.Shen,K.Song,X.Tan,D.Li,W.Lu,andY.Zhuang,“Hugginggpt: 8878–8885,Apr.2020. Solving ai tasks with chatgpt and its friends in huggingface,” arXiv [152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, preprintarXiv:2303.17580,2023.
[172] Z.Xi,W.Chen,X.Guo,W.He,Y.Ding,B.Hong,M.Zhang,J.Wang, [189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, S.Jin,E.Zhouetal.,“Theriseandpotentialoflargelanguagemodel “Looking beyond the surface:a challenge set fo"
470,Large Language Models A Survey.pdf,"del “Looking beyond the surface:a challenge set for reading compre- basedagents:Asurvey,”arXivpreprintarXiv:2309.07864,2023. hension over multiple sentences,” in Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), [173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin et al., “A survey on large language model 2018. basedautonomousagents,”arXivpreprintarXiv:2308.11432,2023. [190] K. Cobbe, V. Kosaraju, M. Bavarian,"
471,Large Language Models A Survey.pdf,"32,2023. [190] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and [174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar, J. Schulman, “Training verifiers to solve math word problems,” R.Taori,Y.Noda,D.Terzopoulos,Y.Choi,K.Ikeuchi,H.Vo,L.Fei- CoRR, vol. abs/2110.14168, 2021. [Online]. Available: https: Fei, and J. Gao, “Agent ai: Surveying the horizons of multimodal interaction,”arXivpreprintarXiv:2401.03568,20"
472,Large Language Models A Survey.pdf,"odal interaction,”arXivpreprintarXiv:2401.03568,2024. //arxiv.org/abs/2110.14168 [191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, [175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo: D.Song,andJ.Steinhardt,“Measuringmathematicalproblemsolving Decouplingreasoningfromobservationsforefficientaugmentedlan- withtheMATHdataset,”CoRR,vol.abs/2103.03874,2021.[Online]. guagemodels,”2023. Available:https://arxiv.org/abs/2103.03874 [176] S.Yao,J.Zhao,D.Yu,N.Du,I.Sh"
473,Large Language Models A Survey.pdf,"g/abs/2103.03874 [176] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.Narasimhan,andY.Cao, [192] R.Zellers,A.Holtzman,Y.Bisk,A.Farhadi,andY.Choi,“Hellaswag: “React:Synergizingreasoningandactinginlanguagemodels,”2023. Canamachinereallyfinishyoursentence?”2019. [177] V. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc- [193] P.Clark,I.Cowhey,O.Etzioni,T.Khot,A.Sabharwal,C.Schoenick, ing large language model completions with dialog-enabled resolving and O. Tafjord, “Think you have solved question answ"
474,Large Language Models A Survey.pdf,"d O. Tafjord, “Think you have solved question answering? try agents,”2023. arc,theAI2reasoningchallenge,”CoRR,vol.abs/1803.05457,2018. [178] Y.Chang,X.Wang,J.Wang,Y.Wu,L.Yang,K.Zhu,H.Chen,X.Yi, [Online].Available:http://arxiv.org/abs/1803.05457 C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, [194] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA: andX.Xie,“Asurveyonevaluationoflargelanguagemodels,”2023. reasoning about physical commonsense in natural language,” CoRR, ["
475,Large Language Models A Survey.pdf,"physical commonsense in natural language,” CoRR, [179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, vol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/ C.Alberti,D.Epstein,I.Polosukhin,J.Devlin,K.Lee,K.Toutanova, 1911.11641 L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, [195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa: Q. Le, and S. Petrov, “Natural questions: A benchmark for Commonsense reasoning about social interactions,”"
476,Large Language Models A Survey.pdf," Commonsense reasoning about social interactions,” CoRR, vol. question answering research,” Transactions of the Association for abs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904. Computational Linguistics, vol. 7, pp. 452–466, 2019. [Online]. 09728 Available:https://aclanthology.org/Q19-1026 [196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of [180] D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,and armor conduct electricity? A new dataset for open book "
477,Large Language Models A Survey.pdf," conduct electricity? A new dataset for open book question J.Steinhardt,“Measuringmassivemultitasklanguageunderstanding,” answering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available: 2021. http://arxiv.org/abs/1809.02789 [181] J.Austin,A.Odena,M.Nye,M.Bosma,H.Michalewski,D.Dohan, [197] S.Lin,J.Hilton,andO.Evans,“Truthfulqa:Measuringhowmodels E.Jiang,C.Cai,M.Terry,Q.Leetal.,“Programsynthesiswithlarge mimichumanfalsehoods,”arXivpreprintarXiv:2109.07958,2021. languagemodels,”arXivpreprintarXiv"
478,Large Language Models A Survey.pdf,"109.07958,2021. languagemodels,”arXivpreprintarXiv:2108.07732,2021. [198] Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov, [182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and C. D. Manning, “Hotpotqa: A dataset for diverse, explainable and L. Zettlemoyer, “QuAC: Question answering in context,” in multi-hop question answering,” CoRR, vol. abs/1809.09600, 2018. Proceedingsofthe2018ConferenceonEmpiricalMethodsinNatural [Online].Available:http://arxiv.org/abs/1809.09"
479,Large Language Models A Survey.pdf,"al [Online].Available:http://arxiv.org/abs/1809.09600 Language Processing, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Brussels, Belgium: Association for Computational [199] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A Linguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available: datasetforllmquestionansweringwithexternaltools,”arXivpreprint https://aclanthology.org/D18-1241 arXiv:2306.13304,2023. [183] D.Hendrycks,S.Basart,S.Kadavath,M.Mazeika,A.Arora,E.Guo, [200"
480,Large Language Models A Survey.pdf,",S.Basart,S.Kadavath,M.Mazeika,A.Arora,E.Guo, [200] D. Chen, J. Bolton, and C. D. Manning, “A thorough examination C.Burns,S.Puranik,H.He,D.Song,andJ.Steinhardt,“Measuring ofthecnn/dailymailreadingcomprehensiontask,”inAssociationfor codingchallengecompetencewithapps,”NeurIPS,2021. ComputationalLinguistics(ACL),2016. [184] V. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured [201] R.Nallapati,B.Zhou,C.Gulcehre,B.Xiangetal.,“Abstractivetext queries from natural language using reinfor"
481,Large Language Models A Survey.pdf,"vetext queries from natural language using reinforcement learning,” arXiv summarization using sequence-to-sequence rnns and beyond,” arXiv preprintarXiv:1709.00103,2017. preprintarXiv:1602.06023,2016. [185] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA: [202] Y.BaiandD.Z.Wang,“Morethanreadingcomprehension:Asurvey A large scale distantly supervised challenge dataset for reading ondatasetsandmetricsoftextualquestionanswering,”arXivpreprint comprehension,” in Proceedings of the 55th Ann"
482,Large Language Models A Survey.pdf,"int comprehension,” in Proceedings of the 55th Annual Meeting of the arXiv:2109.12264,2021. Association for Computational Linguistics (Volume 1: Long Papers), [203] H.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Association history for conversational machine comprehension,” arXiv preprint for Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online]. arXiv:1810.06683,2018. Available:https://aclanthology.org/P17-1147 [204] S.L"
483,Large Language Models A Survey.pdf,"ilable:https://aclanthology.org/P17-1147 [204] S.Lee,J.Lee,H.Moon,C.Park,J.Seo,S.Eo,S.Koo,andH.Lim,“A [186] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale survey on evaluation metrics for machine translation,” Mathematics, ReAding comprehension dataset from examinations,” in Proceedings vol.11,no.4,p.1006,2023. of the 2017 Conference on Empirical Methods in Natural Language Processing, M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen, [205] J. Li, X. Cheng, W. X. Zhao, J.-Y. N"
484,Large Language Models A Survey.pdf,"nhagen, [205] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval: A large-scale hallucination evaluation benchmark for large language Denmark: Association for Computational Linguistics, Sep. 2017, pp. models,”inProceedingsofthe2023ConferenceonEmpiricalMethods 785–794.[Online].Available:https://aclanthology.org/D17-1082 inNaturalLanguageProcessing,2023,pp.6449–6464. [187] P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang,“SQuAD:100,000+ questions for machine comprehension of text,” in Procee"
485,Large Language Models A Survey.pdf,"ions for machine comprehension of text,” in Proceedings of [206] Simon Mark Hughes, “Hughes hallucination evaluation model the 2016 Conference on Empirical Methods in Natural Language (hhem) leaderboard,” 2024, https://huggingface.co/spaces/vectara/ Processing, J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas: Hallucination-evaluation-leaderboard,Lastaccessedon2024-01-21. AssociationforComputationalLinguistics,Nov.2016,pp.2383–2392. [207] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Railean"
486,Large Language Models A Survey.pdf,"ddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and [Online].Available:https://aclanthology.org/D16-1264 R.McHardy,“Challengesandapplicationsoflargelanguagemodels,” arXivpreprintarXiv:2307.10169,2023. [188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, “Boolq: Exploring the surprising difficulty of natural [208] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, yes/no questions,” CoRR, vol. abs/1905.10044, 2019. [Online]. S.Gopi,M.Javaheripi,P.Kau"
487,Large Language Models A Survey.pdf,"5.10044, 2019. [Online]. S.Gopi,M.Javaheripi,P.Kauffmann,G.deRosa,O.Saarikivietal., Available:http://arxiv.org/abs/1905.10044 “Textbooksareallyouneed,”arXivpreprintarXiv:2306.11644,2023.
[209] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. [237] prompttools. prompttools. [Online]. Available: https://github.com/ Lee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv hegelai/prompttools preprintarXiv:2309.05463,2023. [238] promptfoo. promptfoo. [Online]. Availabl"
488,Large Language Models A Survey.pdf,"23. [238] promptfoo. promptfoo. [Online]. Available: https://github.com/ [210] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, promptfoo/promptfoo Y. Bengio, S. Ermon, and C. Re ́, “Hyena hierarchy: Towards larger [239] facebook. faiss. [Online]. Available: https://github.com/ convolutionallanguagemodels,”2023. facebookresearch/faiss [211] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and [240] milvus. milvus. [Online]. Available: https://github.com/milvus-io/ A. Thomas"
489,Large Language Models A Survey.pdf,"Available: https://github.com/milvus-io/ A. Thomas, “StripedHyena: Moving Beyond Transformers with milvus Hybrid Signal Processing Models,” 12 2023. [Online]. Available: [241] qdrant.qdrant.[Online].Available:https://github.com/qdrant/qdrant https://github.com/togethercomputer/stripedhyena [242] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/ [212] D.Y.Fu,S.Arora,J.Grogan,I.Johnson,S.Eyuboglu,A.W.Thomas, weaviate B.Spector,M.Poli,A.Rudra,andC.Re ́,“Monarchmixer:Asimple sub-"
490,Large Language Models A Survey.pdf,".Poli,A.Rudra,andC.Re ́,“Monarchmixer:Asimple sub-quadraticgemm-basedarchitecture,”2023. [243] llama index. llama-index. [Online]. Available: https://github.com/ run-llama/llama index [213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture models,” Annual review of statistics and its application, vol. 6, pp. 355–378,2019. APPENDIX [214] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstructiontuning,”arXiv preprintarXiv:2304.08485,2023. 1. Open Source Toolkits For LLM Development and Deploym"
491,Large Language Models A Survey.pdf,"en Source Toolkits For LLM Development and Deployment [215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus: There are various frameworks and libraries developed for Learningtousetoolsforcreatingmultimodalagents,”arXivpreprint arXiv:2311.05437,2023. LLMtraining,evaluation,anddeployment,andcoveringevery single framework is out of this paper’s scope. But we try to [216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gp"
492,Large Language Models A Survey.pdf,"Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any multimodalllm,”arXivpreprintarXiv:2309.05519,2023. provideabriefintroductionofsomeofthemostpopularones, [217] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and grouped into different categories. D. Zu ̈hlke, “Convgenvismo: Evaluation of conversational generative visionmodels,”2023. A. LLM Training/Inference Frameworks [218] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman, I.Harper,A.Marginean,S.Sengupta,a"
493,Large Language Models A Survey.pdf,"kaya, M. Harman, I.Harper,A.Marginean,S.Sengupta,andE.Wang,“Automatedunit SomeofthepopularframeworkswhichareusefulforLLM testimprovementusinglargelanguagemodelsatmeta,”arXivpreprint training includes (note that some of them can be used beyond arXiv:2402.09171,2024. LLM training too): [219] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large DeepSpeed [220] is a deep learning optimization library languagemodels,”arXivpr"
494,Large Language Models A Survey.pdf,"rning optimization library languagemodels,”arXivpreprintarXiv:2401.05561,2024. that makes distributed training and inference easy, efficient, [220] Microsoft. Deepspeed. [Online]. Available: https://github.com/ and effective. DeepSpeed enables world’s most powerful lan- microsoft/DeepSpeed guage models like MT-530B and BLOOM. It is an easy- [221] HuggingFace. Transformers. [Online]. Available: https://github.com/ to-use deep learning optimization software suite that powers huggingface/transforme"
495,Large Language Models A Survey.pdf, software suite that powers huggingface/transformers unprecedentedscaleandspeedforbothtrainingandinference. [222] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/ With DeepSpeed you can: Megatron-LM [223] BMTrain.Bmtrain.[Online].Available:https://github.com/OpenBMB/ Transformers [221] is library by HuggingFace which BMTrain provides thousands of pretrained models to perform tasks on [224] EleutherAI. gpt-neox. [Online]. Available: https://github.com/ different modalities such a
496,Large Language Models A Survey.pdf,"e: https://github.com/ different modalities such as text, vision, and audio. Using EleutherAI/gpt-neox pretrained models one can reduce compute costs, carbon [225] microsoft. Lora. [Online]. Available: https://github.com/microsoft/ footprint, and save the time and resources required to train LoRA a model from scratch. [226] ColossalAI. Colossalai. [Online]. Available: https://github.com/ hpcaitech/ColossalAI Megatron-LM [222] is a large, powerful transformer [227] FastChat. Fastchat. [Online]. A"
497,Large Language Models A Survey.pdf," transformer [227] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/ developed by the Applied Deep Learning Research team FastChat at NVIDIA. It contains efficient, model-parallel (tensor, se- [228] skypilot.skypilot.[Online].Available:https://github.com/skypilot-org/ quence, and pipeline), and multi-node pre-training of trans- skypilot formerbasedmodelssuchasGPT,BERT,andT5usingmixed [229] vllm.vllm.[Online].Available:https://github.com/vllm-project/vllm precision. [230] huggin"
498,Large Language Models A Survey.pdf,thub.com/vllm-project/vllm precision. [230] huggingface. text-generation-inference. [Online]. Available: https: //github.com/huggingface/text-generation-inference BMTrain [223] is an efficient large model training toolkit [231] langchain. langchain. [Online]. Available: https://github.com/ that can be used to train large models with tens of billions of langchain-ai/langchain parameters. It can train models in a distributed manner while [232] bentoml. Openllm. [Online]. Available: https://github.
499,Large Language Models A Survey.pdf,oml. Openllm. [Online]. Available: https://github.com/bentoml/ keeping the code as simple as stand-alone training. OpenLLM [233] embedchain. embedchain. [Online]. Available: https://github.com/ GPT-NeoX[224]leveragesmanyofthesamefeaturesand embedchain/embedchain technologies as the popular Megatron-DeepSpeed library but [234] microsoft. autogen. [Online]. Available: https://github.com/microsoft/ with substantially increased usability and novel optimizations. autogen LoRA [225] library provides t
500,Large Language Models A Survey.pdf,"timizations. autogen LoRA [225] library provides the support for Low-Rank [235] babyagi. babyagi. [Online]. Available: https://github.com/ yoheinakajima/babyagi Adaptation of Large Language Models. It reduces the number oftrainableparametersbylearningpairsofrank-decompostion [236] guidance. guidance. [Online]. Available: https://github.com/ guidance-ai/guidance matrices while freezing the original weights. This vastly
reduces the storage requirement for large language models relevant embeddings,"
501,Large Language Models A Survey.pdf,"ent for large language models relevant embeddings, and stores them in a vector database for adapted to specific tasks and enables efficient task-switching optimized retrieval. during deployment all without introducing inference latency. Autogen [234] is a framework that enables the devel- LoRA also outperforms several other adaptation methods in- opment of LLM applications using multiple agents that can cluding adapter, prefix-tuning, and fine-tuning. converse with each other to solve tasks. Aut"
502,Large Language Models A Survey.pdf,"ning. converse with each other to solve tasks. AutoGen agents ColossalAI library [226] provides a collection of parallel are customizable, conversable, and seamlessly allow human components. It aims to support developers to write their participation. They can operate in various modes that employ distributed deep learning models just like how they write their combinations of LLMs, human inputs, and tools. model on their laptop. They provide user-friendly tools to BabyAGI [235] is an autonomous Ar"
503,Large Language Models A Survey.pdf,"riendly tools to BabyAGI [235] is an autonomous Artificial Intelligence kickstart distributed training and inference in a few lines. In agent, that is designed to generate and execute tasks based on terms of Parallelism strategies, they support: Data Parallelism, given objectives. It harnesses cutting-edge technologies from Pipeline Parallelism, Sequence Parallelism, Zero Redundancy OpenAI, Pinecone, LangChain, and Chroma to automate tasks Optimizer (ZeRO) [140], and Auto-Parallelism. and achiev"
504,Large Language Models A Survey.pdf,"zer (ZeRO) [140], and Auto-Parallelism. and achieve specific goals. In this blog post, we will dive into the unique features of BabyAGI and explore how it can B. Deployment Tools streamline task automation. WeprovideanoverviewofsomeofthemostpopularLLM C. Prompting Libraries deployment tools here. Guidance [236] is a programming paradigm that offers FastChat [227] is an open platform for training, serv- superior control and efficiency compared to conventional ing, and evaluating large language mo"
505,Large Language Models A Survey.pdf,"conventional ing, and evaluating large language model based chatbots. promptingandchaining.Itallowsuserstoconstraingeneration FastChat’s core features include: The training and evaluation (e.g. with regex and CFGs) as well as to interleave control codeforstate-of-the-artmodels(e.g.,Vicuna,MT-Bench),and (conditional, loops) and generation seamlessly. a distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs. PromptTools [237] offers a set of open-source, self- hostab"
506,Large Language Models A Survey.pdf,"ls [237] offers a set of open-source, self- hostable tools for experimenting with, testing, and evaluating Skypilot [228] is a framework for running LLMs, AI, LLMs, vector databases, and prompts. The core idea is to and batch jobs on any cloud, offering maximum cost savings, enable developers to evaluate using familiar interfaces like highest GPU availability, and managed execution. code, notebooks, and a local playground. vLLM [229] is a fast and easy-to-use library for LLM in- PromptBench [?] "
507,Large Language Models A Survey.pdf,"d easy-to-use library for LLM in- PromptBench [?] is a Pytorch-based Python package for ferenceandserving.vLLMseamlesslysupportsmanyHugging Evaluation of Large Language Models (LLMs). It provides Face models, including the following architectures: Aquila, user-friendly APIs for researchers to conduct evaluation on Baichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big- LLMs. Code,LLaMA,LLaMA2,Mistral,Mixtral,MPT,OPT,Qwen, Promptfoo [238] is a tool for testing and evaluating LLM Yi, and many more. out"
508,Large Language Models A Survey.pdf," testing and evaluating LLM Yi, and many more. output quality. It systematically test prompts, models, and text-generation-inference [230] is a toolkit for deploying RAGs with predefined test cases. and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open- D. VectorDB source LLMs, including Llama, Falcon, StarCoder, BLOOM, Faiss [239] is a library developed by Facebook AI Re- GPT-NeoX, and more. search that provides efficient similarity sea"
509,Large Language Models A Survey.pdf,"ore. search that provides efficient similarity search and clustering LangChain [231] is a framework for developing applica- of dense vectors. It is designed for use with large-scale, tionspoweredbylanguagemodels.Itenablesapplicationsthat: high-dimensional data and supports several index types and algorithms for various use cases. • Are context-aware: connect a language model to Milvus [240] is an open-source vector database built to sources of context (prompt instructions, few shot ex- power emb"
510,Large Language Models A Survey.pdf,"ntext (prompt instructions, few shot ex- power embedding similarity search and AI applications. Mil- amples, content to ground its response in, etc.) vus makes unstructured data search more accessible, and pro- • Reason: rely on a language model to reason (about videsaconsistentuserexperienceregardlessofthedeployment how to answer based on provided context, what ac- environment. tions to take, etc.) Qdrant [241] is a vector similarity search engine and vector database. It provides a production-r"
511,Large Language Models A Survey.pdf,"ne and vector database. It provides a production-ready service with a OpenLLM [232] is an open-source platform designed to convenient API to store, search, and manage points—vectors facilitatethedeploymentandoperationoflargelanguagemod- with an additional payload Qdrant is tailored to extended els (LLMs) in real-world applications. With OpenLLM, you filtering support. environment. can run inference on any open-source LLM, deploy them on the cloud or on-premises, and build powerful AI application"
512,Large Language Models A Survey.pdf," or on-premises, and build powerful AI applications. Weaviate [242] is an open-source, GraphQL-based vec- tor search engine that enables similarity search on high- Embedchain [233] is an Open Source RAG Framework dimensionaldata.Whileitisopen-source,thecommercialver- that makes it easy to create and deploy AI apps. Embedchain sion offers additional features, support, and managed services. streamlinesthecreationofRAGapplications,offeringaseam- less process for managing various types of unstructur"
513,Large Language Models A Survey.pdf,"s process for managing various types of unstructured data. Some of the other popular options includes LlamaIndex Itefficientlysegmentsdataintomanageablechunks,generates [243] and Pinecone.
"
514,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks PatrickLewis†‡,EthanPerez?, AleksandraPiktus†,FabioPetroni†,VladimirKarpukhin†,NamanGoyal†,HeinrichKüttler†, MikeLewis†,Wen-tauYih†,TimRocktäschel†‡,SebastianRiedel†‡,DouweKiela† FacebookAIResearch; UniversityCollegeLondon;?NewYorkUniversity; † ‡ plewis@fb.com Abstract Largepre-trainedlanguagemodelshavebeenshowntostorefactualknowledge intheirparameters,andachievestate-of-the-artresultswhenfine-tunedondown- stream NLP tasks. However"
515,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ltswhenfine-tunedondown- stream NLP tasks. However, their ability to access and precisely manipulate knowledgeisstilllimited,andhenceonknowledge-intensivetasks,theirperfor- mancelagsbehindtask-specificarchitectures. Additionally,providingprovenance fortheirdecisionsandupdatingtheirworldknowledgeremainopenresearchprob- lems. Pre-trainedmodelswithadifferentiableaccessmechanismtoexplicitnon- parametricmemorycanovercomethisissue,buthavesofarbeenonlyinvestigated forextractivedownstreamtasks. Weexplor"
516,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"nvestigated forextractivedownstreamtasks. Weexploreageneral-purposefine-tuningrecipe forretrieval-augmentedgeneration(RAG)—modelswhichcombinepre-trained parametricandnon-parametricmemoryforlanguagegeneration. Weintroduce RAGmodelswheretheparametricmemoryisapre-trainedseq2seqmodeland thenon-parametricmemoryisadensevectorindexofWikipedia,accessedwith a pre-trained neural retriever. We compare two RAG formulations, one which conditionsonthesameretrievedpassagesacrossthewholegeneratedsequence, andan"
517,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"evedpassagesacrossthewholegeneratedsequence, andanotherwhichcanusedifferentpassagespertoken. Wefine-tuneandevaluate ourmodelsonawiderangeofknowledge-intensiveNLPtasksandsetthestateof theartonthreeopendomainQAtasks,outperformingparametricseq2seqmodels andtask-specificretrieve-and-extractarchitectures. Forlanguagegenerationtasks, wefindthatRAGmodelsgeneratemorespecific,diverseandfactuallanguagethan astate-of-the-artparametric-onlyseq2seqbaseline. 1 Introduction Pre-trainedneurallanguagemodelshaveb"
518,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf," Introduction Pre-trainedneurallanguagemodelshavebeenshowntolearnasubstantialamountofin-depthknowl- edgefromdata[47]. Theycandosowithoutanyaccesstoanexternalmemory,asaparameterized implicitknowledgebase[51,52]. Whilethisdevelopmentisexciting,suchmodelsdohavedown- sides: Theycannoteasilyexpandorrevisetheirmemory,can’tstraightforwardlyprovideinsightinto theirpredictions,andmayproduce“hallucinations”[38]. Hybridmodelsthatcombineparametric memorywithnon-parametric(i.e.,retrieval-based)memories[20,26"
519,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"non-parametric(i.e.,retrieval-based)memories[20,26,48]canaddresssomeofthese issuesbecauseknowledgecanbedirectlyrevisedandexpanded,andaccessedknowledgecanbe inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combinemaskedlanguagemodels[8]withadifferentiableretriever,haveshownpromisingresults, 34thConferenceonNeuralInformationProcessingSystems(NeurIPS2020),Vancouver,Canada.
Define(cid:3)""middle(cid:3)ea(cid:85)""(x) The(cid:3)middle(cid:3)ea(cid:85)(cid:3)incl("
520,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"""(x) The(cid:3)middle(cid:3)ea(cid:85)(cid:3)incl(cid:88)de(cid:86) End-to-End Backprop through (cid:84) and(cid:172)p θ (cid:87)he(cid:3)(cid:87)(cid:92)mpanic(cid:3)ca(cid:89)i(cid:87)(cid:92)(cid:3)and Q(cid:88)e(cid:86)(cid:87)i(cid:82)(cid:81) A(cid:81)(cid:86)(cid:90)e(cid:85)i(cid:81)g: (cid:87)he(cid:3)(cid:87)h(cid:85)ee(cid:3)o(cid:86)(cid:86)icle(cid:86).(cid:3)(cid:3)(y) Q(cid:88)e(cid:86)(cid:87)i(cid:82)(cid:81) Q(cid:88)e(cid:85)(cid:92) E Q (cid:81)c (cid:88) (cid:82) e d (cid:85"
521,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,id:92) E Q (cid:81)c (cid:88) (cid:82) e d (cid:85)(cid:92) e(cid:85) Re(cid:87)(cid:85)ie(cid:89)e(cid:85) (cid:83) η D(cid:82) I c (cid:81) (cid:88) d m e(cid:91) e(cid:81)(cid:87) Gene(cid:85)a(cid:87)o(cid:85)(cid:172)(cid:83)(cid:1121) Q A (cid:88) (cid:81) e (cid:86) (cid:86) (cid:90) (cid:87)i e (cid:82) (cid:85) (cid:81) G A e (cid:81) (cid:81) (cid:86) e (cid:90) (cid:85)a e (cid:87) (cid:85) i i (cid:82) (cid:81) (cid:81) g: (N(cid:82)(cid:81)-Pa(cid:85)a(cid:80)e(cid:87)(cid:85)(cid:7
522,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,cid:81)-Pa(cid:85)a(cid:80)e(cid:87)(cid:85)(cid:76)c) (Pa(cid:85)a(cid:80)e(cid:87)(cid:85)(cid:76)c) B b a o (cid:85) (cid:85) a n c (cid:3) k i (cid:3) n O (cid:3) b H a a m (cid:90) a a (cid:3) i (cid:90) i a .( (cid:86) x) (cid:84)((cid:91)) d((cid:93)) z4 (cid:86)(cid:88)ppo(cid:85)(cid:87)(cid:86)(cid:3)(y) Fac(cid:87) Ve(cid:85)i(cid:192)ca(cid:87)i(cid:82)(cid:81): Fac(cid:87) Q(cid:88)e(cid:85)(cid:92) z3 Ma(cid:85)gin- Fac(cid:87) Ve(cid:85)i(cid:192)ca(cid:87)i(cid:82)(cid:81): z2 al
523,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,id:85)i(cid:192)ca(cid:87)i(cid:82)(cid:81): z2 ali(cid:93)e Labe(cid:79) Ge(cid:81)e(cid:85)a(cid:87)i(cid:82)(cid:81) T C h o e m (cid:3) e D d i (cid:92) (cid:89) (cid:3)( i x n ) e (cid:84) MIPS z1 p θ T i h (cid:86) i (cid:3) (cid:86) d (cid:3) i 1 (cid:89) 4 i (cid:87) d h e (cid:3) d c (cid:3) e i n n (cid:87) (cid:87) (cid:88) o (cid:85) (cid:3) (cid:92) 3 (cid:3)(cid:90)o(cid:85)k Je(cid:82)(cid:83)a(cid:85)d(cid:92) Q(cid:88)e(cid:86)(cid:87)i(cid:82)(cid:81) (cid:86)ec(cid:87)ion(cid:
524,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"id:87)i(cid:82)(cid:81) (cid:86)ec(cid:87)ion(cid:86):(cid:3)""Infe(cid:85)no"", Ge(cid:81)e(cid:85)a(cid:87)i(cid:82)(cid:81): ""P(cid:88)(cid:85)ga(cid:87)o(cid:85)io""(cid:3)& A(cid:81)(cid:86)(cid:90)e(cid:85) Q(cid:88)e(cid:85)(cid:92) ""Pa(cid:85)adi(cid:86)o""(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(y) Q(cid:88)e(cid:86)(cid:87)i(cid:82)(cid:81) Ge(cid:81)e(cid:85)a(cid:87)i(cid:82)(cid:81) Figure1:Overviewofourapproach.Wecombineapre-trainedretriever(QueryEncoder+Document"
525,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"combineapre-trainedretriever(QueryEncoder+Document Index)withapre-trainedseq2seqmodel(Generator)andfine-tuneend-to-end. Forqueryx,weuse MaximumInnerProductSearch(MIPS)tofindthetop-Kdocumentsz . Forfinalpredictiony,we i treatzasalatentvariableandmarginalizeoverseq2seqpredictionsgivendifferentdocuments. buthaveonlyexploredopen-domainextractivequestionanswering. Here,webringhybridparametric andnon-parametricmemorytothe“workhorseofNLP,”i.e. sequence-to-sequence(seq2seq)models. Weendowpre-trained,par"
526,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"to-sequence(seq2seq)models. Weendowpre-trained,parametric-memorygenerationmodelswithanon-parametricmemorythrough ageneral-purposefine-tuningapproachwhichwerefertoasretrieval-augmentedgeneration(RAG). WebuildRAGmodelswheretheparametricmemoryisapre-trainedseq2seqtransformer,andthe non-parametricmemoryisadensevectorindexofWikipedia,accessedwithapre-trainedneural retriever. Wecombinethesecomponentsinaprobabilisticmodeltrainedend-to-end(Fig. 1). The retriever(DensePassageRetriever[26],henceforthDPR)p"
527,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"etriever(DensePassageRetriever[26],henceforthDPR)provideslatentdocumentsconditionedon theinput,andtheseq2seqmodel(BART[32])thenconditionsontheselatentdocumentstogetherwith theinputtogeneratetheoutput. Wemarginalizethelatentdocumentswithatop-Kapproximation, eitheronaper-outputbasis(assumingthesamedocumentisresponsibleforalltokens)oraper-token basis(wheredifferentdocumentsareresponsiblefordifferenttokens). LikeT5[51]orBART,RAG canbefine-tunedonanyseq2seqtask,wherebyboththegeneratorandretrieverarej"
528,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"eq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned. Therehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric memorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive knowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgei"
529,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ainedaccessmechanisms,theabilitytoaccessknowledgeis presentwithoutadditionaltraining. Ourresultshighlightthebenefitsofcombiningparametricandnon-parametricmemorywithgenera- tionforknowledge-intensivetasks—tasksthathumanscouldnotreasonablybeexpectedtoperform withoutaccesstoanexternalknowledgesource. OurRAGmodelsachievestate-of-the-artresults onopenNaturalQuestions[29],WebQuestions[3]andCuratedTrec[2]andstronglyoutperform recentapproachesthatusespecialisedpre-trainingobjectivesonTriviaQA[24]. Despi"
530,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ialisedpre-trainingobjectivesonTriviaQA[24]. Despitethesebeing extractivetasks,wefindthatunconstrainedgenerationoutperformspreviousextractiveapproaches. Forknowledge-intensivegeneration,weexperimentwithMS-MARCO[1]andJeopardyquestion generation, and we find that our models generate responses that are more factual, specific, and diversethanaBARTbaseline. ForFEVER[56]factverification,weachieveresultswithin4.3%of state-of-the-artpipelinemodelswhichusestrongretrievalsupervision. Finally,wedemonstrate"
531,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"estrongretrievalsupervision. Finally,wedemonstratethat thenon-parametricmemorycanbereplacedtoupdatethemodels’knowledgeastheworldchanges.1 2 Methods WeexploreRAGmodels,whichusetheinputsequencextoretrievetextdocumentszandusethem as additional context when generating the target sequence y. As shown in Figure 1, our models leveragetwocomponents: (i)aretrieverp (z x)withparameters⌘ thatreturns(top-Ktruncated) ⌘ | distributionsovertextpassagesgivenaqueryxand(ii)ageneratorp (y x,z,y )parametrized ✓ i 1"
532,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ryxand(ii)ageneratorp (y x,z,y )parametrized ✓ i 1:i 1 |   1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ersLibrary[66]andcanbefoundathttps://github.com/huggingface/transformers/blob/master/ examples/rag/.AninteractivedemoofRAGmodelscanbefoundathttps://huggingface.co/rag/ 2
by✓thatgeneratesacurrenttokenbasedonacontextofthepreviousi 1tokensy ,theoriginal 1:i 1 inputxandaretrievedpassagez.     Totraintheretrieverandgeneratorend-to-end,wetreattheretri"
533,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"theretrieverandgeneratorend-to-end,wetreattheretrieveddocumentasalatentvariable. Weproposetwomodelsthatmarginalizeoverthelatentdocumentsindifferentwaystoproducea distributionovergeneratedtext. Inoneapproach,RAG-Sequence,themodelusesthesamedocument topredicteachtargettoken. Thesecondapproach,RAG-Token,canpredicteachtargettokenbased onadifferentdocument. Inthefollowing,weformallyintroducebothmodelsandthendescribethe p andp components,aswellasthetraininganddecodingprocedure. ⌘ ✓ 2.1 Models RAG-Sequ"
534,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ininganddecodingprocedure. ⌘ ✓ 2.1 Models RAG-SequenceModel TheRAG-Sequencemodelusesthesameretrieveddocumenttogenerate thecompletesequence. Technically,ittreatstheretrieveddocumentasasinglelatentvariablethat ismarginalizedtogettheseq2seqprobabilityp(y x)viaatop-Kapproximation. Concretely,the | topKdocumentsareretrievedusingtheretriever,andthegeneratorproducestheoutputsequence probabilityforeachdocument,whicharethenmarginalized, N p (y x) p (z x)p (y x,z) = p (z x) p (y x,z,y ) RAG-Sequence | ⇡ ⌘"
535,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf," (y x,z) = p (z x) p (y x,z,y ) RAG-Sequence | ⇡ ⌘ | ✓ | ⌘ | ✓ i | 1:i   1 z 2 topX-k(p( ·| x)) z 2 topX-k(p( ·| x)) Y i RAG-TokenModel IntheRAG-Tokenmodelwecandrawadifferentlatentdocumentforeach targettokenandmarginalizeaccordingly. Thisallowsthegeneratortochoosecontentfromseveral documents when producing an answer. Concretely, the top K documents are retrieved using the retriever,andthenthegeneratorproducesadistributionforthenextoutputtokenforeachdocument, beforemarginalizing,andrepeatingthepr"
536,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"achdocument, beforemarginalizing,andrepeatingtheprocesswiththefollowingoutputtoken,Formally,wedefine: N p (y x) p (z x)p (y x,z ,y ) RAG-Token | ⇡ ⌘ | ✓ i | i 1:i   1 Y i z 2 topX-k(p( ·| x)) Finally,wenotethatRAGcanbeusedforsequenceclassificationtasksbyconsideringthetargetclass asatargetsequenceoflengthone,inwhichcaseRAG-SequenceandRAG-Tokenareequivalent. 2.2 Retriever: DPR Theretrievalcomponentp (z x)isbasedonDPR[26]. DPRfollowsabi-encoderarchitecture: ⌘ | p ⌘ (z x) exp d(z) > q(x) d(z)=BERT d"
537,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"tecture: ⌘ | p ⌘ (z x) exp d(z) > q(x) d(z)=BERT d (z), q(x)=BERT q (x) | / whered(z)isadenserepre sentationofa documentproducedbyaBERT BASE documentencoder[8], andq(x)aqueryrepresentationproducedbyaqueryencoder,alsobasedonBERT . Calculating BASE top-k(p ( x)),thelistofkdocumentszwithhighestpriorprobabilityp (z x),isaMaximumInner ⌘ ⌘ ·| | ProductSearch(MIPS)problem,whichcanbeapproximatelysolvedinsub-lineartime[23]. Weuse apre-trainedbi-encoderfromDPRtoinitializeourretrieverandtobuildthedocumenti"
538,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"mDPRtoinitializeourretrieverandtobuildthedocumentindex. This retrieverwastrainedtoretrievedocumentswhichcontainanswerstoTriviaQA[24]questionsand NaturalQuestions[29]. Werefertothedocumentindexasthenon-parametricmemory. 2.3 Generator: BART Thegeneratorcomponentp (y x,z,y )couldbemodelledusinganyencoder-decoder. Weuse ✓ i 1:i 1 BART-large[32],apre-trainedse | q2seqtra nsformer[58]with400Mparameters.Tocombinetheinput xwiththeretrievedcontentzwhengeneratingfromBART,wesimplyconcatenatethem. BARTwas p"
539,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"eratingfromBART,wesimplyconcatenatethem. BARTwas pre-trainedusingadenoisingobjectiveandavarietyofdifferentnoisingfunctions. Ithasobtained state-of-the-artresultsonadiversesetofgenerationtasksandoutperformscomparably-sizedT5 models[32]. WerefertotheBARTgeneratorparameters✓astheparametricmemoryhenceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what documentshouldberetrieved. Givenafine-tuningtrainingcorpusofinput/outputpairs(x ,y ),we"
540,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"-tuningtrainingcorpusofinput/outputpairs(x ,y ),we j j 3
minimizethenegativemarginallog-likelihoodofeachtarget, logp(y x )usingstochastic j  j | j gradientdescentwithAdam[28]. UpdatingthedocumentencoderBERT duringtrainingiscostlyas d P itrequiresthedocumentindextobeperiodicallyupdatedasREALMdoesduringpre-training[20]. We do not find this step necessary for strong performance, and keep the document encoder (and index)fixed,onlyfine-tuningthequeryencoderBERT andtheBARTgenerator. q 2.5 Decoding Att"
541,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ncoderBERT andtheBARTgenerator. q 2.5 Decoding Attesttime,RAG-SequenceandRAG-Tokenrequiredifferentwaystoapproximateargmax p(y x). y | RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: p (y x,y ) = p (z x)p (y x,z ,y ) To 0✓ i | 1:i   1 z 2 top-k(p( ·| x)) ⌘ i | ✓ i | i 1:i   1 decode,wecanplugp (y x,y )intoastandardbeamdecoder. 0✓ i | 1:i   1 P RAG-Sequence ForRAG-Sequence,thelikelihoodp(y x)doesnotbreakintoaconventionalper- "
542,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"likelihoodp(y x)doesnotbreakintoaconventionalper- | tokenlikelihood,hencewecannotsolveitwithasinglebeamsearch. Instead,werunbeamsearchfor eachdocumentz,scoringeachhypothesisusingp (y x,z,y ). Thisyieldsasetofhypotheses ✓ i 1:i 1 Y,someofwhichmaynothaveappearedinthebeams | ofalldoc uments. Toestimatetheprobability of an hypothesis y we run an additional forward pass for each document z for which y does not appearinthebeam,multiplygeneratorprobabilitywithp (z x)andthensumtheprobabilitiesacross ⌘ |"
543,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"itywithp (z x)andthensumtheprobabilitiesacross ⌘ | beamsforthemarginals. Werefertothisdecodingprocedureas“ThoroughDecoding.” Forlonger outputsequences, Y canbecomelarge,requiringmanyforwardpasses. Formoreefficientdecoding, | | wecanmakeafurtherapproximationthatp (y x,z ) 0whereywasnotgeneratedduringbeam ✓ i | ⇡ searchfromx,z . ThisavoidstheneedtorunadditionalforwardpassesoncethecandidatesetY has i beengenerated. Werefertothisdecodingprocedureas“FastDecoding.” 3 Experiments WeexperimentwithRAGina"
544,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"astDecoding.” 3 Experiments WeexperimentwithRAGinawiderangeofknowledge-intensivetasks. Forallexperiments,weuse asingleWikipediadumpforournon-parametricknowledgesource. FollowingLeeetal.[31]and Karpukhinetal.[26],weusetheDecember2018dump. EachWikipediaarticleissplitintodisjoint 100-wordchunks,tomakeatotalof21Mdocuments. Weusethedocumentencodertocomputean embeddingforeachdocument,andbuildasingleMIPSindexusingFAISS[23]withaHierarchical NavigableSmallWorldapproximationforfastretrieval[37]. Duringtra"
545,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"lWorldapproximationforfastretrieval[37]. Duringtraining,weretrievethetop kdocumentsforeachquery. Weconsiderk 5,10 fortrainingandsetkfortesttimeusingdev 2{ } data. Wenowdiscussexperimentaldetailsforeachtask. 3.1 Open-domainQuestionAnswering Open-domainquestionanswering(QA)isanimportantreal-worldapplicationandcommontestbed forknowledge-intensivetasks[20]. Wetreatquestionsandanswersasinput-outputtextpairs(x,y) andtrainRAGbydirectlyminimizingthenegativelog-likelihoodofanswers. WecompareRAGto thepopu"
546,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ivelog-likelihoodofanswers. WecompareRAGto thepopularextractiveQAparadigm[5,7,31,26],whereanswersareextractedspansfromretrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA”approaches[52],which,likeRAG,generateanswers,butwhichdonotexploitretrieval,instead relyingpurelyonparametricknowledge.Weconsiderfourpopularopen-domainQAdatasets:Natural Questions(NQ)[29],TriviaQA(TQA)[24]. WebQuestions(WQ)[3]andCuratedTrec(CT)[2]. As CTandWQaresmall,wefollowDPR["
547,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"uratedTrec(CT)[2]. As CTandWQaresmall,wefollowDPR[26]byinitializingCTandWQmodelswithourNQRAG model. Weusethesametrain/dev/testsplitsaspriorwork[31,26]andreportExactMatch(EM) scores. ForTQA,tocomparewithT5[52],wealsoevaluateontheTQAWikitestset. 3.2 AbstractiveQuestionAnswering RAGmodelscangobeyondsimpleextractiveQAandanswerquestionswithfree-form,abstractive textgeneration. TotestRAG’snaturallanguagegeneration(NLG)inaknowledge-intensivesetting, we use the MSMARCO NLG task v2.1 [43]. The task consi"
548,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrievedpassages. Wedonotusethesuppliedpassages,onlythequestionsandanswers,totreat 4
MSMARCOasanopen-domainabstractiveQAtask. MSMARCOhassomequestionsthatcannotbe answeredinawaythatmatchesthereferenceanswerwithoutaccesstothegoldpassages,suchas “WhatistheweatherinVolcano,CA?”soperformancewillbelowerwithoutusinggoldpas"
549,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"no,CA?”soperformancewillbelowerwithoutusinggoldpassages. WealsonotethatsomeMSMARCOquestionscannotbeansweredusingWikipediaalone. Here, RAGcanrelyonparametricknowledgetogeneratereasonableresponses. 3.3 JeopardyQuestionGeneration ToevaluateRAG’sgenerationabilitiesinanon-QAsetting,westudyopen-domainquestiongen- eration. Ratherthanusequestionsfromstandardopen-domainQAtasks,whichtypicallyconsist ofshort,simplequestions,weproposethemoredemandingtaskofgeneratingJeopardyquestions. Jeopardyisanunusualform"
550,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"neratingJeopardyquestions. Jeopardyisanunusualformatthatconsistsoftryingtoguessanentityfromafactaboutthatentity. Forexample,“TheWorldCup”istheanswertothequestion“In1986Mexicoscoredasthefirst country to host this international sports competition twice.” As Jeopardy questions are precise, factualstatements,generatingJeopardyquestionsconditionedontheiranswerentitiesconstitutesa challengingknowledge-intensivegenerationtask. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test"
551,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"rchQA [10], with 100K train, 14K dev, and 27K test examples. As thisisanewtask,wetrainaBARTmodelforcomparison. Following[67],weevaluateusingthe SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standardmetrics. Wealsoperformtwohumanevaluations,onetoassessgenerationfactuality,and oneforspecificity.Wedefinefactualityaswhetherastatementcanbecorroboratedbytrustedexternal"
552,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"hetherastatementcanbecorroboratedbytrustedexternal sources,andspecificityashighmutualdependencebetweentheinputandoutput[33]. Wefollow bestpracticeandusepairwisecomparativeevaluation[34]. Evaluatorsareshownananswerandtwo generatedquestions,onefromBARTandonefromRAG.Theyarethenaskedtopickoneoffour options—quuestionAisbetter,questionBisbetter,botharegood,orneitherisgood. 3.4 FactVerification FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whe"
553,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whethertheclaimistrue,false,orunverifiablefromWikipediaalone. FEVERisaretrievalproblem coupledwithanchallengingentailmentreasoningtask. Italsoprovidesanappropriatetestbedfor exploringtheRAGmodels’abilitytohandleclassificationratherthangeneration. WemapFEVER classlabels(support"
554,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"therthangeneration. WemapFEVER classlabels(supports, refutes, ornotenoughinfo)tosingleoutputtokensanddirectlytrainwith claim-classpairs. Crucially,unlikemostotherapproachestoFEVER,wedonotusesupervisionon retrievedevidence. Inmanyreal-worldapplications,retrievalsupervisionsignalsaren’tavailable,and modelsthatdonotrequiresuchsupervisionwillbeapplicabletoawiderrangeoftasks. Weexplore twovariants: thestandard3-wayclassificationtask(supports/refutes/notenoughinfo)andthe2-way (supports/refutes)taskstu"
555,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"otenoughinfo)andthe2-way (supports/refutes)taskstudiedinThorneandVlachos[57]. Inbothcaseswereportlabelaccuracy. 4 Results 4.1 Open-domainQuestionAnswering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks,RAGsetsanewstateoftheart(onlyontheT5-comparablesplitforTQA).RAGcombines thegenerationflexibilityofthe“closed-book”(parametriconly)approachesandtheperformanceof ""open-book""retrieval-basedapproaches. UnlikeREALMandT5+SSM,RAGenjoysstrongresults with"
556,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,". UnlikeREALMandT5+SSM,RAGenjoysstrongresults withoutexpensive,specialized“salientspanmasking”pre-training[20]. ItisworthnotingthatRAG’s retrieverisinitializedusingDPR’sretriever,whichusesretrievalsupervisiononNaturalQuestions andTriviaQA.RAGcomparesfavourablytotheDPRQAsystem,whichusesaBERT-based“cross- encoder”tore-rankdocuments,alongwithanextractivereader. RAGdemonstratesthatneithera re-rankernorextractivereaderisnecessaryforstate-of-the-artperformance. Thereareseveraladvantagestogeneratingans"
557,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"formance. Thereareseveraladvantagestogeneratinganswersevenwhenitispossibletoextractthem. Docu- mentswithcluesabouttheanswerbutdonotcontaintheanswerverbatimcanstillcontributetowards acorrectanswerbeinggenerated,whichisnotpossiblewithstandardextractiveapproaches,leading 5
Table1:Open-DomainQATestScores.ForTQA, Table2:GenerationandclassificationTestScores. left column uses the standard test set for Open- MS-MARCOSotAis[4],FEVER-3is[68]and Domain QA, right column uses the TQA-Wiki FEVER-2 is [57] *U"
558,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf, right column uses the TQA-Wiki FEVER-2 is [57] *Uses gold context/evidence. testset. SeeAppendixDforfurtherdetails. Bestmodelwithoutgoldaccessunderlined. Model NQ TQA WQ CT Model Jeopardy MSMARCO FVR3 FVR2 Closed T5-11B[52] 34.5 - /50.1 37.4 - B-1 QB-1 R-L B-1 LabelAcc. Book T5-11B+SSM[52] 36.6 - /60.5 44.7 - SotA - - 49.8* 49.9* 76.8 92.2* Open REALM[20] 40.4 - / - 40.7 46.8 Book DPR[26] 41.5 57.9/ - 41.1 50.6 BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Tok. 17.3 
559,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf," RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5 RAG-Seq. 44.5 56.8/68.0 45.2 52.2 RAG-Seq. 14.7 21.4 40.8 44.2 tomoreeffectivemarginalizationoverdocuments. Furthermore,RAGcangeneratecorrectanswers evenwhenthecorrectanswerisnotinanyretrieveddocument,achieving11.8%accuracyinsuch casesforNQ,whereanextractivemodelwouldscore0%. 4.2 AbstractiveQuestionAnswering AsshowninTable2,RAG-SequenceoutperformsBARTonOpenMS-MARCONLGby2.6Bleu points and 2.6 Rouge-L points. RAG approaches"
560,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressivegiventhat(i)thosemodelsaccessgoldpassageswithspecificinformationrequiredto generatethereferenceanswer,(ii)manyquestionsareunanswerablewithoutthegoldpassages,and (iii)notallquestionsareanswerablefromWikipediaalone. Table3showssomegeneratedanswers fromourmodels. Qualitatively,wefindthatRAGmodelshallucinatelessandgeneratefactually correcttextmoreoftenthanBART.Later,wealsoshowthatRAGgenerationsa"
561,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"eoftenthanBART.Later,wealsoshowthatRAGgenerationsaremorediversethan BARTgenerations(see§4.5). 4.3 JeopardyQuestionGeneration Table2showsthatRAG-TokenperformsbetterthanRAG-SequenceonJeopardyquestiongeneration, withbothmodelsoutperformingBARTonQ-BLEU-1. Table4showshumanevaluationresults,over 452pairsofgenerationsfromBARTandRAG-Token. EvaluatorsindicatedthatBARTwasmore factualthanRAGinonly7.1%ofcases,whileRAGwasmorefactualin42.7%ofcases,andboth RAGandBARTwerefactualinafurther17%ofcases,clearlydemon"
562,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ndBARTwerefactualinafurther17%ofcases,clearlydemonstratingtheeffectivenessof RAGonthetaskoverastate-of-the-artgenerationmodel. EvaluatorsalsofindRAGgenerationsto bemorespecificbyalargemargin. Table3showstypicalgenerationsfromeachmodel. Jeopardyquestionsoftencontaintwoseparatepiecesofinformation,andRAG-Tokenmayperform bestbecauseitcangenerateresponsesthatcombinecontentfromseveraldocuments. Figure2shows anexample. Whengenerating“Sun”, theposteriorishighfordocument2whichmentions“The Sun Also Rises”"
563,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"shighfordocument2whichmentions“The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly,afterthefirsttokenofeachbookisgenerated,thedocumentposteriorflattens. Thisobservationsuggeststhatthegeneratorcancompletethetitleswithoutdependingonspecific documents. Inotherwords,themodel’sparametricknowledgeissufficienttocompletethetitles. We findevidenceforthishypothesisbyfeedingtheBART-onlybaselinewiththepartialdecoding""The Sun. BARTcompletest"
564,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"linewiththepartialdecoding""The Sun. BARTcompletesthegeneration""TheSunAlsoRises""isanovelbythisauthorof""TheSun AlsoRises""indicatingthetitle""TheSunAlsoRises""isstoredinBART’sparameters. Similarly, BART will complete the partial decoding ""TheSunAlsoRises""isanovelbythisauthorof""A with""TheSunAlsoRises""isanovelbythisauthorof""AFarewelltoArms"". Thisexampleshows howparametricandnon-parametricmemoriesworktogether—thenon-parametriccomponenthelps toguidethegeneration,drawingoutspecificknowledgestoredinthepara"
565,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"eration,drawingoutspecificknowledgestoredintheparametricmemory. 4.4 FactVerification Table 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-artmodels,whicharecomplexpipelinesystemswithdomain-specificarchitecturesand substantialengineering,trainedusingintermediateretrievalsupervision,whichRAGdoesnotrequire. 6
Document1:hisworksareconsideredclassicsofAmerican Doc1 literature...Hiswartimeexperiencesformedthebasisforhisnovel Doc2 ”AFarewelltoArms”(19"
566,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ormedthebasisforhisnovel Doc2 ”AFarewelltoArms”(1929)... Doc3 Document2:...artistsofthe1920s”LostGeneration”expatriate Doc4 community.Hisdebutnovel,”TheSunAlsoRises”,waspublished in1926. Doc5 B OS” The Sun AlsoR ises”isa novelby this authorof”A Fare wellto Ar ms” Figure2: RAG-Tokendocumentposteriorp(z x,y ,y )foreachgeneratedtokenforinput“Hem- i i i ingway""forJeopardygenerationwith5retriev | eddocu ments. Theposteriorfordocument1ishigh whengenerating“AFarewelltoArms""andfordocument2whengenerating"
567,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ting“AFarewelltoArms""andfordocument2whengenerating“TheSunAlsoRises"". Table3: Examplesfromgenerationtasks. RAGmodelsgeneratemorespecificandfactuallyaccurate responses. ‘?’ indicatesfactuallyincorrectresponses,*indicatespartiallycorrectresponses. Task Input Model Generation BART ?Themiddleearisthepartoftheearbetweenthemiddleearandthenose. definemiddle RAG-T Themiddleearistheportionoftheearinternaltotheeardrum. ear MS- RAG-S Themiddleearincludesthetympaniccavityandthethreeossicles. MARCO whatcurren"
568,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ympaniccavityandthethreeossicles. MARCO whatcurrency BART ThecurrencyneededinScotlandisPoundsterling. neededin RAG-T PoundisthecurrencyneededinScotland. scotland RAG-S ThecurrencyneededinScotlandisthepoundsterling. BART ?ThisstatehasthelargestnumberofcountiesintheU.S. Jeopardy Washington RAG-T It’stheonlyU.S.statenamedforaU.S.president Question RAG-S It’sthestatewhereyou’llfindMountRainierNationalPark Gener BART *ThisepicpoembyDanteisdividedinto3parts:theInferno,thePurgatorio&thePurgatorio -atio"
569,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"parts:theInferno,thePurgatorio&thePurgatorio -ation TheDivine RAG-T Dante’s""Inferno""isthefirstpartofthisepicpoem Comedy RAG-S This14thcenturyworkisdividedinto3sections:""Inferno"",""Purgatorio""&""Paradiso"" For2-wayclassification,wecompareagainstThorneandVlachos[57],whotrainRoBERTa[35] toclassifytheclaimastrueorfalsegiventhegoldevidencesentence. RAGachievesanaccuracy within2.7%ofthismodel,despitebeingsuppliedwithonlytheclaimandretrievingitsownevidence. WealsoanalyzewhetherdocumentsretrievedbyRAGcorre"
570,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,". WealsoanalyzewhetherdocumentsretrievedbyRAGcorrespondtodocumentsannotatedasgold evidenceinFEVER.Wecalculatetheoverlapinarticletitlesbetweenthetopkdocumentsretrieved byRAGandgoldevidenceannotations. Wefindthatthetopretrieveddocumentisfromagoldarticle in71%ofcases,andagoldarticleispresentinthetop10retrievedarticlesin90%ofcases. 4.5 AdditionalResults Generation Diversity Section 4.3 shows that RAG models are more factual and specific than BARTforJeopardyquestiongeneration. Followingrecentworkondi"
571,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"eopardyquestiongeneration. Followingrecentworkondiversity-promotingdecoding [33,59,39],wealsoinvestigategenerationdiversitybycalculatingtheratioofdistinctngramsto totalngramsgeneratedbydifferentmodels. Table5showsthatRAG-Sequence’sgenerationsare morediversethanRAG-Token’s,andbotharesignificantlymorediversethanBARTwithoutneeding anydiversity-promotingdecoding. RetrievalAblations AkeyfeatureofRAGislearningtoretrieverelevantinformationforthetask. Toassesstheeffectivenessoftheretrievalmechanism,weru"
572,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"assesstheeffectivenessoftheretrievalmechanism,werunablationswherewefreezetheretriever duringtraining. AsshowninTable6,learnedretrievalimprovesresultsforalltasks. Wecompare RAG’sdenseretrievertoawordoverlap-basedBM25retriever[53].Here,wereplaceRAG’sretriever withafixedBM25system,anduseBM25retrievalscoresaslogitswhencalculatingp(z x). Table | 6 show the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centricandthuswell-suitedforwordoverlap-basedretrieval. Diff"
573,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"thuswell-suitedforwordoverlap-basedretrieval. Differentiableretrievalimproves resultsonallothertasks,especiallyforOpen-DomainQA,whereitiscrucial. Indexhot-swapping Anadvantageofnon-parametricmemorymodelslikeRAGisthatknowledge canbeeasilyupdatedattesttime. Parametric-onlymodelslikeT5orBARTneedfurthertrainingto updatetheirbehaviorastheworldchanges. Todemonstrate,webuildanindexusingtheDrQA[5] WikipediadumpfromDecember2016andcompareoutputsfromRAGusingthisindextothenewer indexfromourmainresults(Decem"
574,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"gthisindextothenewer indexfromourmainresults(December2018). Wepreparealistof82worldleaderswhohadchanged betweenthesedatesanduseatemplate“Whois{position}?” (e.g. “WhoisthePresidentofPeru?”) 7
Table 4: Human assessments for the Jeopardy Table 5: Ratio of distinct to total tri-grams for QuestionGenerationTask. generationtasks. Factuality Specificity MSMARCO JeopardyQGen BARTbetter 7.1% 16.8% Gold 89.6% 90.0% RAGbetter 42.7% 37.4% BART 70.7% 32.4% Bothgood 11.7% 11.8% RAG-Token 77.8% 46.8% Bothpoor "
575,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"thgood 11.7% 11.8% RAG-Token 77.8% 46.8% Bothpoor 17.7% 6.9% RAG-Seq. 83.5% 53.8% Nomajority 20.8% 20.1% Table6:Ablationsonthedevset.AsFEVERisaclassificationtask,bothRAGmodelsareequivalent. Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2 ExactMatch B-1 QB-1 R-L B-1 LabelAccuracy RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6 RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9 RAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4 RAG-Sequence-Frozen 41.2 "
576,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"21.7 55.9 49.4 72.9 89.4 RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3 RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6 RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5 toqueryourNQRAGmodelwitheachindex. RAGanswers70%correctlyusingthe2016indexfor 2016worldleadersand68%usingthe2018indexfor2018worldleaders. Accuracywithmismatched indicesislow(12%withthe2018indexand2016leaders,4%withthe2016indexand2018leaders). ThisshowswecanupdateRAG’sworldknowledgebysimplyreplacingits"
577,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"wecanupdateRAG’sworldknowledgebysimplyreplacingitsnon-parametricmemory. Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents,andwedonotobservesignificantdifferencesinperformancebetweenthem. Wehavethe flexibilitytoadjustthenumberofretrieveddocumentsattesttime,whichcanaffectperformanceand runtime. Figure3(left)showsthatretrievingmoredocumentsattesttimemonotonicallyimproves Open-domainQAresultsforRAG-Sequence,butperformancepeaksforRAG-Tokenat10retrie"
578,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"Sequence,butperformancepeaksforRAG-Tokenat10retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-TokenattheexpenseofBleu-1,buttheeffectislesspronouncedforRAG-Sequence. 44 43 42 41 40 39 10 20 30 40 50 KRetrievedDocs hctaMtcaxEQN 80 70 60 50 RAG-Tok RAG-Seq 40 10 20 30 40 50 KRetrievedDocs K@llaceRrewsnAQN 56 54 52 RAG-Tok RAG-Seq 50 FixedDPR BM25 48 10 20 30 40 50 KRetrievedDocs erocsL-eguoR/1-uelB RAG-TokR-L RAG-TokB-1 RAG-SeqR-L RAG-SeqB-1 "
579,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"-uelB RAG-TokR-L RAG-TokB-1 RAG-SeqR-L RAG-SeqB-1 Figure3: Left: NQperformanceasmoredocumentsareretrieved. Center: Retrievalrecallperfor- manceinNQ.Right: MS-MARCOBleu-1andRouge-Lasmoredocumentsareretrieved. 5 RelatedWork Single-TaskRetrieval Priorworkhasshownthatretrievalimprovesperformanceacrossavarietyof NLPtaskswhenconsideredinisolation. Suchtasksincludeopen-domainquestionanswering[5,29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generatio"
580,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"estion answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our workunifiesprevioussuccessesinincorporatingretrievalintoindividualtasks,showingthatasingle retrieval-basedarchitectureiscapableofachievingstrongperformanceacrossseveraltasks. 8
General-PurposeArchitecturesforNLP Priorworkongeneral-purposearchitecturesforNLP taskshasshowngreatsuccesswithouttheuseofretrieval. Asingle, pre-trainedlanguagemodel hasbeenshowntoachie"
581,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ngle, pre-trainedlanguagemodel hasbeenshowntoachievestrongperformanceonvariousclassificationtasksintheGLUEbench- marks[60,61]afterfine-tuning[49,8].GPT-2[50]latershowedthatasingle,left-to-right,pre-trained languagemodelcouldachievestrongperformanceacrossbothdiscriminativeandgenerativetasks. Forfurtherimprovement,BART[32]andT5[51,52]proposeasingle,pre-trainedencoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative andgenerativetasks. Ourworka"
582,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"nce on discriminative andgenerativetasks. Ourworkaimstoexpandthespaceofpossibletaskswithasingle, unified architecture,bylearningaretrievalmoduletoaugmentpre-trained,generativelanguagemodels. Learned Retrieval There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some workoptimizestheretrievalmoduletoaidinaspecific,downstreamtasksuchasquestionanswering, usingsearch[46],reinforcementlea"
583,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"uestionanswering, usingsearch[46],reinforcementlearning[6,63,62],oralatentvariableapproach[31,20]asinour work. Thesesuccessesleveragedifferentretrieval-basedarchitecturesandoptimizationtechniquesto achievestrongperformanceonasingletask,whileweshowthatasingleretrieval-basedarchitecture canbefine-tunedforstrongperformanceonavarietyoftasks. Memory-basedArchitectures Ourdocumentindexcanbeseenasalargeexternalmemoryfor neuralnetworkstoattendto,analogoustomemorynetworks[64,55]. Concurrentwork[14]learns"
584,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"stomemorynetworks[64,55]. Concurrentwork[14]learns toretrieveatrainedembeddingforeachentityintheinput,ratherthantoretrieverawtextasinour work. Otherworkimprovestheabilityofdialogmodelstogeneratefactualtextbyattendingover factembeddings[9,13]or,closertoourwork,overretrievedtextdirectly[15]. Akeyfeatureofour memoryisthatitiscomprisedofrawtextratherdistributedrepresentations,whichmakesthememory both(i)human-readable,lendingaformofinterpretabilitytoourmodel,and(ii)human-writable, enablingustodynamic"
585,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"urmodel,and(ii)human-writable, enablingustodynamicallyupdatethemodel’smemorybyeditingthedocumentindex. Retrieve-and-Editapproaches Ourmethodsharessomesimilaritieswithretrieve-and-editstyle approaches,whereasimilartraininginput-outputpairisretrievedforagiveninput,andthenedited toprovideafinaloutput. Theseapproacheshaveprovedsuccessfulinanumberofdomainsincluding MachineTranslation [18,22]andSemanticParsing[21].Ourapproachdoeshaveseveraldifferences, includinglessofemphasisonlightlyeditingaretrieved"
586,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf," includinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral piecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments ratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and couldrepresentpromisingfuturework. 6 Discussion Inthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric memory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We foundthatpeoplepreferRAG’sge"
587,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ltsonopen-domainQA.We foundthatpeoplepreferRAG’sgenerationoverpurelyparametricBART,findingRAGmorefactual andspecific. Weconductedanthoroughinvestigationofthelearnedretrievalcomponent,validating itseffectiveness,andweillustratedhowtheretrievalindexcanbehot-swappedtoupdatethemodel withoutrequiringanyretraining.Infuturework,itmaybefruitfultoinvestigateifthetwocomponents canbejointlypre-trainedfromscratch,eitherwithadenoisingobjectivesimilartoBARTorsome anotherobjective. Ourworkopensupnewresearchdir"
588,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ome anotherobjective. Ourworkopensupnewresearchdirectionsonhowparametricandnon-parametric memoriesinteractandhowtomosteffectivelycombinethem,showingpromiseinbeingappliedtoa widevarietyofNLPtasks. 9
BroaderImpact This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less withgenerationsthataremorefactual,andoffersmorecontrolandinterpretability. RAGcouldbe employed"
589,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"orecontrolandinterpretability. RAGcouldbe employedinawidevarietyofscenarioswithdirectbenefittosociety,forexamplebyendowingit withamedicalindexandaskingitopen-domainquestionsonthattopic,orbyhelpingpeoplebemore effectiveattheirjobs. Withtheseadvantagesalsocomepotentialdownsides:Wikipedia,oranypotentialexternalknowledge source,willprobablyneverbeentirelyfactualandcompletelydevoidofbias. SinceRAGcanbe employedasalanguagemodel,similarconcernsasforGPT-2[50]arevalidhere,althougharguably toalesserextent"
590,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"2[50]arevalidhere,althougharguably toalesserextent,includingthatitmightbeusedtogenerateabuse,fakedormisleadingcontentin thenewsoronsocialmedia;toimpersonateothers;ortoautomatetheproductionofspam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the comingdecades[16]. Inordertomitigatetheserisks,AIsystemscouldbeemployedtofightagainst misleadingcontentandautomatedspam/phishing. Acknowledgments Theauthorswouldliketothankthereviewersfortheirthoughtful"
591,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"horswouldliketothankthereviewersfortheirthoughtfulandconstructivefeedbackonthis paper,aswellasHuggingFacefortheirhelpinopen-sourcingcodetorunRAGmodels. Theauthors wouldalsoliketothankKyunghyunChoandSewonMinforproductivediscussionsandadvice. FundingDisclosure EPthankssupportsfromtheNSFGraduateResearchFellowship. PLissupportedbytheFAIRPhD program. ThisworkwasfundedbyFacebook. References [1] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan Majumder,AndrewMcNamara,BhaskarMi"
592,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"aodongLiu,Rangan Majumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http: //arxiv.org/abs/1611.09268. arXiv: 1611.09268. [2] PetrBaudišandJanŠedivy`. Modelingofthequestionansweringtaskintheyodaqasystem. In InternationalConferenceoftheCross-LanguageEvaluationForumforEuropeanLanguages, pages222–228.Springer,2015. URLhttps://link."
593,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ages, pages222–228.Springer,2015. URLhttps://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20. [3] JonathanBerant,AndrewChou,RoyFrostig,andPercyLiang. SemanticParsingonFreebase fromQuestion-AnswerPairs. InProceedingsofthe2013ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages1533–1544,Seattle,Washington,USA,October2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ D13-1160. [4] BinBi,ChenliangLi,ChenWu,MingYan,andWeiWang. Palm: Pre-traininga"
594,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"gLi,ChenWu,MingYan,andWeiWang. Palm: Pre-traininganautoencod- ing&autoregressivelanguagemodelforcontext-conditionedgeneration. ArXiv,abs/2004.07159, 2020. URLhttps://arxiv.org/abs/2004.07159. [5] DanqiChen,AdamFisch,JasonWeston,andAntoineBordes. ReadingWikipediatoAnswer Open-DomainQuestions. InProceedingsofthe55thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages1870–1879,Vancouver,Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P1"
595,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171. [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and JonathanBerant. Coarse-to-finequestionansweringforlongdocuments. InProceedingsofthe 10
55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers), pages209–220,Vancouver,Canada,July2017.AssociationforComputationalLinguistics. doi: 10.18653/v1/P17-1020. URLhttps://www.aclweb.org/ant"
596,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"0.18653/v1/P17-1020. URLhttps://www.aclweb.org/anthology/P17-1020. [7] ChristopherClarkandMattGardner. SimpleandEffectiveMulti-ParagraphReadingCompre- hension. arXiv:1710.10723[cs],October2017. URLhttp://arxiv.org/abs/1710.10723. arXiv: 1710.10723. [8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof DeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofthe2019Con- ferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human Langu"
597,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"AssociationforComputationalLinguistics:Human LanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis, Minnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423. URLhttps://www.aclweb.org/anthology/N19-1423. [9] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston. Wiz- ardofwikipedia: Knowledge-poweredconversationalagents. InInternationalConferenceon LearningRepresentations,2019. URLhttps://openreview.net/forum?id=r1l73iRqKm. [1"
598,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf," URLhttps://openreview.net/forum?id=r1l73iRqKm. [10] MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179. [11] AngelaFan,MikeLewis,andYannDauphin. Hierarchicalneuralstorygeneration. InProceed- ingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages889–898,Melbourne,Aust"
599,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"s(Volume1: LongPapers),pages889–898,Melbourne,Australia,July2018.AssociationforComputational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/ P18-1082. [12] AngelaFan,YacineJernite,EthanPerez,DavidGrangier,JasonWeston,andMichaelAuli.ELI5: Longformquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation forComputationalLinguistics,pages3558–3567,Florence,Italy,July2019.Associationfor ComputationalLinguistics. doi: 10.18653/v1/P19-1346. URLhttps://www.acl"
600,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ics. doi: 10.18653/v1/P19-1346. URLhttps://www.aclweb.org/ anthology/P19-1346. [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers withKNN-basedcompositememory, 2020. URLhttps://openreview.net/forum?id= H1gx1CNKPH. [14] ThibaultFévry,LivioBaldiniSoares,NicholasFitzGerald,EunsolChoi,andTomKwiatkowski. Entitiesasexperts: Sparsememoryaccesswithentitysupervision. ArXiv, abs/2004.07202, 2020. URLhttps://arxiv.org/abs/2004.07202. [15] Marjan Ghazvininejad, Chris B"
601,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"abs/2004.07202. [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI ConferenceonArtificialIntelligence,2018.URLhttps://www.aaai.org/ocs/index.php/ AAAI/AAAI18/paper/view/16710. [16] KatjaGrace,JohnSalvatier,AllanDafoe,BaobaoZhang,andOwainEvans. WhenwillAI exceedhumanperformance? evidencefromAIexperts. CoRR,abs/1705.08807,2017. URL http://arxiv.org/abs/1705.08807. [17] Jiatao G"
602,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"URL http://arxiv.org/abs/1705.08807. [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https: //www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282. [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machinetranslation. In 32ndAAAIConferenceonArtificialIntelligence, AAAI 2018, 32nd AAAIConferenceonArtificialIntelligence,AAAI2018,pages513"
603,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"nferenceonArtificialIntelligence,AAAI2018,pages5133–5140.AAAIpress,2018. 32ndAAAIConferenceonArtificialIntelligence,AAAI2018;Conferencedate: 02-02-2018 Through07-02-2018. 11
[19] KelvinGuu,TatsunoriB.Hashimoto,YonatanOren,andPercyLiang. Generatingsentencesby editingprototypes. TransactionsoftheAssociationforComputationalLinguistics,6:437–450, 2018. doi: 10.1162/tacl_a_00030. URLhttps://www.aclweb.org/anthology/Q18-1031. [20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang"
604,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"e, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmentedlanguagemodelpre-training. ArXiv,abs/2002.08909,2020. URLhttps: //arxiv.org/abs/2002.08909. [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed- itors, Advances in Neural Information Processing Systems 31, pages 10052– 10062. Curran Associates, In"
605,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"tems 31, pages 10052– 10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs. pdf. [22] NabilHossain,MarjanGhazvininejad,andLukeZettlemoyer. Simpleandeffectiveretrieve- edit-reranktextgeneration. InProceedingsofthe58thAnnualMeetingoftheAssociationfor ComputationalLinguistics,pages2532–2538,Online,July2020.AssociationforComputa- tionalLinguistics. doi: 10.18653/v1/2020.acl-main.228. URLhttps://www.aclweb.org/ ant"
606,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"/2020.acl-main.228. URLhttps://www.aclweb.org/ anthology/2020.acl-main.228. [23] JeffJohnson,MatthijsDouze,andHervéJégou. Billion-scalesimilaritysearchwithgpus. arXiv preprintarXiv:1702.08734,2017. URLhttps://arxiv.org/abs/1702.08734. [24] MandarJoshi,EunsolChoi,DanielWeld,andLukeZettlemoyer. TriviaQA:ALargeScale DistantlySupervisedChallengeDatasetforReadingComprehension. InProceedingsofthe 55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers), pages1601–1611,Vancouver"
607,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"tics(Volume1:LongPapers), pages1601–1611,Vancouver,Canada,July2017.AssociationforComputationalLinguistics. doi: 10.18653/v1/P17-1147. URLhttps://www.aclweb.org/anthology/P17-1147. [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack- augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam- bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/ 5857-inferri"
608,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ss. URL https://papers.nips.cc/paper/ 5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets. [26] VladimirKarpukhin,BarlasOguz,SewonMin,LedellWu,SergeyEdunov,DanqiChen,and Wen-tauYih. Densepassageretrievalforopen-domainquestionanswering. arXivpreprint arXiv:2004.04906,2020. URLhttps://arxiv.org/abs/2004.04906. [27] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.Generaliza- tionthroughmemorization: Nearestneighborlanguagemodels. InInternationalConferenceon L"
609,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ghborlanguagemodels. InInternationalConferenceon LearningRepresentations,2020. URLhttps://openreview.net/forum?id=HklBjCEKvH. [28] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua BengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations, ICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,2015. URL http://arxiv.org/abs/1412.6980. [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, "
610,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken- ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques- tion Answering Research. Transactions of the Association of Computational Lin- guistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/ natural-questions/main-1455-kwiatkowski.pdf. [30] GuillaumeLample,Ale"
611,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ain-1455-kwiatkowski.pdf. [30] GuillaumeLample,AlexandreSablayrolles,Marc’AurelioRanzato,LudovicDenoyer,and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In- formationProcessingSystems32,pages8548–8559.CurranAssociates,Inc.,2019. URLhttp: //papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf. 12
[31] KentonLee,Ming-WeiChang,andKristinaToutanova. Latentretrieval"
612,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ing-WeiChang,andKristinaToutanova. Latentretrievalforweaklysupervised opendomainquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation forComputationalLinguistics,pages6086–6096,Florence,Italy,July2019.Associationfor ComputationalLinguistics. doi: 10.18653/v1/P19-1612. URLhttps://www.aclweb.org/ anthology/P19-1612. [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, OmerLevy,VeselinStoyanov,andLukeZettlemoyer. BART:Denoisingsequence-to-sequence pr"
613,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"Zettlemoyer. BART:Denoisingsequence-to-sequence pre-trainingfornaturallanguagegeneration,translation,andcomprehension. arXivpreprint arXiv:1910.13461,2019. URLhttps://arxiv.org/abs/1910.13461. [33] JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan. Adiversity-promoting objectivefunctionforneuralconversationmodels. InProceedingsofthe2016Conferenceofthe NorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage Technologies,pages110–119,SanDiego,California,June2016.As"
614,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ogies,pages110–119,SanDiego,California,June2016.AssociationforComputational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/ N16-1014. [34] MargaretLi, JasonWeston, andStephenRoller. Acute-eval: Improveddialogueevaluation with optimizedquestions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087. [35] HairongLiu,MingboMa,LiangHuang,HaoXiong,andZhongjunHe. Robustneuralmachine translation with joint textual and phonetic embedd"
615,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual MeetingoftheAssociationforComputationalLinguistics,pages3044–3049,Florence,Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291. [36] PeterJ.Liu*,MohammadSaleh*,EtiennePot,BenGoodrich,RyanSepassi,LukaszKaiser, andNoamShazeer. Generatingwikipediabysummarizinglongsequences. InInternational ConferenceonLearningRepresentations,2018."
616,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"national ConferenceonLearningRepresentations,2018. URLhttps://openreview.net/forum? id=Hyg0vbWC-. [37] YuryA.MalkovandD.A.Yashunin. Efficientandrobustapproximatenearestneighborsearch usinghierarchicalnavigablesmallworldgraphs. IEEETransactionsonPatternAnalysisand MachineIntelligence,42:824–836,2016. URLhttps://arxiv.org/abs/1603.09320. [38] GaryMarcus. Thenextdecadeinai: fourstepstowardsrobustartificialintelligence. arXiv preprintarXiv:2002.06177,2020. URLhttps://arxiv.org/abs/2002.06177. [39] L"
617,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,",2020. URLhttps://arxiv.org/abs/2002.06177. [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https: //arxiv.org/abs/1911.03587. [40] PauliusMicikevicius,SharanNarang,JonahAlben,GregoryDiamos,ErichElsen,DavidGarcia, BorisGinsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh,andHaoWu. Mixed "
618,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"n,OleksiiKuchaiev,GaneshVenkatesh,andHaoWu. Mixed precisiontraining. InICLR,2018. URLhttps://openreview.net/forum?id=r1gs9JgRZ. [41] NikitaMoghe,SiddharthaArora,SumanBanerjee,andMiteshM.Khapra. Towardsexploit- ing background knowledge for building conversation systems. In Proceedings of the 2018 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2322–2332,Brus- sels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URLhttps://www.acl"
619,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ics. doi: 10.18653/v1/D18-1255. URLhttps://www.aclweb.org/anthology/D18-1255. [42] PrekshaNemaandMiteshM.Khapra.Towardsabettermetricforevaluatingquestiongeneration systems. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage Processing,pages3950–3959,Brussels,Belgium,October-November2018.Associationfor ComputationalLinguistics. doi: 10.18653/v1/D18-1429. URLhttps://www.aclweb.org/ anthology/D18-1429. 13
[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Ranga"
620,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"erg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, andLiDeng. MSMARCO:Ahumangeneratedmachinereadingcomprehensiondataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches2016co-locatedwiththe30thAnnualConferenceonNeuralInformationProcessing Systems(NIPS2016),Barcelona,Spain,December9,2016,volume1773ofCEURWorkshop Proceedings. CEUR-WS.org, 2016. URL "
621,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"fCEURWorkshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf. [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085,2019. URLhttps://arxiv.org/abs/1901.04085. [45] MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,SamGross,NathanNg,DavidGrangier, andMichaelAuli. fairseq: Afast,extensibletoolkitforsequencemodeling. InProceedings ofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational Lingui"
622,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"icanChapteroftheAssociationforComputational Linguistics(Demonstrations),pages48–53,Minneapolis,Minnesota,June2019.Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb. org/anthology/N19-4009. [46] EthanPerez,SiddharthKaramcheti,RobFergus,JasonWeston,DouweKiela,andKyunghyun Cho. Findinggeneralizableevidencebylearningtoconvinceq&amodels. InProceedings ofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th International Joint Conference on "
623,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"essingandthe9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411,HongKong,China,November2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/D19-1244. URLhttps://www.aclweb.org/anthology/D19-1244. [47] FabioPetroni,TimRocktäschel,SebastianRiedel,PatrickLewis,AntonBakhtin,YuxiangWu, andAlexanderMiller. Languagemodelsasknowledgebases? InProceedingsofthe2019 ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternational JointConfe"
624,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"anguageProcessingandthe9thInternational JointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2463–2473,Hong Kong,China,November2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/ D19-1250. URLhttps://www.aclweb.org/anthology/D19-1250. [48] FabioPetroni,PatrickLewis,AleksandraPiktus,TimRocktäschel,YuxiangWu,AlexanderH. Miller,andSebastianRiedel. Howcontextaffectslanguagemodels’factualpredictions. In AutomatedKnowledgeBaseConstruction,2020. URLhttps://openreview.net/forum? id=0"
625,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"uction,2020. URLhttps://openreview.net/forum? id=025X0zPfn. [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im- proving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/ language-unsupervised/language_understanding_paper.pdf. [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.c"
626,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ltitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf. [51] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena, YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified text-to-texttransformer. arXive-prints,2019. URLhttps://arxiv.org/abs/1910.10683. [52] AdamRoberts, ColinRaffel, andNoamShazeer. Howmuchknowledgecanyoupackinto theparametersofalanguagemodel? ar"
627,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"gecanyoupackinto theparametersofalanguagemodel? arXive-prints,2020. URLhttps://arxiv.org/abs/ 2002.08910. [53] StephenRobertsonandHugoZaragoza. Theprobabilisticrelevanceframework: Bm25and beyond. Found.TrendsInf.Retr.,3(4):333–389,April2009. ISSN1554-0669. doi: 10.1561/ 1500000019. URLhttps://doi.org/10.1561/1500000019. [54] IreneSolaiman,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeffWu,Alec Radford,andJian-BingWang. Releasestrategiesandthesocialimpactsoflanguagemodels. ArXiv,abs/19"
628,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"sandthesocialimpactsoflanguagemodels. ArXiv,abs/1908.09203,2019. 14
[55] SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus. End-to-endmemorynet- works.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,Advances inNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,Inc.,2015. URLhttp://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf. [56] JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal. FEVER:a large-scaledatasetforfactextractiona"
629,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ttal. FEVER:a large-scaledatasetforfactextractionandVERification. InProceedingsofthe2018Conference of the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume1(LongPapers), pages809–819, NewOrleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074. [57] JamesH.ThorneandAndreasVlachos. Avoidingcatastrophicforgettinginmitigatingmodel biasesinsentence-pairc"
630,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"forgettinginmitigatingmodel biasesinsentence-pairclassificationwithelasticweightconsolidation. ArXiv,abs/2004.14366, 2020. URLhttps://arxiv.org/abs/2004.14366. [58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez, ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon,U.V.Luxburg, S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural InformationProcessingSystems30,pages5998–6008.CurranAssociates,Inc.,2017. URL http://papers.nips.cc"
631,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"ranAssociates,Inc.,2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. [59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David Crandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes. AAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17329. [60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman. GLUE: A multi-task benchmark and analysis platform"
632,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting NeuralNetworksforNLP,pages353–355,Brussels,Belgium,November2018.Associationfor ComputationalLinguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/ anthology/W18-5446. [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benc"
633,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"evy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General- Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F.d\textquotesingleAlché-Buc,E.Fox,andR.Garnett,editors,AdvancesinNeuralInformation Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https:// arxiv.org/abs/1905.00537. [62] ShuohangWang,MoYu,XiaoxiaoGuo,ZhiguoWang,TimKlinger,WeiZhang,ShiyuChang, 3 GerryTesauro,BowenZhou,andJingJiang. R : Reinforcedranker-readerforopen"
634,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"u,andJingJiang. R : Reinforcedranker-readerforopen-domain questionanswering. InSheilaA.McIlraithandKilianQ.Weinberger,editors,Proceedingsof theThirty-SecondAAAIConferenceonArtificialIntelligence,(AAAI-18),the30thinnovative ApplicationsofArtificialIntelligence(IAAI-18),andthe8thAAAISymposiumonEducational AdvancesinArtificialIntelligence(EAAI-18), NewOrleans, Louisiana, USA,February2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/16712. ["
635,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"org/ocs/index. php/AAAI/AAAI18/paper/view/16712. [63] ShuohangWang,MoYu,JingJiang,WeiZhang,XiaoxiaoGuo,ShiyuChang,ZhiguoWang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re- rankinginopen-domainquestionanswering. InICLR,2018. URLhttps://openreview. net/forum?id=rJl3yM-Ab. [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio andYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,ICLR 2015, San Diego, CA, USA"
636,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"rningRepresentations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916. [65] JasonWeston,EmilyDinan,andAlexanderMiller. Retrieveandrefine: Improvedsequence generationmodelsfordialogue. InProceedingsofthe2018EMNLPWorkshopSCAI:The2nd 15
InternationalWorkshoponSearch-OrientedConversationalAI,pages87–92,Brussels,Belgium, October2018.AssociationforComputationalLinguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/antholog"
637,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,"3/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713. [66] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony Moi, PierricCistac, TimRault, RémiLouf, MorganFuntowicz, JoeDavison, SamShleifer, PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,Sylvain Gugger,MariamaDrame,QuentinLhoest,andAlexanderM.Rush.Huggingface’stransformers: State-of-the-artnaturallanguageprocessing. ArXiv,abs/1910.03771,2019. [67] ShiyueZhangandMohitBansal. Addressingsemant"
638,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf,". [67] ShiyueZhangandMohitBansal. Addressingsemanticdriftinquestiongenerationforsemi- supervisedquestionanswering. InProceedingsofthe2019ConferenceonEmpiricalMeth- odsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem- ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253. [68] WanjunZhong,JingjingXu,DuyuTang,ZenanXu,NanDuan,M"
639,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf," WanjunZhong,JingjingXu,DuyuTang,ZenanXu,NanDuan,MingZhou,JiahaiWang,and JianYin. Reasoningoversemantic-levelgraphforfactchecking. ArXiv,abs/1909.03745,2019. URLhttps://arxiv.org/abs/1909.03745. 16
"
