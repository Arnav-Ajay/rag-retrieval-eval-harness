question_id,question,question_intent,gold_chunk_id,gold_doc_id,retrieved_chunk_ids,rank_of_first_relevant,retrieved_in_top_k,notes
1,What architectural components are removed in the Transformer model?,definition,1,1,56|630|276|130|232|61|609|618|536|54|178|17|488|565|627|491|291|1|364|83,18,FALSE,relevant chunk retrieved but ranked low
2,What aspect of recurrent neural networks prevents parallelization during training?,mechanism,6,1,630|536|61|56|130|54|276|17|232|565|62|581|1|291|84|49|518|15|572|123,—,FALSE,relevant chunk not retrieved in inspect_k
3,What capability do attention mechanisms provide for modeling dependencies in sequences?,capability,7,1,630|130|56|536|61|17|54|276|232|581|15|627|62|495|109|386|49|565|31|24,—,FALSE,relevant chunk not retrieved in inspect_k
4,What mechanism does the Transformer rely on instead of recurrence?,definition,8,1,56|630|130|276|232|178|627|61|565|609|491|618|17|580|536|54|291|561|20|83,—,FALSE,relevant chunk not retrieved in inspect_k
5,How does the number of operations required to relate distant positions differ between convolutional models and the Transformer?,comparative,9,1,41|8|591|586|573|538|9|572|31|529|26|588|38|517|4|53|300|6|612|562,7,FALSE,relevant chunk retrieved outside top_k
6,How is self-attention defined?,definition,10,1,595|375|628|627|131|171|568|241|178|220|395|630|565|616|228|195|636|274|545|299,—,FALSE,relevant chunk not retrieved in inspect_k
7,What distinguishes the Transformer from earlier transduction models?,definition,11,1,56|630|276|232|130|536|61|491|17|178|609|627|54|1|83|565|229|618|591|571,—,FALSE,relevant chunk not retrieved in inspect_k
8,What overall encoderâ€“decoder architecture does the Transformer use?,definition,13,1,605|431|384|491|523|54|522|56|124|354|130|171|509|627|432|393|397|67|84|617,—,FALSE,relevant chunk not retrieved in inspect_k
9,What two sub-layers make up each encoder layer?,definition,14,1,627|576|565|630|491|178|628|274|580|56|616|352|55|609|59|132|276|232|182|241,—,FALSE,relevant chunk not retrieved in inspect_k
10,What additional sub-layer is added to each decoder layer?,definition,15,1,491|565|56|627|630|130|232|178|276|59|1|11|17|274|182|15|555|609|48|61,16,FALSE,relevant chunk retrieved but ranked low
11,Why is masking applied in the decoder self-attention layer?,mechanism,16,1,56|565|627|491|630|232|130|276|178|1|61|536|59|11|182|488|257|17|291|83,—,FALSE,relevant chunk not retrieved in inspect_k
12,What operation is applied to scaled dot products before weighting the values?,mechanism,18,1,630|56|61|536|276|130|17|54|1|232|62|565|291|49|581|31|83|627|182|84,—,FALSE,relevant chunk not retrieved in inspect_k
13,What advantage does multi-head attention provide?,capability,21,1,491|576|627|565|630|56|580|130|628|232|274|178|276|128|17|55|182|1|241|609,—,FALSE,relevant chunk not retrieved in inspect_k
14,How are illegal attention connections prevented in the decoder?,mechanism,25,1,56|627|565|130|630|232|178|276|59|1|83|536|491|291|61|182|11|17|15|62,—,FALSE,relevant chunk not retrieved in inspect_k
15,What is the functional form of the position-wise feed-forward network?,mechanism,26,1,56|630|276|232|130|536|609|178|17|62|229|61|491|83|1|226|291|20|31|49,—,FALSE,relevant chunk not retrieved in inspect_k
16,What mathematical functions are used to construct positional encodings?,mechanism,30,1,56|630|276|130|232|609|61|536|17|178|62|488|49|83|1|182|291|229|20|84,—,FALSE,relevant chunk not retrieved in inspect_k
17,How do statistical language models compute the probability of text?,definition,66,2,56|630|276|491|232|130|61|178|627|536|488|609|17|618|291|1|62|565|49|83,—,FALSE,relevant chunk not retrieved in inspect_k
18,What context length does an n-gram model condition on?,definition,67,2,491|627|630|565|56|178|276|576|59|128|232|17|555|130|182|11|580|1|536|291,—,FALSE,relevant chunk not retrieved in inspect_k
19,What technique is used to mitigate data sparsity in n-gram models?,mechanism,68,2,56|630|276|130|61|565|491|609|17|232|83|536|178|1|627|618|114|364|291|580,—,FALSE,relevant chunk not retrieved in inspect_k
20,How do neural language models address data sparsity differently from n-gram models?,comparative,70,2,630|130|56|536|232|276|54|61|565|17|62|518|1|15|572|49|163|109|386|581,—,FALSE,relevant chunk not retrieved in inspect_k
21,What property of embedding vectors enables semantic similarity computation?,principle,71,2,56|630|276|130|178|536|232|61|17|83|62|1|627|571|609|491|84|31|565|518,—,FALSE,relevant chunk not retrieved in inspect_k
22,What distinguishes pre-trained language models from early neural language models?,comparative,73,2,630|56|61|536|130|232|17|276|581|1|565|49|54|15|229|62|123|291|572|31,—,FALSE,relevant chunk not retrieved in inspect_k
23,What are the two stages of the pre-training and fine-tuning paradigm?,process,74,2,56|630|232|276|130|536|178|17|609|1|83|84|15|291|627|145|571|62|31|20,—,FALSE,relevant chunk not retrieved in inspect_k
24,How are large language models defined in terms of scale and architecture?,definition,76,2,56|630|130|276|536|232|62|609|61|17|1|178|49|488|20|31|83|54|565|291,—,FALSE,relevant chunk not retrieved in inspect_k
25,What emergent abilities are observed in large language models?,capability,80,2,627|565|56|630|232|276|130|178|59|491|11|182|609|61|121|536|229|1|48|291,—,FALSE,relevant chunk not retrieved in inspect_k
26,What capability does instruction tuning enable?,capability,81,2,627|178|565|576|491|628|274|276|630|56|609|232|580|616|55|241|105|128|228|130,—,FALSE,relevant chunk not retrieved in inspect_k
27,Which neural architectures were commonly used before Transformers?,historical,83,2,56|630|232|276|130|536|61|291|565|178|488|84|609|627|59|491|17|54|1|83,20,FALSE,relevant chunk retrieved but ranked last
28,What advantage do Transformers have over recurrent neural networks?,comparative,85,2,56|630|130|232|276|61|17|536|54|618|178|609|364|491|565|291|561|1|627|488,—,FALSE,relevant chunk not retrieved in inspect_k
29,What components make up the BERT architecture?,definition,91,2,627|576|565|178|630|274|628|491|55|559|616|56|276|241|609|228|232|204|224|49,—,FALSE,relevant chunk not retrieved in inspect_k
30,What pre-training objectives are used in BERT?,process,92,2,627|628|576|565|630|55|616|274|491|178|580|228|56|352|432|204|241|232|276|59,—,FALSE,relevant chunk not retrieved in inspect_k
31,How does replaced token detection differ from masked language modeling?,mechanism,104,2,56|630|276|232|130|536|178|609|61|62|488|17|83|182|1|572|84|54|20|616,—,FALSE,relevant chunk not retrieved in inspect_k
32,What training approaches are used in XLM?,process,107,2,627|628|491|630|565|375|616|540|228|241|274|131|55|178|395|576|479|195|204|182,—,FALSE,relevant chunk not retrieved in inspect_k
33,What architectural modifications distinguish LLaMA from GPT-3?,comparative,128,2,56|565|627|630|491|276|232|178|130|11|59|536|61|182|17|1|229|576|291|609,—,FALSE,relevant chunk not retrieved in inspect_k
34,What scaling rule does the Chinchilla study identify for compute-optimal training?,principle,155,2,630|536|56|130|61|232|15|276|17|54|1|49|565|386|84|572|581|123|109|291,—,FALSE,relevant chunk not retrieved in inspect_k
35,What types of memory are combined in retrieval-augmented generation models?,definition,516,3,56|630|276|61|130|536|232|178|17|263|83|609|54|627|291|1|565|561|49|182,—,FALSE,relevant chunk not retrieved in inspect_k
36,What serves as the non-parametric memory in RAG models?,definition,516,3,491|627|565|56|630|276|312|128|59|232|291|536|576|61|130|11|580|17|274|47,—,FALSE,relevant chunk not retrieved in inspect_k
37,How do the two RAG formulations differ in how retrieved passages are used?,comparative,517,3,56|630|276|130|61|232|178|536|62|83|17|1|54|627|609|84|565|572|182|31,—,FALSE,relevant chunk not retrieved in inspect_k
38,What limitations of purely parametric language models are identified?,principle,518,3,56|630|130|276|232|609|17|178|536|491|571|61|1|15|509|627|84|580|20|291,—,FALSE,relevant chunk not retrieved in inspect_k
39,Which models combine masked language models with a differentiable retriever?,definition,519,3,56|630|276|536|61|130|232|178|17|609|291|627|62|1|84|54|15|182|565|581,—,FALSE,relevant chunk not retrieved in inspect_k
40,What search method is used to retrieve top-K documents in RAG?,mechanism,525,3,56|491|627|565|630|276|178|232|59|11|130|536|1|618|182|15|17|291|609|571,—,FALSE,relevant chunk not retrieved in inspect_k
41,Which sequence-to-sequence model is used as the generator in RAG?,definition,526,3,56|630|565|627|232|61|276|130|536|491|54|571|618|178|291|182|59|17|1|264,—,FALSE,relevant chunk not retrieved in inspect_k
42,What approximation is used to marginalize over retrieved documents during generation?,mechanism,527,3,630|56|130|536|61|232|17|276|54|62|15|565|109|49|581|1|572|518|386|627,—,FALSE,relevant chunk not retrieved in inspect_k
43,How does this approach differ from training non-parametric memory from scratch?,comparative,528,3,630|56|536|130|61|54|276|193|149|232|291|17|1|565|62|84|15|591|572|257,—,FALSE,relevant chunk not retrieved in inspect_k
44,Which open-domain QA datasets achieve state-of-the-art results with RAG?,historical,529,3,56|630|276|130|609|178|61|232|536|237|17|84|618|627|1|20|565|182|555|34,—,FALSE,relevant chunk not retrieved in inspect_k
45,What components make up a retrieval-augmented generation model?,definition,531,3,56|565|630|276|627|178|232|130|59|609|1|491|61|536|182|561|11|291|17|83,—,FALSE,relevant chunk not retrieved in inspect_k
46,What assumption does the RAG-Sequence model make about retrieved documents?,mechanism,533,3,56|630|276|130|232|178|536|61|17|1|609|15|571|627|48|62|54|291|31|565,—,FALSE,relevant chunk not retrieved in inspect_k
47,What capability does RAG-Token introduce compared to RAG-Sequence?,comparative,535,3,56|630|276|232|130|178|609|565|491|580|61|627|17|536|618|291|1|561|488|62,—,FALSE,relevant chunk not retrieved in inspect_k
48,How can RAG be adapted for sequence classification?,process,536,3,491|565|576|56|627|276|630|232|178|128|130|580|11|182|274|257|114|59|536|291,19,FALSE,relevant chunk retrieved but ranked low
49,What similarity operation is used to score queryâ€“document pairs in DPR?,mechanism,537,3,222|43|491|424|575|229|345|83|471|524|123|399|232|523|136|82|77|630|276|237,—,FALSE,relevant chunk not retrieved in inspect_k
50,On which datasets was the DPR retriever originally trained?,historical,538,3,56|565|627|491|630|276|232|11|83|130|1|182|61|59|20|536|49|178|17|609,—,FALSE,relevant chunk not retrieved in inspect_k
51,What model is used as the generator in the RAG framework?,definition,539,3,491|627|565|630|56|276|232|182|130|368|1|178|83|257|11|59|58|241|121|17,—,FALSE,relevant chunk not retrieved in inspect_k
52,"Which components are fine-tuned during training, and which are kept fixed?",process,540,3,56|630|276|83|130|61|536|232|178|609|488|62|627|17|54|1|84|571|565|291,—,FALSE,relevant chunk retrieved but ranked low
53,What distinguishes Fast Decoding from Thorough Decoding in RAG?,comparative,543,3,56|627|565|630|232|276|491|178|130|536|59|61|17|11|1|609|182|291|48|263,—,FALSE,relevant chunk not retrieved in inspect_k
54,How can RAG models update world knowledge without retraining?,mechanism,573,3,565|56|627|276|630|491|83|232|130|59|61|536|1|62|182|178|291|241|11|257,—,FALSE,relevant chunk not retrieved in inspect_k
