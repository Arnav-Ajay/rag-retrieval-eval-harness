question_id,question,question_intent,gold_chunk_id,gold_doc_id,retrieved_chunk_ids,rank_of_first_relevant,retrieved_in_top_k,notes
1,What architectural components are removed in the Transformer model?,definition,1,1,56|630|276|130|232|61|609|618|536|54|178|17|488|565|627|491|291|1|364|83|591|182|62|31|84|571|561|263|138|20|149|241|59|580|15|257|226|616|581|49|327|163|34|161|204|572|41|237|30|517,18,FALSE,relevant chunk retrieved but ranked low
2,What aspect of recurrent neural networks prevents parallelization during training?,mechanism,6,1,630|536|61|56|130|54|276|17|232|565|62|581|1|291|84|49|518|15|572|123|109|561|545|386|182|527|31|616|552|627|591|629|83|570|357|241|557|221|105|204|586|517|525|6|24|80|495|224|529|157,45,FALSE,relevant chunk retrieved but ranked low
3,What capability do attention mechanisms provide for modeling dependencies in sequences?,capability,7,1,630|130|56|536|61|17|54|276|232|581|15|627|62|495|109|386|49|565|31|24|518|557|572|616|308|1|545|552|53|561|517|48|527|89|586|219|204|123|224|609|87|353|632|570|6|10|562|591|85|542,—,FALSE,relevant chunk not retrieved in inspect_k
4,What mechanism does the Transformer rely on instead of recurrence?,definition,8,1,56|630|130|276|232|178|627|61|565|609|491|618|17|580|536|54|291|561|20|83|1|15|182|59|571|591|31|195|488|84|572|263|62|570|257|555|48|299|30|509|105|386|24|364|224|581|545|41|511|34,—,FALSE,relevant chunk not retrieved in inspect_k
5,How does the number of operations required to relate distant positions differ between convolutional models and the Transformer?,comparative,9,1,41|8|591|586|573|538|9|572|31|529|26|588|38|517|4|53|300|6|612|562|528|638|554|17|24|589|219|20|33|552|570|29|85|584|557|3|45|568|533|30|353|5|34|250|7|519|518|527|539|47,7,FALSE,relevant chunk retrieved outside top_k
6,How is self-attention defined?,definition,10,1,595|375|628|627|131|171|568|241|178|220|395|630|565|616|228|195|636|274|545|299|224|327|540|580|578|182|392|113|479|308|257|152|2|105|56|193|35|177|204|87|107|386|376|491|247|621|114|59|344|187,—,FALSE,relevant chunk not retrieved in inspect_k
7,What distinguishes the Transformer from earlier transduction models?,definition,11,1,56|630|276|232|130|536|61|491|17|178|609|627|54|1|83|565|229|618|591|571|31|20|263|291|182|204|561|488|580|84|226|581|572|15|555|62|364|250|187|105|323|49|259|386|6|163|87|264|545|41,—,FALSE,relevant chunk not retrieved in inspect_k
8,What overall encoderâ€“decoder architecture does the Transformer use?,definition,13,1,605|431|384|491|523|54|522|56|124|354|130|171|509|627|432|393|397|67|84|617|177|387|592|186|352|27|129|62|595|227|630|625|503|618|134|364|221|248|370|511|498|87|65|271|224|506|604|121|296|262,—,FALSE,relevant chunk not retrieved in inspect_k
9,What two sub-layers make up each encoder layer?,definition,14,1,627|576|565|630|491|178|628|274|580|56|616|352|55|609|59|132|276|232|182|241|17|536|105|204|21|291|228|130|20|264|121|161|11|83|54|591|31|128|257|552|432|49|15|263|42|323|48|1|61|391,—,FALSE,relevant chunk not retrieved in inspect_k
10,What additional sub-layer is added to each decoder layer?,definition,15,1,491|565|56|627|630|130|232|178|276|59|1|11|17|274|182|15|555|609|48|61|291|536|265|83|20|257|571|2|47|241|128|31|171|58|121|224|576|618|392|552|87|591|49|386|572|54|376|488|546|352,16,FALSE,relevant chunk retrieved but ranked low
11,Why is masking applied in the decoder self-attention layer?,mechanism,16,1,56|565|627|491|630|232|130|276|178|1|61|536|59|11|182|488|257|17|291|83|48|591|20|15|555|241|47|561|31|58|609|571|49|576|62|618|386|163|327|193|114|34|80|149|580|352|105|54|572|546,—,FALSE,relevant chunk not retrieved in inspect_k
12,What operation is applied to scaled dot products before weighting the values?,mechanism,18,1,630|56|61|536|276|130|17|54|1|232|62|565|291|49|581|31|83|627|182|84|123|518|48|572|591|571|15|561|109|178|263|386|527|609|570|80|545|552|491|229|509|495|24|192|204|257|364|347|35|87,—,FALSE,relevant chunk not retrieved in inspect_k
13,What advantage does multi-head attention provide?,capability,21,1,491|576|627|565|630|56|580|130|628|232|274|178|276|128|17|55|182|1|241|609|59|20|11|83|15|224|105|291|121|31|559|552|42|488|47|61|401|536|616|352|257|571|591|124|62|49|583|299|48|555,—,FALSE,relevant chunk not retrieved in inspect_k
14,How are illegal attention connections prevented in the decoder?,mechanism,25,1,56|627|565|130|630|232|178|276|59|1|83|536|491|291|61|182|11|17|15|62|571|609|257|54|48|591|576|31|561|237|171|618|555|224|229|149|572|20|84|274|352|241|87|204|386|47|34|80|264|109,—,FALSE,relevant chunk not retrieved in inspect_k
15,What is the functional form of the position-wise feed-forward network?,mechanism,26,1,56|630|276|232|130|536|609|178|17|62|229|61|491|83|1|226|291|20|31|49|263|581|571|15|182|84|591|488|364|618|627|565|572|327|250|54|555|149|570|479|386|616|6|533|34|392|41|264|124|109,—,FALSE,relevant chunk not retrieved in inspect_k
16,What mathematical functions are used to construct positional encodings?,mechanism,30,1,56|630|276|130|232|609|61|536|17|178|62|488|49|83|1|182|291|229|20|84|264|571|618|591|31|545|299|54|386|581|565|15|518|495|572|40|570|109|627|121|546|616|510|41|250|555|34|105|48|241,—,FALSE,relevant chunk not retrieved in inspect_k
17,How do statistical language models compute the probability of text?,definition,66,2,56|630|276|491|232|130|61|178|627|536|488|609|17|618|291|1|62|565|49|83|571|555|257|54|84|591|182|20|580|546|224|241|31|149|572|561|495|552|263|123|581|87|509|121|15|226|323|41|48|386,—,FALSE,relevant chunk not retrieved in inspect_k
18,What context length does an n-gram model condition on?,definition,67,2,491|627|630|565|56|178|276|576|59|128|232|17|555|130|182|11|580|1|536|291|15|323|274|609|105|20|312|591|552|48|257|229|34|47|61|21|571|121|31|241|264|49|10|265|2|204|574|148|224|84,—,FALSE,relevant chunk not retrieved in inspect_k
19,What technique is used to mitigate data sparsity in n-gram models?,mechanism,68,2,56|630|276|130|61|565|491|609|17|232|83|536|178|1|627|618|114|364|291|580|571|54|182|257|229|561|161|555|591|62|20|84|31|49|224|572|250|509|105|488|59|263|87|518|581|6|181|552|15|121,—,FALSE,relevant chunk not retrieved in inspect_k
20,How do neural language models address data sparsity differently from n-gram models?,comparative,70,2,630|130|56|536|232|276|54|61|565|17|62|518|1|15|572|49|163|109|386|581|561|178|93|31|552|627|224|123|241|591|529|189|24|545|495|582|527|586|84|89|291|629|327|6|557|517|353|192|237|390,—,FALSE,relevant chunk not retrieved in inspect_k
21,What property of embedding vectors enables semantic similarity computation?,principle,71,2,56|630|276|130|178|536|232|61|17|83|62|1|627|571|609|491|84|31|565|518|54|291|49|572|15|397|109|581|591|555|182|570|203|263|192|20|224|89|546|592|48|123|529|30|552|40|299|386|41|420,—,FALSE,relevant chunk not retrieved in inspect_k
22,What distinguishes pre-trained language models from early neural language models?,comparative,73,2,630|56|61|536|130|232|17|276|581|1|565|49|54|15|229|62|123|291|572|31|627|84|109|241|570|386|87|518|89|338|24|495|591|545|527|83|364|616|561|105|552|629|48|390|6|557|178|529|189|517,—,FALSE,relevant chunk not retrieved in inspect_k
23,What are the two stages of the pre-training and fine-tuning paradigm?,process,74,2,56|630|232|276|130|536|178|17|609|1|83|84|15|291|627|145|571|62|31|20|491|61|182|54|581|591|488|618|565|49|572|555|10|149|263|323|616|327|386|561|570|124|229|257|224|517|204|226|41|250,—,FALSE,relevant chunk not retrieved in inspect_k
24,How are large language models defined in terms of scale and architecture?,definition,76,2,56|630|130|276|536|232|62|609|61|17|1|178|49|488|20|31|83|54|565|291|571|109|84|545|386|518|572|591|182|555|616|562|15|581|263|41|279|34|149|163|30|241|570|491|397|40|204|552|627|618,—,FALSE,relevant chunk not retrieved in inspect_k
25,What emergent abilities are observed in large language models?,capability,80,2,627|565|56|630|232|276|130|178|59|491|11|182|609|61|121|536|229|1|48|291|561|17|572|591|83|224|571|58|54|87|15|618|105|237|264|31|241|20|257|49|34|546|393|84|62|386|299|545|581|518,—,FALSE,relevant chunk not retrieved in inspect_k
26,What capability does instruction tuning enable?,capability,81,2,627|178|565|576|491|628|274|276|630|56|609|232|580|616|55|241|105|128|228|130|48|182|2|352|204|17|59|121|62|11|224|83|264|574|536|432|559|42|291|1|552|161|47|31|257|61|21|591|132|20,—,FALSE,relevant chunk not retrieved in inspect_k
27,Which neural architectures were commonly used before Transformers?,historical,83,2,56|630|232|276|130|536|61|291|565|178|488|84|609|627|59|491|17|54|1|83|182|618|571|572|591|561|224|20|580|31|257|15|555|62|581|149|161|263|11|41|121|226|364|510|220|49|386|241|323|545,20,FALSE,relevant chunk retrieved but ranked low
28,What advantage do Transformers have over recurrent neural networks?,comparative,85,2,56|630|130|232|276|61|17|536|54|618|178|609|364|491|565|291|561|1|627|488|571|84|83|20|31|591|580|581|182|62|264|49|509|59|572|15|518|552|545|257|555|570|229|386|220|41|263|299|274|250,—,FALSE,relevant chunk not retrieved in inspect_k
29,What components make up the BERT architecture?,definition,91,2,627|576|565|178|630|274|628|491|55|559|616|56|276|241|609|228|232|204|224|49|121|17|128|580|59|182|130|2|264|11|352|574|131|48|105|229|432|62|42|536|1|552|83|61|47|312|263|291|591|21,—,FALSE,relevant chunk not retrieved in inspect_k
30,What pre-training objectives are used in BERT?,process,92,2,627|628|576|565|630|55|616|274|491|178|580|228|56|352|432|204|241|232|276|59|17|130|540|131|105|182|49|609|128|54|11|479|591|264|552|224|31|83|48|291|15|375|536|1|21|257|574|20|323|618,—,FALSE,relevant chunk not retrieved in inspect_k
31,How does replaced token detection differ from masked language modeling?,mechanism,104,2,56|630|276|232|130|536|178|609|61|62|488|17|83|182|1|572|84|54|20|616|291|49|31|618|591|327|627|571|518|15|325|565|109|181|545|386|570|229|555|257|41|241|263|581|161|557|250|552|353|495,—,FALSE,relevant chunk not retrieved in inspect_k
32,What training approaches are used in XLM?,process,107,2,627|628|491|630|565|375|616|540|228|241|274|131|55|178|395|576|479|195|204|182|618|574|105|232|56|275|59|224|327|58|352|276|15|609|128|138|130|121|161|393|517|2|31|17|11|48|49|552|591|122,—,FALSE,relevant chunk not retrieved in inspect_k
33,What architectural modifications distinguish LLaMA from GPT-3?,comparative,128,2,56|565|627|630|491|276|232|178|130|11|59|536|61|182|17|1|229|576|291|609|618|241|591|352|15|20|274|204|257|62|48|31|54|128|83|580|571|552|616|561|224|312|49|264|323|572|263|121|84|47,34,FALSE,relevant chunk retrieved but ranked low
34,What scaling rule does the Chinchilla study identify for compute-optimal training?,principle,155,2,630|536|56|130|61|232|15|276|17|54|1|49|565|386|84|572|581|123|109|291|62|518|498|561|31|591|495|545|517|552|357|616|327|570|229|629|265|192|557|178|241|627|157|592|182|24|6|263|21|10,—,FALSE,relevant chunk not retrieved in inspect_k
35,What types of memory are combined in retrieval-augmented generation models?,definition,516,3,56|630|276|61|130|536|232|178|17|263|83|609|54|627|291|1|565|561|49|182|62|31|109|555|518|241|571|84|591|581|572|48|386|491|15|570|510|41|618|357|488|552|20|254|149|10|517|573|34|40,—,FALSE,relevant chunk not retrieved in inspect_k
36,What serves as the non-parametric memory in RAG models?,definition,516,3,491|627|565|56|630|276|312|128|59|232|291|536|576|61|130|11|580|17|274|47|1|15|178|241|204|187|182|327|263|618|323|193|257|561|83|31|583|591|264|224|571|552|49|609|10|20|273|54|221|149,—,FALSE,relevant chunk not retrieved in inspect_k
37,How do the two RAG formulations differ in how retrieved passages are used?,comparative,517,3,56|630|276|130|61|232|178|536|62|83|17|1|54|627|609|84|565|572|182|31|123|571|291|591|488|518|15|49|491|109|163|20|241|80|546|224|41|527|561|192|386|581|189|552|570|48|38|545|181|592,—,FALSE,relevant chunk not retrieved in inspect_k
38,What limitations of purely parametric language models are identified?,principle,518,3,56|630|130|276|232|609|17|178|536|491|571|61|1|15|509|627|84|580|20|291|31|83|591|62|397|182|49|386|565|618|572|237|561|364|54|581|105|555|263|109|163|488|224|546|570|204|592|545|41|34,—,FALSE,relevant chunk not retrieved in inspect_k
39,Which models combine masked language models with a differentiable retriever?,definition,519,3,56|630|276|536|61|130|232|178|17|609|291|627|62|1|84|54|15|182|565|581|571|83|31|591|49|149|495|109|263|386|518|552|572|517|48|41|570|561|555|89|562|616|123|20|264|193|8|491|420|166,—,FALSE,relevant chunk not retrieved in inspect_k
40,What search method is used to retrieve top-K documents in RAG?,mechanism,525,3,56|491|627|565|630|276|178|232|59|11|130|536|1|618|182|15|17|291|609|571|61|20|257|591|83|576|226|48|34|237|274|121|546|580|31|561|58|263|193|572|87|118|224|273|54|241|555|195|105|49,—,FALSE,relevant chunk not retrieved in inspect_k
41,Which sequence-to-sequence model is used as the generator in RAG?,definition,526,3,56|630|565|627|232|61|276|130|536|491|54|571|618|178|291|182|59|17|1|264|609|83|15|84|11|591|561|580|193|31|257|20|204|274|364|149|121|119|118|572|277|114|488|241|48|389|581|41|47|62,—,FALSE,relevant chunk not retrieved in inspect_k
42,What approximation is used to marginalize over retrieved documents during generation?,mechanism,527,3,630|56|130|536|61|232|17|276|54|62|15|565|109|49|581|1|572|518|386|627|552|31|632|591|24|527|357|545|557|263|6|84|573|517|529|10|89|582|291|491|561|123|525|570|586|80|495|157|390|203,26,FALSE,relevant chunk retrieved but ranked low
43,How does this approach differ from training non-parametric memory from scratch?,comparative,528,3,630|56|536|130|61|54|276|193|149|232|291|17|1|565|62|84|15|591|572|257|518|109|49|581|89|517|182|627|31|527|123|570|386|552|83|616|263|325|561|178|10|491|347|6|557|24|327|573|495|204,—,FALSE,relevant chunk not retrieved in inspect_k
44,Which open-domain QA datasets achieve state-of-the-art results with RAG?,historical,529,3,56|630|276|130|609|178|61|232|536|237|17|84|618|627|1|20|565|182|555|34|49|591|62|488|561|291|31|571|48|572|518|54|15|386|83|109|581|263|570|149|491|8|562|552|105|545|241|41|264|123,—,FALSE,relevant chunk not retrieved in inspect_k
45,What components make up a retrieval-augmented generation model?,definition,531,3,56|565|630|276|627|178|232|130|59|609|1|491|61|536|182|561|11|291|17|83|224|264|48|571|555|618|263|15|257|572|591|312|241|20|229|31|114|62|54|128|368|47|84|193|581|49|237|386|367|576,—,FALSE,relevant chunk not retrieved in inspect_k
46,What assumption does the RAG-Sequence model make about retrieved documents?,mechanism,533,3,56|630|276|130|232|178|536|61|17|1|609|15|571|627|48|62|54|291|31|565|109|49|581|562|572|83|386|591|123|263|84|495|555|570|182|518|561|241|618|397|149|491|41|517|592|586|40|20|552|80,—,FALSE,relevant chunk not retrieved in inspect_k
47,What capability does RAG-Token introduce compared to RAG-Sequence?,comparative,535,3,56|630|276|232|130|178|609|565|491|580|61|627|17|536|618|291|1|561|488|62|83|54|59|182|263|20|555|571|591|581|31|229|257|84|495|572|48|224|15|105|241|364|226|390|161|49|250|386|41|545,—,FALSE,relevant chunk not retrieved in inspect_k
48,How can RAG be adapted for sequence classification?,process,536,3,491|565|576|56|627|276|630|232|178|128|130|580|11|182|274|257|114|59|536|291|1|224|118|83|17|105|308|591|571|561|15|10|509|574|31|628|61|312|20|62|273|327|34|21|226|47|572|162|609|552,19,FALSE,relevant chunk retrieved but ranked low
49,What similarity operation is used to score queryâ€“document pairs in DPR?,mechanism,537,3,222|43|491|424|575|229|345|83|471|524|123|399|232|523|136|82|77|630|276|237|93|48|312|56|90|134|61|632|565|265|555|178|130|268|230|1|210|502|145|67|249|387|561|89|580|288|221|273|340|359,—,FALSE,relevant chunk not retrieved in inspect_k
50,On which datasets was the DPR retriever originally trained?,historical,538,3,56|565|627|491|630|276|232|11|83|130|1|182|61|59|20|536|49|178|17|609|257|15|591|291|571|241|274|48|121|616|10|31|118|376|500|62|576|54|161|618|552|105|47|264|34|555|352|204|572|58,—,FALSE,relevant chunk not retrieved in inspect_k
51,What model is used as the generator in the RAG framework?,definition,539,3,491|627|565|630|56|276|232|182|130|368|1|178|83|257|11|59|58|241|121|17|571|536|274|61|20|128|609|49|15|576|291|618|591|509|31|47|119|264|166|224|193|555|204|87|561|10|34|572|552|109,—,FALSE,relevant chunk not retrieved in inspect_k
52,"Which components are fine-tuned during training, and which are kept fixed?",process,540,3,56|630|276|83|130|61|536|232|178|609|488|62|627|17|54|1|84|571|565|291|263|31|109|572|581|555|20|591|182|224|392|49|386|552|48|80|149|264|518|15|41|570|491|257|495|241|193|561|527|8,—,FALSE,relevant chunk retrieved but ranked low
53,What distinguishes Fast Decoding from Thorough Decoding in RAG?,comparative,543,3,56|627|565|630|232|276|491|178|130|536|59|61|17|11|1|609|182|291|48|263|257|204|576|54|83|618|15|241|591|561|20|386|580|224|31|229|571|352|49|128|572|264|62|555|188|581|552|323|84|109,—,FALSE,relevant chunk not retrieved in inspect_k
54,How can RAG models update world knowledge without retraining?,mechanism,573,3,565|56|627|276|630|491|83|232|130|59|61|536|1|62|182|178|291|241|11|257|226|618|488|17|591|571|561|609|58|121|54|114|31|15|20|572|48|224|84|555|113|47|34|386|41|319|299|80|204|509,—,FALSE,relevant chunk not retrieved in inspect_k
